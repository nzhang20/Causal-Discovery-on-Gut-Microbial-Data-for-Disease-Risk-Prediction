{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb253250-8808-4a7e-ae17-1539697cc949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "124107f0-715f-4853-8996-928d0918f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isometric Log-Ratio Transformation\n",
    "def ilr_transform(X):\n",
    "    epsilon = 1e-10\n",
    "    X = X + epsilon\n",
    "    return np.log(X / np.mean(X, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77542ce-7ca5-4f4f-bc15-4a3afa303b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centered Log-Ratio Transformation\n",
    "from scipy.stats import gmean\n",
    "\n",
    "def clr_transform(X, pseudocount=1e-6):\n",
    "    X += pseudocount\n",
    "    geometric_means = gmean(otu_values, axis=1, keepdims=True)\n",
    "    clr_values = np.log(otu_values / geometric_means)\n",
    "    \n",
    "    return clr_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d4c6a63-5350-452a-b8b0-26ed3cb0c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d = pd.read_csv(\"../qiime2/relative-frequency/genus_transposed.tsv\", sep=\"\\t\")\n",
    "t2d = t2d.rename(columns={\"Unnamed: 0\":\"sample-id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1ee3a6f-7fd7-417b-a238-2aaac797eac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample-id</th>\n",
       "      <th>d__Bacteria;p__Bacillota_A_368345;c__Clostridia_258483;o__Lachnospirales;f__Lachnospiraceae;__</th>\n",
       "      <th>d__Bacteria;p__Bacillota_I;c__Bacilli_A;o__Erysipelotrichales;f__Coprobacillaceae;g__Faecalibacillus</th>\n",
       "      <th>d__Bacteria;p__Bacteroidota;c__Bacteroidia;o__Bacteroidales;f__Bacteroidaceae;g__Prevotella</th>\n",
       "      <th>d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Burkholderiales;f__Burkholderiaceae_A_595427;g__Sutterella</th>\n",
       "      <th>d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Enterobacterales_737866;f__Enterobacteriaceae_A_725029;g__Enterobacter_B_683926</th>\n",
       "      <th>d__Bacteria;p__Bacillota_A_368345;c__Clostridia_258483;o__Oscillospirales;f__Ruminococcaceae;g__Faecalibacterium</th>\n",
       "      <th>d__Bacteria;p__Bacillota_I;c__Bacilli_A;o__Erysipelotrichales;f__Erysipelotrichaceae;g__Holdemanella</th>\n",
       "      <th>d__Bacteria;p__Bacillota_A_368345;c__Clostridia_258483;o__Lachnospirales;f__Lachnospiraceae;g__Lachnospira</th>\n",
       "      <th>d__Bacteria;p__Actinomycetota;c__Actinomycetes;o__Actinomycetales;f__Bifidobacteriaceae;g__Bifidobacterium_388775</th>\n",
       "      <th>...</th>\n",
       "      <th>d__Bacteria;p__Bacillota_I;c__Bacilli_A;o__Lactobacillales;f__Lactobacillaceae;g__Fructilactobacillus</th>\n",
       "      <th>d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Burkholderiales;f__Burkholderiaceae_A_595427;g__</th>\n",
       "      <th>d__Bacteria;p__Bacillota_A_368345;c__Clostridia_258483;o__Lachnospirales;f__Anaerotignaceae;g__Fimicola</th>\n",
       "      <th>d__Bacteria;p__Bacteroidota;c__Bacteroidia;o__Bacteroidales;f__Muribaculaceae;g__SFTJ01</th>\n",
       "      <th>d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Enterobacterales_737866;f__Enterobacteriaceae_A_729055;g__Serratia_L_726994</th>\n",
       "      <th>d__Bacteria;p__Bacillota_A_368345;c__Clostridia_258483;o__Lachnospirales;f__Lachnospiraceae;g__Shuttleworthia</th>\n",
       "      <th>d__Archaea;p__Thermoplasmatota;c__Thermoplasmata_1773;o__Methanomassiliicoccales;f__Methanomethylophilaceae;g__Methanomethylophilus</th>\n",
       "      <th>d__Bacteria;p__Bacillota_A_368345;c__Clostridia_258483;o__Oscillospirales;f__Acutalibacteraceae;g__Limousia</th>\n",
       "      <th>d__Bacteria;p__Actinomycetota;c__Actinomycetes;o__Actinomycetales;f__Cellulomonadaceae;g__Populibacterium</th>\n",
       "      <th>d__Bacteria;p__Bacteroidota;c__Bacteroidia;o__Bacteroidales;f__UBA932;g__Bact-08</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SRR16113010</td>\n",
       "      <td>0.130579</td>\n",
       "      <td>0.04775</td>\n",
       "      <td>0.260685</td>\n",
       "      <td>0.010117</td>\n",
       "      <td>0.004633</td>\n",
       "      <td>0.015696</td>\n",
       "      <td>0.014561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SRR16112701</td>\n",
       "      <td>0.067886</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067365</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRR9679959</td>\n",
       "      <td>0.023116</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>0.204478</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>0.141866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ERR9763111</td>\n",
       "      <td>0.089366</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.044211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.121484</td>\n",
       "      <td>0.003439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ERR9763104</td>\n",
       "      <td>0.062183</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.010584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095590</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.031643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 615 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample-id  \\\n",
       "0  SRR16113010   \n",
       "1  SRR16112701   \n",
       "2   SRR9679959   \n",
       "3   ERR9763111   \n",
       "4   ERR9763104   \n",
       "\n",
       "   d__Bacteria;p__Bacillota_A_368345;c__Clostridia_258483;o__Lachnospirales;f__Lachnospiraceae;__  \\\n",
       "0                                           0.130579                                                \n",
       "1                                           0.067886                                                \n",
       "2                                           0.023116                                                \n",
       "3                                           0.089366                                                \n",
       "4                                           0.062183                                                \n",
       "\n",
       "   d__Bacteria;p__Bacillota_I;c__Bacilli_A;o__Erysipelotrichales;f__Coprobacillaceae;g__Faecalibacillus  \\\n",
       "0                                            0.04775                                                      \n",
       "1                                            0.00000                                                      \n",
       "2                                            0.00033                                                      \n",
       "3                                            0.00000                                                      \n",
       "4                                            0.00000                                                      \n",
       "\n",
       "   d__Bacteria;p__Bacteroidota;c__Bacteroidia;o__Bacteroidales;f__Bacteroidaceae;g__Prevotella  \\\n",
       "0                                           0.260685                                             \n",
       "1                                           0.000000                                             \n",
       "2                                           0.204478                                             \n",
       "3                                           0.044211                                             \n",
       "4                                           0.010584                                             \n",
       "\n",
       "   d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Burkholderiales;f__Burkholderiaceae_A_595427;g__Sutterella  \\\n",
       "0                                           0.010117                                                                    \n",
       "1                                           0.000000                                                                    \n",
       "2                                           0.005746                                                                    \n",
       "3                                           0.000000                                                                    \n",
       "4                                           0.000000                                                                    \n",
       "\n",
       "   d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Enterobacterales_737866;f__Enterobacteriaceae_A_725029;g__Enterobacter_B_683926  \\\n",
       "0                                           0.004633                                                                                         \n",
       "1                                           0.067365                                                                                         \n",
       "2                                           0.000000                                                                                         \n",
       "3                                           0.000111                                                                                         \n",
       "4                                           0.000000                                                                                         \n",
       "\n",
       "   d__Bacteria;p__Bacillota_A_368345;c__Clostridia_258483;o__Oscillospirales;f__Ruminococcaceae;g__Faecalibacterium  \\\n",
       "0                                           0.015696                                                                  \n",
       "1                                           0.000069                                                                  \n",
       "2                                           0.038108                                                                  \n",
       "3                                           0.121484                                                                  \n",
       "4                                           0.095590                                                                  \n",
       "\n",
       "   d__Bacteria;p__Bacillota_I;c__Bacilli_A;o__Erysipelotrichales;f__Erysipelotrichaceae;g__Holdemanella  \\\n",
       "0                                           0.014561                                                      \n",
       "1                                           0.000000                                                      \n",
       "2                                           0.000000                                                      \n",
       "3                                           0.003439                                                      \n",
       "4                                           0.000551                                                      \n",
       "\n",
       "   d__Bacteria;p__Bacillota_A_368345;c__Clostridia_258483;o__Lachnospirales;f__Lachnospiraceae;g__Lachnospira  \\\n",
       "0                                           0.000000                                                            \n",
       "1                                           0.000000                                                            \n",
       "2                                           0.001849                                                            \n",
       "3                                           0.000000                                                            \n",
       "4                                           0.000221                                                            \n",
       "\n",
       "   d__Bacteria;p__Actinomycetota;c__Actinomycetes;o__Actinomycetales;f__Bifidobacteriaceae;g__Bifidobacterium_388775  \\\n",
       "0                                           0.160647                                                                   \n",
       "1                                           0.275813                                                                   \n",
       "2                                           0.141866                                                                   \n",
       "3                                           0.296777                                                                   \n",
       "4                                           0.031643                                                                   \n",
       "\n",
       "   ...  \\\n",
       "0  ...   \n",
       "1  ...   \n",
       "2  ...   \n",
       "3  ...   \n",
       "4  ...   \n",
       "\n",
       "   d__Bacteria;p__Bacillota_I;c__Bacilli_A;o__Lactobacillales;f__Lactobacillaceae;g__Fructilactobacillus  \\\n",
       "0                                                0.0                                                       \n",
       "1                                                0.0                                                       \n",
       "2                                                0.0                                                       \n",
       "3                                                0.0                                                       \n",
       "4                                                0.0                                                       \n",
       "\n",
       "   d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Burkholderiales;f__Burkholderiaceae_A_595427;g__  \\\n",
       "0                                                0.0                                                          \n",
       "1                                                0.0                                                          \n",
       "2                                                0.0                                                          \n",
       "3                                                0.0                                                          \n",
       "4                                                0.0                                                          \n",
       "\n",
       "   d__Bacteria;p__Bacillota_A_368345;c__Clostridia_258483;o__Lachnospirales;f__Anaerotignaceae;g__Fimicola  \\\n",
       "0                                                0.0                                                         \n",
       "1                                                0.0                                                         \n",
       "2                                                0.0                                                         \n",
       "3                                                0.0                                                         \n",
       "4                                                0.0                                                         \n",
       "\n",
       "   d__Bacteria;p__Bacteroidota;c__Bacteroidia;o__Bacteroidales;f__Muribaculaceae;g__SFTJ01  \\\n",
       "0                                                0.0                                         \n",
       "1                                                0.0                                         \n",
       "2                                                0.0                                         \n",
       "3                                                0.0                                         \n",
       "4                                                0.0                                         \n",
       "\n",
       "   d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Enterobacterales_737866;f__Enterobacteriaceae_A_729055;g__Serratia_L_726994  \\\n",
       "0                                                0.0                                                                                     \n",
       "1                                                0.0                                                                                     \n",
       "2                                                0.0                                                                                     \n",
       "3                                                0.0                                                                                     \n",
       "4                                                0.0                                                                                     \n",
       "\n",
       "   d__Bacteria;p__Bacillota_A_368345;c__Clostridia_258483;o__Lachnospirales;f__Lachnospiraceae;g__Shuttleworthia  \\\n",
       "0                                                0.0                                                               \n",
       "1                                                0.0                                                               \n",
       "2                                                0.0                                                               \n",
       "3                                                0.0                                                               \n",
       "4                                                0.0                                                               \n",
       "\n",
       "   d__Archaea;p__Thermoplasmatota;c__Thermoplasmata_1773;o__Methanomassiliicoccales;f__Methanomethylophilaceae;g__Methanomethylophilus  \\\n",
       "0                                                0.0                                                                                     \n",
       "1                                                0.0                                                                                     \n",
       "2                                                0.0                                                                                     \n",
       "3                                                0.0                                                                                     \n",
       "4                                                0.0                                                                                     \n",
       "\n",
       "   d__Bacteria;p__Bacillota_A_368345;c__Clostridia_258483;o__Oscillospirales;f__Acutalibacteraceae;g__Limousia  \\\n",
       "0                                                0.0                                                             \n",
       "1                                                0.0                                                             \n",
       "2                                                0.0                                                             \n",
       "3                                                0.0                                                             \n",
       "4                                                0.0                                                             \n",
       "\n",
       "   d__Bacteria;p__Actinomycetota;c__Actinomycetes;o__Actinomycetales;f__Cellulomonadaceae;g__Populibacterium  \\\n",
       "0                                                0.0                                                           \n",
       "1                                                0.0                                                           \n",
       "2                                                0.0                                                           \n",
       "3                                                0.0                                                           \n",
       "4                                                0.0                                                           \n",
       "\n",
       "   d__Bacteria;p__Bacteroidota;c__Bacteroidia;o__Bacteroidales;f__UBA932;g__Bact-08  \n",
       "0                                                0.0                                 \n",
       "1                                                0.0                                 \n",
       "2                                                0.0                                 \n",
       "3                                                0.0                                 \n",
       "4                                                0.0                                 \n",
       "\n",
       "[5 rows x 615 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9b9920e-e632-4950-a154-0f8e6ed612b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = t2d.iloc[:, 0]\n",
    "otu_values = t2d.iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f01f78aa-0087-493b-b612-a8c0a0a7592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ilr_transformed_values = ilr_transform(otu_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "530ac518-c0ba-44ea-a411-e80960f0d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clr_transformed_values = clr_transform(otu_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59328d9b-071d-49b4-abe5-2a95531226a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../qiime2/our-metadata.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11591c63-5b8d-4f17-9cb2-50adbfa09fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample-id</th>\n",
       "      <th>bioproject</th>\n",
       "      <th>sample-alias</th>\n",
       "      <th>t2d</th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>sex</th>\n",
       "      <th>metformin</th>\n",
       "      <th>diet-type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SRR12599850</td>\n",
       "      <td>PRJNA661673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>China</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SRR12599861</td>\n",
       "      <td>PRJNA661673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>China</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRR12599862</td>\n",
       "      <td>PRJNA661673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>China</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SRR12599863</td>\n",
       "      <td>PRJNA661673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>China</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SRR12599864</td>\n",
       "      <td>PRJNA661673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>China</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample-id   bioproject sample-alias  t2d country  age  bmi  sex  \\\n",
       "0  SRR12599850  PRJNA661673          NaN  Yes   China  NaN  NaN  NaN   \n",
       "1  SRR12599861  PRJNA661673          NaN  Yes   China  NaN  NaN  NaN   \n",
       "2  SRR12599862  PRJNA661673          NaN  Yes   China  NaN  NaN  NaN   \n",
       "3  SRR12599863  PRJNA661673          NaN  Yes   China  NaN  NaN  NaN   \n",
       "4  SRR12599864  PRJNA661673          NaN  Yes   China  NaN  NaN  NaN   \n",
       "\n",
       "  metformin  diet-type  \n",
       "0       NaN        NaN  \n",
       "1       NaN        NaN  \n",
       "2       NaN        NaN  \n",
       "3       NaN        NaN  \n",
       "4       NaN        NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e04bdb3f-ab38-4195-966f-2b7bfee8b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = clr_transformed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8540edf-2fe8-47d2-8019-1b9c7eab98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_labels = t2d.merge(metadata, on=\"sample-id\")['t2d'].apply(lambda x: 1 if x=='Yes' else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d6d4a5d-dad6-4c59-b94d-bc7621cd18a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X_data, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y_labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2faf65a-bc5e-43ea-b220-fa5eef57f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18259d51-4881-4e36-a930-4ad98f1e8adc",
   "metadata": {},
   "source": [
    "## Modified VAE Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "950fbe86-e0dd-4b12-aba5-2ead9adcf8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim_z1, latent_dim_z2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim + 1, latent_dim_z1 + latent_dim_z2)\n",
    "        self.fc2_mu_z1 = nn.Linear(latent_dim_z1 + latent_dim_z2, latent_dim_z1)\n",
    "        self.fc2_logvar_z1 = nn.Linear(latent_dim_z1 + latent_dim_z2, latent_dim_z1)\n",
    "        self.fc2_mu_z2 = nn.Linear(latent_dim_z1 + latent_dim_z2, latent_dim_z2)\n",
    "        self.fc2_logvar_z2 = nn.Linear(latent_dim_z1 + latent_dim_z2, latent_dim_z2)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = y.view(-1, 1)\n",
    "        x_cond = torch.cat([x, y], dim=1)  # Concatenate x and y\n",
    "        h = F.relu(self.fc1(x_cond))\n",
    "        \n",
    "        mu_z1 = self.fc2_mu_z1(h)\n",
    "        logvar_z1 = self.fc2_logvar_z1(h)\n",
    "        mu_z2 = self.fc2_mu_z2(h)\n",
    "        logvar_z2 = self.fc2_logvar_z2(h)\n",
    "\n",
    "        return mu_z1, logvar_z1, mu_z2, logvar_z2\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim_z1, latent_dim_z2, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim_z1 + latent_dim_z2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "        z = torch.cat([z1, z2], dim=1)  # Concatenate z1 and z2\n",
    "        h = F.relu(self.fc1(z))\n",
    "        return torch.sigmoid(self.fc2(h))\n",
    "\n",
    "class ModifiedVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim_z1, latent_dim_z2, hidden_dim):\n",
    "        super(ModifiedVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, latent_dim_z1, latent_dim_z2)\n",
    "        self.decoder = Decoder(latent_dim_z1, latent_dim_z2, hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)  # Sample from a standard normal\n",
    "        return mu + eps * std  # Sample from the latent distribution\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mu_z1, logvar_z1, mu_z2, logvar_z2 = self.encoder(x, y)\n",
    "        z1 = self.reparameterize(mu_z1, logvar_z1)\n",
    "        z2 = self.reparameterize(mu_z2, logvar_z2)\n",
    "        recon_x = self.decoder(z1, z2)\n",
    "        return recon_x, mu_z1, logvar_z1, mu_z2, logvar_z2\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu_z1, logvar_z1, mu_z2, logvar_z2, y, causal_reg=0.1):\n",
    "    reconstruction_loss = F.mse_loss(recon_x, x, reduction='mean')\n",
    "\n",
    "    # KL Divergence for z2 (unconditional prior)\n",
    "    KL_z2 = -0.5 * torch.sum(1 + logvar_z2 - mu_z2.pow(2) - logvar_z2.exp())\n",
    "\n",
    "    # Conditional prior for z1\n",
    "    mu_Y = torch.where(y.unsqueeze(1) == 1, \n",
    "                       torch.full_like(mu_z1, 2.0, device=y.device), \n",
    "                       torch.zeros_like(mu_z1))\n",
    "    KL_z1 = -0.5 * torch.sum(1 + logvar_z1 - ((mu_z1 - mu_Y).pow(2) + logvar_z1.exp()))\n",
    "\n",
    "    # Use summing or apply a mechanism to handle dimension match\n",
    "    # Example: using a simple method that combines information without direct multiplication\n",
    "    causal_loss = causal_reg * (mu_z1.mean(dim=1) @ mu_z2.mean(dim=1))  # Dot product across means\n",
    "\n",
    "    return reconstruction_loss + KL_z1 + KL_z2 + causal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97aee1e6-3b57-46a3-aa0c-79765117db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, epochs, causal_reg):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for x, y in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            recon_x, mu_z1, logvar_z1, mu_z2, logvar_z2 = model(x, y)\n",
    "            loss = loss_function(recon_x, x, mu_z1, logvar_z1, mu_z2, logvar_z2, y, causal_reg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(data_loader)}\")\n",
    "        total_loss += epoch_loss / len(data_loader)\n",
    "    \n",
    "    return total_loss / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "ac55b9d7-692b-49d4-8742-ba66b22e1751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameter tuning\n",
    "    latent_dim_z1 = trial.suggest_int(\"latent_dim_z1\", 10, 80)\n",
    "    latent_dim_z2 = trial.suggest_int(\"latent_dim_z2\", 10, 80)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 10, 260)\n",
    "\n",
    "    epochs = trial.suggest_int(\"epochs\", 10, 200)\n",
    "    causal_reg = trial.suggest_float(\"causal_reg\", 0.001, 1.0)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-1)\n",
    "\n",
    "    model = ModifiedVAE(614, latent_dim_z1, latent_dim_z2, hidden_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    avg_loss = train(model, data_loader, optimizer, epochs, causal_reg)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "9307fd57-739e-40f4-9c7a-818ec287a3b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:49:56,347] A new study created in memory with name: no-name-b506a009-454a-423e-a4f9-64b1dc67260d\n",
      "/var/folders/02/jj1jlsn97sj550kl_82_8lp40000gn/T/ipykernel_11945/917332750.py:9: FutureWarning:\n",
      "\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 264190.89565629227\n",
      "Epoch 2, Loss: 336.59163841834436\n",
      "Epoch 3, Loss: 250.99523397592398\n",
      "Epoch 4, Loss: 225.02817212618314\n",
      "Epoch 5, Loss: 219.83715233435998\n",
      "Epoch 6, Loss: 218.83264688345102\n",
      "Epoch 7, Loss: 218.4911135160006\n",
      "Epoch 8, Loss: 218.28239470261795\n",
      "Epoch 9, Loss: 218.3807883629432\n",
      "Epoch 10, Loss: 218.12854444063626\n",
      "Epoch 11, Loss: 218.13764982957107\n",
      "Epoch 12, Loss: 218.32415360670822\n",
      "Epoch 13, Loss: 218.1219740647536\n",
      "Epoch 14, Loss: 218.18657684326172\n",
      "Epoch 15, Loss: 218.01918910099909\n",
      "Epoch 16, Loss: 218.10800669743463\n",
      "Epoch 17, Loss: 217.93098361675555\n",
      "Epoch 18, Loss: 218.16925929142877\n",
      "Epoch 19, Loss: 217.89206783588116\n",
      "Epoch 20, Loss: 218.00320728008563\n",
      "Epoch 21, Loss: 217.87613325852615\n",
      "Epoch 22, Loss: 217.89228409987228\n",
      "Epoch 23, Loss: 218.04623295710638\n",
      "Epoch 24, Loss: 217.98059258094202\n",
      "Epoch 25, Loss: 218.0547764117901\n",
      "Epoch 26, Loss: 218.02867948091946\n",
      "Epoch 27, Loss: 217.8291605435885\n",
      "Epoch 28, Loss: 218.00534556462213\n",
      "Epoch 29, Loss: 217.88688131479117\n",
      "Epoch 30, Loss: 217.97393916203424\n",
      "Epoch 31, Loss: 218.056457226093\n",
      "Epoch 32, Loss: 217.77393810565655\n",
      "Epoch 33, Loss: 217.81907829871545\n",
      "Epoch 34, Loss: 218.12744346031775\n",
      "Epoch 35, Loss: 218.91301903357873\n",
      "Epoch 36, Loss: 217.9802240224985\n",
      "Epoch 37, Loss: 218.0029634328989\n",
      "Epoch 38, Loss: 218.0113868713379\n",
      "Epoch 39, Loss: 218.1006880540114\n",
      "Epoch 40, Loss: 217.8447638291579\n",
      "Epoch 41, Loss: 217.836668748122\n",
      "Epoch 42, Loss: 218.0117962176983\n",
      "Epoch 43, Loss: 217.86823654174805\n",
      "Epoch 44, Loss: 217.8149076608511\n",
      "Epoch 45, Loss: 217.7995611337515\n",
      "Epoch 46, Loss: 218.02819501436673\n",
      "Epoch 47, Loss: 217.8586208636944\n",
      "Epoch 48, Loss: 217.93447758601263\n",
      "Epoch 49, Loss: 217.68629514254056\n",
      "Epoch 50, Loss: 218.01751092764047\n",
      "Epoch 51, Loss: 217.89929287250226\n",
      "Epoch 52, Loss: 218.25980083759015\n",
      "Epoch 53, Loss: 218.33429629986102\n",
      "Epoch 54, Loss: 217.78504709097055\n",
      "Epoch 55, Loss: 217.67365895784818\n",
      "Epoch 56, Loss: 217.90788107651932\n",
      "Epoch 57, Loss: 217.87692906306341\n",
      "Epoch 58, Loss: 217.84174053485577\n",
      "Epoch 59, Loss: 217.97634829007663\n",
      "Epoch 60, Loss: 217.83094611534705\n",
      "Epoch 61, Loss: 218.22199572049655\n",
      "Epoch 62, Loss: 218.84701978243314\n",
      "Epoch 63, Loss: 219.5569780789889\n",
      "Epoch 64, Loss: 217.8522641108586\n",
      "Epoch 65, Loss: 217.79827205951398\n",
      "Epoch 66, Loss: 217.96120893038236\n",
      "Epoch 67, Loss: 218.42418876061072\n",
      "Epoch 68, Loss: 218.10875232403095\n",
      "Epoch 69, Loss: 217.9034544137808\n",
      "Epoch 70, Loss: 217.83844962486853\n",
      "Epoch 71, Loss: 218.3678976205679\n",
      "Epoch 72, Loss: 217.77980364285983\n",
      "Epoch 73, Loss: 217.7846873356746\n",
      "Epoch 74, Loss: 217.7536374605619\n",
      "Epoch 75, Loss: 217.80018058189978\n",
      "Epoch 76, Loss: 218.11722447321966\n",
      "Epoch 77, Loss: 217.826538672814\n",
      "Epoch 78, Loss: 218.81599044799805\n",
      "Epoch 79, Loss: 219.08442629300632\n",
      "Epoch 80, Loss: 217.94451845609225\n",
      "Epoch 81, Loss: 217.8461421086238\n",
      "Epoch 82, Loss: 218.22513756385217\n",
      "Epoch 83, Loss: 218.80728237445538\n",
      "Epoch 84, Loss: 218.69181677011343\n",
      "Epoch 85, Loss: 218.07907618009128\n",
      "Epoch 86, Loss: 218.08161163330078\n",
      "Epoch 87, Loss: 217.79795661339392\n",
      "Epoch 88, Loss: 217.80712875953088\n",
      "Epoch 89, Loss: 217.94930942241962\n",
      "Epoch 90, Loss: 218.65794665996845\n",
      "Epoch 91, Loss: 218.09465731107272\n",
      "Epoch 92, Loss: 218.56941721989557\n",
      "Epoch 93, Loss: 217.87842237032376\n",
      "Epoch 94, Loss: 217.76786921574518\n",
      "Epoch 95, Loss: 218.22066805912897\n",
      "Epoch 96, Loss: 217.88727510892429\n",
      "Epoch 97, Loss: 217.7352811373197\n",
      "Epoch 98, Loss: 218.15922957200272\n",
      "Epoch 99, Loss: 217.95983270498422\n",
      "Epoch 100, Loss: 218.13836435171274\n",
      "Epoch 101, Loss: 218.07539279644305\n",
      "Epoch 102, Loss: 218.13526564378006\n",
      "Epoch 103, Loss: 217.88307072566107\n",
      "Epoch 104, Loss: 218.09942803016077\n",
      "Epoch 105, Loss: 218.43916467519907\n",
      "Epoch 106, Loss: 218.52909117478592\n",
      "Epoch 107, Loss: 218.98281948383038\n",
      "Epoch 108, Loss: 219.5863048846905\n",
      "Epoch 109, Loss: 217.89999947181116\n",
      "Epoch 110, Loss: 217.88839310866135\n",
      "Epoch 111, Loss: 218.18027936495267\n",
      "Epoch 112, Loss: 218.06974205603967\n",
      "Epoch 113, Loss: 217.99570787869968\n",
      "Epoch 114, Loss: 217.69034459040716\n",
      "Epoch 115, Loss: 217.7389288682204\n",
      "Epoch 116, Loss: 217.9149874173678\n",
      "Epoch 117, Loss: 217.80761894812952\n",
      "Epoch 118, Loss: 218.05867532583383\n",
      "Epoch 119, Loss: 217.76774362417368\n",
      "Epoch 120, Loss: 218.10091136052057\n",
      "Epoch 121, Loss: 217.89749438946063\n",
      "Epoch 122, Loss: 218.23675771859976\n",
      "Epoch 123, Loss: 218.15894170907828\n",
      "Epoch 124, Loss: 219.19742701603815\n",
      "Epoch 125, Loss: 217.9665148808406\n",
      "Epoch 126, Loss: 218.41354399461014\n",
      "Epoch 127, Loss: 218.01052005474384\n",
      "Epoch 128, Loss: 217.77558502784143\n",
      "Epoch 129, Loss: 217.74423511211688\n",
      "Epoch 130, Loss: 218.63644614586462\n",
      "Epoch 131, Loss: 217.91719759427585\n",
      "Epoch 132, Loss: 217.8851517897386\n",
      "Epoch 133, Loss: 217.99935443584735\n",
      "Epoch 134, Loss: 217.86816905095026\n",
      "Epoch 135, Loss: 218.7639526954064\n",
      "Epoch 136, Loss: 217.8961410522461\n",
      "Epoch 137, Loss: 218.45679268470178\n",
      "Epoch 138, Loss: 218.0902478144719\n",
      "Epoch 139, Loss: 218.01994646512546\n",
      "Epoch 140, Loss: 218.47378510695236\n",
      "Epoch 141, Loss: 218.48664004986102\n",
      "Epoch 142, Loss: 217.98111372727615\n",
      "Epoch 143, Loss: 218.2344495333158\n",
      "Epoch 144, Loss: 218.38363001896784\n",
      "Epoch 145, Loss: 218.1370814396785\n",
      "Epoch 146, Loss: 218.13947266798752\n",
      "Epoch 147, Loss: 218.177546281081\n",
      "Epoch 148, Loss: 217.85672173133264\n",
      "Epoch 149, Loss: 218.02825927734375\n",
      "Epoch 150, Loss: 217.7898712158203\n",
      "Epoch 151, Loss: 218.09044500497671\n",
      "Epoch 152, Loss: 218.5638715303861\n",
      "Epoch 153, Loss: 218.0103645324707\n",
      "Epoch 154, Loss: 218.51709365844727\n",
      "Epoch 155, Loss: 217.62416516817532\n",
      "Epoch 156, Loss: 218.5383042555589\n",
      "Epoch 157, Loss: 217.87658955500677\n",
      "Epoch 158, Loss: 217.96673936110275\n",
      "Epoch 159, Loss: 218.27417021531326\n",
      "Epoch 160, Loss: 217.67980252779446\n",
      "Epoch 161, Loss: 218.11531448364258\n",
      "Epoch 162, Loss: 218.02901752178485\n",
      "Epoch 163, Loss: 218.22131729125977\n",
      "Epoch 164, Loss: 217.94876509446365\n",
      "Epoch 165, Loss: 217.79791787954477\n",
      "Epoch 166, Loss: 217.821714254526\n",
      "Epoch 167, Loss: 217.90471267700195\n",
      "Epoch 168, Loss: 218.20422392625076\n",
      "Epoch 169, Loss: 217.88328610933743\n",
      "Epoch 170, Loss: 217.87273788452148\n",
      "Epoch 171, Loss: 218.02112227219803\n",
      "Epoch 172, Loss: 218.19910049438477\n",
      "Epoch 173, Loss: 218.38870738102838\n",
      "Epoch 174, Loss: 218.49092776958759\n",
      "Epoch 175, Loss: 218.20004096397986\n",
      "Epoch 176, Loss: 218.2387569134052\n",
      "Epoch 177, Loss: 217.7267784705529\n",
      "Epoch 178, Loss: 218.22909868680514\n",
      "Epoch 179, Loss: 219.13570638803336\n",
      "Epoch 180, Loss: 217.7240163362943\n",
      "Epoch 181, Loss: 217.77220344543457\n",
      "Epoch 182, Loss: 218.02951167179987\n",
      "Epoch 183, Loss: 218.14233955970178\n",
      "Epoch 184, Loss: 217.9870317899264\n",
      "Epoch 185, Loss: 218.01134241544284\n",
      "Epoch 186, Loss: 218.17986092200647\n",
      "Epoch 187, Loss: 218.088135939378\n",
      "Epoch 188, Loss: 217.8363741361178\n",
      "Epoch 189, Loss: 217.9230675330529\n",
      "Epoch 190, Loss: 218.09484247060922\n",
      "Epoch 191, Loss: 217.84421421931341\n",
      "Epoch 192, Loss: 217.87598243126502\n",
      "Epoch 193, Loss: 217.9151037656344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:00,433] Trial 0 finished with value: 1565.7153605883288 and parameters: {'latent_dim_z1': 15, 'latent_dim_z2': 15, 'hidden_dim': 199, 'epochs': 196, 'causal_reg': 0.91338050936866, 'learning_rate': 0.023336709822344474}. Best is trial 0 with value: 1565.7153605883288.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, Loss: 218.8953484755296\n",
      "Epoch 195, Loss: 217.8029984694261\n",
      "Epoch 196, Loss: 218.10612751887396\n",
      "Epoch 1, Loss: 1986.1828308105469\n",
      "Epoch 2, Loss: 1430.7750537578877\n",
      "Epoch 3, Loss: 1103.8180189866287\n",
      "Epoch 4, Loss: 851.952883206881\n",
      "Epoch 5, Loss: 726.6383555485652\n",
      "Epoch 6, Loss: 655.610245337853\n",
      "Epoch 7, Loss: 607.994861309345\n",
      "Epoch 8, Loss: 574.2914111797626\n",
      "Epoch 9, Loss: 542.7821643535907\n",
      "Epoch 10, Loss: 516.2769329364484\n",
      "Epoch 11, Loss: 494.4765853881836\n",
      "Epoch 12, Loss: 472.2635803222656\n",
      "Epoch 13, Loss: 451.9280994121845\n",
      "Epoch 14, Loss: 436.6460659320538\n",
      "Epoch 15, Loss: 415.4760932922363\n",
      "Epoch 16, Loss: 400.4168959397536\n",
      "Epoch 17, Loss: 385.9684143066406\n",
      "Epoch 18, Loss: 370.92870330810547\n",
      "Epoch 19, Loss: 354.39316059992865\n",
      "Epoch 20, Loss: 340.1526371882512\n",
      "Epoch 21, Loss: 327.86404771071216\n",
      "Epoch 22, Loss: 317.1145263084999\n",
      "Epoch 23, Loss: 305.15957494882434\n",
      "Epoch 24, Loss: 290.08043817373425\n",
      "Epoch 25, Loss: 278.1753771855281\n",
      "Epoch 26, Loss: 267.83294560359076\n",
      "Epoch 27, Loss: 256.53209510216345\n",
      "Epoch 28, Loss: 247.16413263174204\n",
      "Epoch 29, Loss: 239.72305356539212\n",
      "Epoch 30, Loss: 228.4301041823167\n",
      "Epoch 31, Loss: 218.48566187345065\n",
      "Epoch 32, Loss: 210.9236940237192\n",
      "Epoch 33, Loss: 198.52219772338867\n",
      "Epoch 34, Loss: 190.53841576209436\n",
      "Epoch 35, Loss: 183.81950422433707\n",
      "Epoch 36, Loss: 175.0646641071026\n",
      "Epoch 37, Loss: 165.80904696537897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:01,266] Trial 1 finished with value: 441.94031427456787 and parameters: {'latent_dim_z1': 43, 'latent_dim_z2': 74, 'hidden_dim': 30, 'epochs': 40, 'causal_reg': 0.16875011792445635, 'learning_rate': 0.00011444816788743448}. Best is trial 1 with value: 441.94031427456787.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Loss: 159.97513242868277\n",
      "Epoch 39, Loss: 151.619627732497\n",
      "Epoch 40, Loss: 145.80533761244553\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 15:50:04,258] Trial 2 failed with parameters: {'latent_dim_z1': 35, 'latent_dim_z2': 59, 'hidden_dim': 55, 'epochs': 140, 'causal_reg': 0.7931584596954059, 'learning_rate': 0.07917456539506224} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 15:50:04,259] Trial 2 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 1, Loss: 1037.7454188420222\n",
      "Epoch 2, Loss: 479.0180317805364\n",
      "Epoch 3, Loss: 411.3396471463717\n",
      "Epoch 4, Loss: 333.9158407358023\n",
      "Epoch 5, Loss: 312.73585158128003\n",
      "Epoch 6, Loss: 232.40716200608475\n",
      "Epoch 7, Loss: 222.93174303494968\n",
      "Epoch 8, Loss: 216.18730192918045\n",
      "Epoch 9, Loss: 210.18008657602164\n",
      "Epoch 10, Loss: 163.23416900634766\n",
      "Epoch 11, Loss: 142.90993030254657\n",
      "Epoch 12, Loss: 163.8193633006169\n",
      "Epoch 13, Loss: 133.91839959071234\n",
      "Epoch 14, Loss: 144.2103465153621\n",
      "Epoch 15, Loss: 160.2180749453031\n",
      "Epoch 16, Loss: 106.65396675696739\n",
      "Epoch 17, Loss: 93.79974453265851\n",
      "Epoch 18, Loss: 93.78304400810829\n",
      "Epoch 19, Loss: 92.5720859674307\n",
      "Epoch 20, Loss: 113.39834242600661\n",
      "Epoch 21, Loss: 87.96065169114333\n",
      "Epoch 22, Loss: 80.58175138326791\n",
      "Epoch 23, Loss: 86.698760839609\n",
      "Epoch 24, Loss: 65.0240127490117\n",
      "Epoch 25, Loss: 74.02940141237698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:04,987] Trial 3 finished with value: 168.43752080067253 and parameters: {'latent_dim_z1': 54, 'latent_dim_z2': 10, 'hidden_dim': 192, 'epochs': 34, 'causal_reg': 0.9797074951025692, 'learning_rate': 0.004235852069209617}. Best is trial 3 with value: 168.43752080067253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: 88.80216891948993\n",
      "Epoch 27, Loss: 69.62595338087816\n",
      "Epoch 28, Loss: 53.79980571453388\n",
      "Epoch 29, Loss: 46.136709029857926\n",
      "Epoch 30, Loss: 41.1127227636484\n",
      "Epoch 31, Loss: 34.04571738609901\n",
      "Epoch 32, Loss: 37.58475894194383\n",
      "Epoch 33, Loss: 37.700573260967545\n",
      "Epoch 34, Loss: 58.79416876572829\n",
      "Epoch 1, Loss: 6401.461060744065\n",
      "Epoch 2, Loss: 880.5412292480469\n",
      "Epoch 3, Loss: 547.5650772681603\n",
      "Epoch 4, Loss: 419.97826796311597\n",
      "Epoch 5, Loss: 369.75095044649566\n",
      "Epoch 6, Loss: 313.23768146221454\n",
      "Epoch 7, Loss: 276.8747144845816\n",
      "Epoch 8, Loss: 279.4474061819223\n",
      "Epoch 9, Loss: 233.0249311006986\n",
      "Epoch 10, Loss: 212.36917935884915\n",
      "Epoch 11, Loss: 190.07082711733304\n",
      "Epoch 12, Loss: 180.7386905963604\n",
      "Epoch 13, Loss: 193.7726650238037\n",
      "Epoch 14, Loss: 156.7402560894306\n",
      "Epoch 15, Loss: 143.67802825340857\n",
      "Epoch 16, Loss: 142.15219438993014\n",
      "Epoch 17, Loss: 113.1978209568904\n",
      "Epoch 18, Loss: 115.4120600773738\n",
      "Epoch 19, Loss: 142.59586847745456\n",
      "Epoch 20, Loss: 214.7864148066594\n",
      "Epoch 21, Loss: 153.06039707477277\n",
      "Epoch 22, Loss: 133.48486298781174\n",
      "Epoch 23, Loss: 101.86744954035832\n",
      "Epoch 24, Loss: 92.38770939753606\n",
      "Epoch 25, Loss: 84.95635766249437\n",
      "Epoch 26, Loss: 76.8539299598107\n",
      "Epoch 27, Loss: 71.91235358898456\n",
      "Epoch 28, Loss: 69.04581891573392\n",
      "Epoch 29, Loss: 89.43134946089525\n",
      "Epoch 30, Loss: 93.85164304879996\n",
      "Epoch 31, Loss: 82.14600218259372\n",
      "Epoch 32, Loss: 70.20513351146991\n",
      "Epoch 33, Loss: 64.16611157930814\n",
      "Epoch 34, Loss: 53.17634061666635\n",
      "Epoch 35, Loss: 50.61412411469679\n",
      "Epoch 36, Loss: 76.3789719068087\n",
      "Epoch 37, Loss: 67.03505809490497\n",
      "Epoch 38, Loss: 90.48671201559213\n",
      "Epoch 39, Loss: 55.43084658109225\n",
      "Epoch 40, Loss: 40.91586773212139\n",
      "Epoch 41, Loss: 42.69716868033776\n",
      "Epoch 42, Loss: 41.82618441948524\n",
      "Epoch 43, Loss: 42.20081974909856\n",
      "Epoch 44, Loss: 40.19108266096849\n",
      "Epoch 45, Loss: 42.344416911785416\n",
      "Epoch 46, Loss: 30.248984556931717\n",
      "Epoch 47, Loss: 33.93145773960994\n",
      "Epoch 48, Loss: 68.7373533982497\n",
      "Epoch 49, Loss: 52.788269996643066\n",
      "Epoch 50, Loss: 43.09635287064772\n",
      "Epoch 51, Loss: 35.21769090799185\n",
      "Epoch 52, Loss: 28.088397979736328\n",
      "Epoch 53, Loss: 24.344059210557205\n",
      "Epoch 54, Loss: 23.532014039846565\n",
      "Epoch 55, Loss: 20.99371180167565\n",
      "Epoch 56, Loss: 38.883262377518875\n",
      "Epoch 57, Loss: 33.00252316548274\n",
      "Epoch 58, Loss: 34.30653623434213\n",
      "Epoch 59, Loss: 79.69758466573862\n",
      "Epoch 60, Loss: 62.348960362947906\n",
      "Epoch 61, Loss: 55.14837602468637\n",
      "Epoch 62, Loss: 35.26473826628465\n",
      "Epoch 63, Loss: 32.23365365541898\n",
      "Epoch 64, Loss: 23.240244461939884\n",
      "Epoch 65, Loss: 20.80111679664025\n",
      "Epoch 66, Loss: 17.691351377047024\n",
      "Epoch 67, Loss: 15.6789639546321\n",
      "Epoch 68, Loss: 14.49379642193134\n",
      "Epoch 69, Loss: 18.792801526876595\n",
      "Epoch 70, Loss: 17.09633940916795\n",
      "Epoch 71, Loss: 22.895735887380745\n",
      "Epoch 72, Loss: 53.97005741412823\n",
      "Epoch 73, Loss: 38.33440615580632\n",
      "Epoch 74, Loss: 28.751154826237606\n",
      "Epoch 75, Loss: 21.518824797410232\n",
      "Epoch 76, Loss: 28.032470336327187\n",
      "Epoch 77, Loss: 37.31806292900672\n",
      "Epoch 78, Loss: 30.865671157836914\n",
      "Epoch 79, Loss: 25.752676486968994\n",
      "Epoch 80, Loss: 19.32274682705219\n",
      "Epoch 81, Loss: 18.69689915730403\n",
      "Epoch 82, Loss: 20.046379566192627\n",
      "Epoch 83, Loss: 68.02293337308444\n",
      "Epoch 84, Loss: 35.53794684776893\n",
      "Epoch 85, Loss: 18.970327065541195\n",
      "Epoch 86, Loss: 19.286360924060528\n",
      "Epoch 87, Loss: 23.022302077366756\n",
      "Epoch 88, Loss: 16.37355454151447\n",
      "Epoch 89, Loss: 17.62209646518414\n",
      "Epoch 90, Loss: 23.492249452150784\n",
      "Epoch 91, Loss: 30.44746307226328\n",
      "Epoch 92, Loss: 34.665396433610184\n",
      "Epoch 93, Loss: 38.54400950211745\n",
      "Epoch 94, Loss: 28.777898641733024\n",
      "Epoch 95, Loss: 23.778166697575497\n",
      "Epoch 96, Loss: 39.16369852652917\n",
      "Epoch 97, Loss: 26.96544727912316\n",
      "Epoch 98, Loss: 27.987028011908897\n",
      "Epoch 99, Loss: 20.812001411731426\n",
      "Epoch 100, Loss: 18.780090038593\n",
      "Epoch 101, Loss: 41.084621356083794\n",
      "Epoch 102, Loss: 56.5033980516287\n",
      "Epoch 103, Loss: 36.548218947190506\n",
      "Epoch 104, Loss: 33.806539902320274\n",
      "Epoch 105, Loss: 25.552530435415413\n",
      "Epoch 106, Loss: 21.871184202340935\n",
      "Epoch 107, Loss: 18.894481823994564\n",
      "Epoch 108, Loss: 32.255427635633026\n",
      "Epoch 109, Loss: 29.693137829120342\n",
      "Epoch 110, Loss: 23.125155925750732\n",
      "Epoch 111, Loss: 22.324807277092567\n",
      "Epoch 112, Loss: 19.43857295696552\n",
      "Epoch 113, Loss: 12.97025623688331\n",
      "Epoch 114, Loss: 10.056539865640493\n",
      "Epoch 115, Loss: 10.7245687521421\n",
      "Epoch 116, Loss: 11.246049807621883\n",
      "Epoch 117, Loss: 10.627569437026978\n",
      "Epoch 118, Loss: 11.067252709315373\n",
      "Epoch 119, Loss: 28.839074556644146\n",
      "Epoch 120, Loss: 22.333604702582726\n",
      "Epoch 121, Loss: 32.55958912922786\n",
      "Epoch 122, Loss: 36.90223554464487\n",
      "Epoch 123, Loss: 36.28971895804772\n",
      "Epoch 124, Loss: 31.522324488713192\n",
      "Epoch 125, Loss: 25.94459500679603\n",
      "Epoch 126, Loss: 23.417266882382908\n",
      "Epoch 127, Loss: 22.197659785930927\n",
      "Epoch 128, Loss: 15.127760502008291\n",
      "Epoch 129, Loss: 13.694083360525278\n",
      "Epoch 130, Loss: 13.115992050904493\n",
      "Epoch 131, Loss: 14.99879318017226\n",
      "Epoch 132, Loss: 21.314757163708027\n",
      "Epoch 133, Loss: 13.122289969370915\n",
      "Epoch 134, Loss: 12.142653538630558\n",
      "Epoch 135, Loss: 13.015581351060133\n",
      "Epoch 136, Loss: 11.663384180802565\n",
      "Epoch 137, Loss: 12.727798700332642\n",
      "Epoch 138, Loss: 22.548434330866886\n",
      "Epoch 139, Loss: 13.921260466942421\n",
      "Epoch 140, Loss: 14.679018222368681\n",
      "Epoch 141, Loss: 16.46819884960468\n",
      "Epoch 142, Loss: 21.577908754348755\n",
      "Epoch 143, Loss: 20.155116374676044\n",
      "Epoch 144, Loss: 17.52325223042415\n",
      "Epoch 145, Loss: 17.841968683096077\n",
      "Epoch 146, Loss: 16.181113499861496\n",
      "Epoch 147, Loss: 23.001255768996018\n",
      "Epoch 148, Loss: 27.139554042082565\n",
      "Epoch 149, Loss: 19.690440618074856\n",
      "Epoch 150, Loss: 21.717388446514423\n",
      "Epoch 151, Loss: 17.536341227017918\n",
      "Epoch 152, Loss: 21.00725060242873\n",
      "Epoch 153, Loss: 40.4499971316411\n",
      "Epoch 154, Loss: 26.524479755988487\n",
      "Epoch 155, Loss: 20.938667150644157\n",
      "Epoch 156, Loss: 14.763988348153921\n",
      "Epoch 157, Loss: 14.959901773012602\n",
      "Epoch 158, Loss: 13.289764330937313\n",
      "Epoch 159, Loss: 11.797279431269718\n",
      "Epoch 160, Loss: 14.108639772121723\n",
      "Epoch 161, Loss: 15.316718578338623\n",
      "Epoch 162, Loss: 10.289561179968027\n",
      "Epoch 163, Loss: 9.14126784984882\n",
      "Epoch 164, Loss: 10.298970350852379\n",
      "Epoch 165, Loss: 13.556000727873583\n",
      "Epoch 166, Loss: 17.246846914291382\n",
      "Epoch 167, Loss: 15.039839781247652\n",
      "Epoch 168, Loss: 11.662318596473106\n",
      "Epoch 169, Loss: 10.521555937253511\n",
      "Epoch 170, Loss: 11.738062014946571\n",
      "Epoch 171, Loss: 15.835092397836538\n",
      "Epoch 172, Loss: 32.310249695411095\n",
      "Epoch 173, Loss: 26.739719464228703\n",
      "Epoch 174, Loss: 20.925969197199894\n",
      "Epoch 175, Loss: 20.77211280969473\n",
      "Epoch 176, Loss: 28.33421046917255\n",
      "Epoch 177, Loss: 17.425839772591225\n",
      "Epoch 178, Loss: 18.00648670930129\n",
      "Epoch 179, Loss: 14.59564812366779\n",
      "Epoch 180, Loss: 33.582247458971466\n",
      "Epoch 181, Loss: 37.83515534034142\n",
      "Epoch 182, Loss: 33.65289409344013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:09,202] Trial 4 finished with value: 90.28951249731355 and parameters: {'latent_dim_z1': 42, 'latent_dim_z2': 18, 'hidden_dim': 195, 'epochs': 188, 'causal_reg': 0.9421189040522634, 'learning_rate': 0.020337060181796377}. Best is trial 4 with value: 90.28951249731355.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183, Loss: 33.52323275346022\n",
      "Epoch 184, Loss: 55.160719064565804\n",
      "Epoch 185, Loss: 42.631081067598785\n",
      "Epoch 186, Loss: 22.71520420221182\n",
      "Epoch 187, Loss: 29.57579102882972\n",
      "Epoch 188, Loss: 17.98022435261653\n",
      "Epoch 1, Loss: 2278878774.975539\n",
      "Epoch 2, Loss: 244904816944296.8\n",
      "Epoch 3, Loss: 80976.47295673077\n",
      "Epoch 4, Loss: 86385.75991586539\n",
      "Epoch 5, Loss: 87180.26096754808\n",
      "Epoch 6, Loss: 83624.54950420673\n",
      "Epoch 7, Loss: 76557.15294471153\n",
      "Epoch 8, Loss: 71479.88551682692\n",
      "Epoch 9, Loss: 66907.10757211539\n",
      "Epoch 10, Loss: 63156.79942908654\n",
      "Epoch 11, Loss: 60608.226111778844\n",
      "Epoch 12, Loss: 59091.10854867788\n",
      "Epoch 13, Loss: 58303.529822716344\n",
      "Epoch 14, Loss: 57787.724083533656\n",
      "Epoch 15, Loss: 57089.70935997596\n",
      "Epoch 16, Loss: 56298.05581430288\n",
      "Epoch 17, Loss: 55964.29672475962\n",
      "Epoch 18, Loss: 55483.191856971156\n",
      "Epoch 19, Loss: 56182.072716346156\n",
      "Epoch 20, Loss: 55261.01427283654\n",
      "Epoch 21, Loss: 54627.64047475962\n",
      "Epoch 22, Loss: 54295.29957932692\n",
      "Epoch 23, Loss: 54510.897761418266\n",
      "Epoch 24, Loss: 55260.71364182692\n",
      "Epoch 25, Loss: 55629.365534855766\n",
      "Epoch 26, Loss: 53324.75161508413\n",
      "Epoch 27, Loss: 72106.7431640625\n",
      "Epoch 28, Loss: 57290.06505408654\n",
      "Epoch 29, Loss: 57868.202223557695\n",
      "Epoch 30, Loss: 57445.010216346156\n",
      "Epoch 31, Loss: 57443.046799879805\n",
      "Epoch 32, Loss: 57191.269005408656\n",
      "Epoch 33, Loss: 57148.75916466346\n",
      "Epoch 34, Loss: 57184.4609375\n",
      "Epoch 35, Loss: 57041.95703125\n",
      "Epoch 36, Loss: 57222.5654296875\n",
      "Epoch 37, Loss: 56973.34157151442\n",
      "Epoch 38, Loss: 56531.21559495192\n",
      "Epoch 39, Loss: 56958.016902043266\n",
      "Epoch 40, Loss: 56768.85268930288\n",
      "Epoch 41, Loss: 56039.998722956734\n",
      "Epoch 42, Loss: 56194.20860877404\n",
      "Epoch 43, Loss: 56869.99233774038\n",
      "Epoch 44, Loss: 55862.648512620195\n",
      "Epoch 45, Loss: 55783.516075721156\n",
      "Epoch 46, Loss: 55669.44809194712\n",
      "Epoch 47, Loss: 56520.247971754805\n",
      "Epoch 48, Loss: 198644.59292367788\n",
      "Epoch 49, Loss: 65343.2646484375\n",
      "Epoch 50, Loss: 67096.60817307692\n",
      "Epoch 51, Loss: 67024.03207632211\n",
      "Epoch 52, Loss: 67576.95169771634\n",
      "Epoch 53, Loss: 68475.59194711539\n",
      "Epoch 54, Loss: 66918.72062800481\n",
      "Epoch 55, Loss: 68448.41526442308\n",
      "Epoch 56, Loss: 71784.17022235577\n",
      "Epoch 57, Loss: 71500.18359375\n",
      "Epoch 58, Loss: 72111.34239783653\n",
      "Epoch 59, Loss: 74898.5615234375\n",
      "Epoch 60, Loss: 74041.68727463942\n",
      "Epoch 61, Loss: 72590.09299879808\n",
      "Epoch 62, Loss: 73712.11403245192\n",
      "Epoch 63, Loss: 72023.78861177884\n",
      "Epoch 64, Loss: 72649.7412109375\n",
      "Epoch 65, Loss: 73080.13882211539\n",
      "Epoch 66, Loss: 72820.91586538461\n",
      "Epoch 67, Loss: 71768.28185096153\n",
      "Epoch 68, Loss: 70904.98722956731\n",
      "Epoch 69, Loss: 70052.84652944711\n",
      "Epoch 70, Loss: 69452.24391526442\n",
      "Epoch 71, Loss: 69976.91180889423\n",
      "Epoch 72, Loss: 68974.63701923077\n",
      "Epoch 73, Loss: 68769.72806490384\n",
      "Epoch 74, Loss: 70122.3662109375\n",
      "Epoch 75, Loss: 73699.16225961539\n",
      "Epoch 76, Loss: 74680.62101862981\n",
      "Epoch 77, Loss: 73576.24451622597\n",
      "Epoch 78, Loss: 74265.14235276442\n",
      "Epoch 79, Loss: 73056.18712439903\n",
      "Epoch 80, Loss: 73728.62890625\n",
      "Epoch 81, Loss: 80028.57173978366\n",
      "Epoch 82, Loss: 77103.62259615384\n",
      "Epoch 83, Loss: 74166.40985576923\n",
      "Epoch 84, Loss: 72856.18772536058\n",
      "Epoch 85, Loss: 71053.76682692308\n",
      "Epoch 86, Loss: 69475.38258713942\n",
      "Epoch 87, Loss: 68470.2294921875\n",
      "Epoch 88, Loss: 79820805.02621695\n",
      "Epoch 89, Loss: 71600.66962139423\n",
      "Epoch 90, Loss: 72236.48963341347\n",
      "Epoch 91, Loss: 72057.91098257211\n",
      "Epoch 92, Loss: 71941.84698016827\n",
      "Epoch 93, Loss: 71876.93171574519\n",
      "Epoch 94, Loss: 71922.58879206731\n",
      "Epoch 95, Loss: 71346.1865234375\n",
      "Epoch 96, Loss: 71077.89039963942\n",
      "Epoch 97, Loss: 71647.79281850961\n",
      "Epoch 98, Loss: 71150.24692007211\n",
      "Epoch 99, Loss: 73370.25856370192\n",
      "Epoch 100, Loss: 80384672.88423978\n",
      "Epoch 101, Loss: 84598.27043269231\n",
      "Epoch 102, Loss: 83538.84329927884\n",
      "Epoch 103, Loss: 85975.49631911058\n",
      "Epoch 104, Loss: 86007.27764423077\n",
      "Epoch 105, Loss: 89827.43539663461\n",
      "Epoch 106, Loss: 91293.48692908653\n",
      "Epoch 107, Loss: 94239.86583533653\n",
      "Epoch 108, Loss: 97628.92427884616\n",
      "Epoch 109, Loss: 96031.40827824519\n",
      "Epoch 110, Loss: 94840.91233473558\n",
      "Epoch 111, Loss: 93470.82046274039\n",
      "Epoch 112, Loss: 92637.75465745192\n",
      "Epoch 113, Loss: 91613.46739783653\n",
      "Epoch 114, Loss: 90875.89948918269\n",
      "Epoch 115, Loss: 312098.7231069711\n",
      "Epoch 116, Loss: 95646.16346153847\n",
      "Epoch 117, Loss: 2800787.4747596155\n",
      "Epoch 118, Loss: 102276.71950120192\n",
      "Epoch 119, Loss: 101475.63266225961\n",
      "Epoch 120, Loss: 100764.00570913461\n",
      "Epoch 121, Loss: 100029.99579326923\n",
      "Epoch 122, Loss: 100554.68103966347\n",
      "Epoch 123, Loss: 154167.88221153847\n",
      "Epoch 124, Loss: 105355.76592548077\n",
      "Epoch 125, Loss: 110305.27817007211\n",
      "Epoch 126, Loss: 111078.28966346153\n",
      "Epoch 127, Loss: 109464.99263822116\n",
      "Epoch 128, Loss: 109830.05844350961\n",
      "Epoch 129, Loss: 113407.56069711539\n",
      "Epoch 130, Loss: 123460.39603365384\n",
      "Epoch 131, Loss: 117688.42112379808\n",
      "Epoch 132, Loss: 121138.81355168269\n",
      "Epoch 133, Loss: 118501.51186899039\n",
      "Epoch 134, Loss: 115073.19486177884\n",
      "Epoch 135, Loss: 112295.32542067308\n",
      "Epoch 136, Loss: 111552.37439903847\n",
      "Epoch 137, Loss: 111531.40504807692\n",
      "Epoch 138, Loss: 109086.712890625\n",
      "Epoch 139, Loss: 108174.29507211539\n",
      "Epoch 140, Loss: 109236.93810096153\n",
      "Epoch 141, Loss: 107866.91721754808\n",
      "Epoch 142, Loss: 107164.81745793269\n",
      "Epoch 143, Loss: 152619.82016225962\n",
      "Epoch 144, Loss: 109955.79326923077\n",
      "Epoch 145, Loss: 111539.51231971153\n",
      "Epoch 146, Loss: 116896.31550480769\n",
      "Epoch 147, Loss: 117809.41045673077\n",
      "Epoch 148, Loss: 118033.15985576923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:12,861] Trial 5 finished with value: 1580046912071.3708 and parameters: {'latent_dim_z1': 34, 'latent_dim_z2': 34, 'hidden_dim': 228, 'epochs': 155, 'causal_reg': 0.2955546692290027, 'learning_rate': 0.04453812176423834}. Best is trial 4 with value: 90.28951249731355.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149, Loss: 117774.630859375\n",
      "Epoch 150, Loss: 117054.38716947116\n",
      "Epoch 151, Loss: 117058.90054086539\n",
      "Epoch 152, Loss: 122035.34164663461\n",
      "Epoch 153, Loss: 121656.17623197116\n",
      "Epoch 154, Loss: 122981.49338942308\n",
      "Epoch 155, Loss: 123252.77524038461\n",
      "Epoch 1, Loss: 402.63443286602313\n",
      "Epoch 2, Loss: 172.7365361727201\n",
      "Epoch 3, Loss: 133.27917010967548\n",
      "Epoch 4, Loss: 106.34174889784593\n",
      "Epoch 5, Loss: 94.69739547142616\n",
      "Epoch 6, Loss: 91.09896835914024\n",
      "Epoch 7, Loss: 78.26773819556603\n",
      "Epoch 8, Loss: 68.26118322519156\n",
      "Epoch 9, Loss: 55.12877684373122\n",
      "Epoch 10, Loss: 55.63178884066068\n",
      "Epoch 11, Loss: 50.10300922393799\n",
      "Epoch 12, Loss: 41.591386574965256\n",
      "Epoch 13, Loss: 37.75164875617394\n",
      "Epoch 14, Loss: 37.19688059733464\n",
      "Epoch 15, Loss: 34.55017273242657\n",
      "Epoch 16, Loss: 37.71791692880484\n",
      "Epoch 17, Loss: 35.15885932628925\n",
      "Epoch 18, Loss: 30.999783295851486\n",
      "Epoch 19, Loss: 29.34338140487671\n",
      "Epoch 20, Loss: 27.831798993624172\n",
      "Epoch 21, Loss: 26.742890798128567\n",
      "Epoch 22, Loss: 27.389575994931736\n",
      "Epoch 23, Loss: 22.999455763743473\n",
      "Epoch 24, Loss: 25.471310432140644\n",
      "Epoch 25, Loss: 19.345979103675255\n",
      "Epoch 26, Loss: 16.88410942371075\n",
      "Epoch 27, Loss: 15.935244927039513\n",
      "Epoch 28, Loss: 18.882600637582634\n",
      "Epoch 29, Loss: 26.745886912712685\n",
      "Epoch 30, Loss: 37.66084040128268\n",
      "Epoch 31, Loss: 27.532089930314285\n",
      "Epoch 32, Loss: 22.937038274911735\n",
      "Epoch 33, Loss: 25.614382010239822\n",
      "Epoch 34, Loss: 17.472480957324688\n",
      "Epoch 35, Loss: 14.51567323391254\n",
      "Epoch 36, Loss: 14.52477765083313\n",
      "Epoch 37, Loss: 13.075659586833073\n",
      "Epoch 38, Loss: 11.725284503056454\n",
      "Epoch 39, Loss: 10.578604698181152\n",
      "Epoch 40, Loss: 13.028702955979567\n",
      "Epoch 41, Loss: 20.33494127713717\n",
      "Epoch 42, Loss: 29.201360849233772\n",
      "Epoch 43, Loss: 17.061632761588463\n",
      "Epoch 44, Loss: 17.400498756995567\n",
      "Epoch 45, Loss: 13.147464422079233\n",
      "Epoch 46, Loss: 12.623592193310078\n",
      "Epoch 47, Loss: 14.238267770180336\n",
      "Epoch 48, Loss: 13.550330125368559\n",
      "Epoch 49, Loss: 12.594884982475868\n",
      "Epoch 50, Loss: 12.679758420357338\n",
      "Epoch 51, Loss: 11.455591770318838\n",
      "Epoch 52, Loss: 10.822648653617271\n",
      "Epoch 53, Loss: 15.021492371192345\n",
      "Epoch 54, Loss: 12.345030271089994\n",
      "Epoch 55, Loss: 9.427718694393452\n",
      "Epoch 56, Loss: 10.086140926067646\n",
      "Epoch 57, Loss: 9.768072256675133\n",
      "Epoch 58, Loss: 9.325910109740038\n",
      "Epoch 59, Loss: 9.311819993532621\n",
      "Epoch 60, Loss: 9.02050124681913\n",
      "Epoch 61, Loss: 9.105623960494995\n",
      "Epoch 62, Loss: 8.383534596516537\n",
      "Epoch 63, Loss: 9.003132490011362\n",
      "Epoch 64, Loss: 10.87493476500878\n",
      "Epoch 65, Loss: 10.373178261976976\n",
      "Epoch 66, Loss: 9.05855996792133\n",
      "Epoch 67, Loss: 9.148511208020723\n",
      "Epoch 68, Loss: 9.49376993912917\n",
      "Epoch 69, Loss: 8.631055666850163\n",
      "Epoch 70, Loss: 10.170581047351543\n",
      "Epoch 71, Loss: 9.303624354876005\n",
      "Epoch 72, Loss: 9.487849107155434\n",
      "Epoch 73, Loss: 12.722751158934374\n",
      "Epoch 74, Loss: 14.035940683805025\n",
      "Epoch 75, Loss: 18.52857758448674\n",
      "Epoch 76, Loss: 15.326625970693735\n",
      "Epoch 77, Loss: 14.81082272529602\n",
      "Epoch 78, Loss: 13.878414337451641\n",
      "Epoch 79, Loss: 20.776068687438965\n",
      "Epoch 80, Loss: 27.48083705168504\n",
      "Epoch 81, Loss: 24.970346230726975\n",
      "Epoch 82, Loss: 14.783382048973671\n",
      "Epoch 83, Loss: 11.34206801194411\n",
      "Epoch 84, Loss: 9.60478452535776\n",
      "Epoch 85, Loss: 8.247159719467163\n",
      "Epoch 86, Loss: 8.440753368230967\n",
      "Epoch 87, Loss: 8.019135255080004\n",
      "Epoch 88, Loss: 7.561509792621319\n",
      "Epoch 89, Loss: 7.662189520322359\n",
      "Epoch 90, Loss: 7.4818945481227\n",
      "Epoch 91, Loss: 7.488340377807617\n",
      "Epoch 92, Loss: 7.467284752772405\n",
      "Epoch 93, Loss: 7.4753729196695184\n",
      "Epoch 94, Loss: 10.89323328091548\n",
      "Epoch 95, Loss: 11.663245017711933\n",
      "Epoch 96, Loss: 13.01702569081233\n",
      "Epoch 97, Loss: 18.286107320051926\n",
      "Epoch 98, Loss: 12.800398991658138\n",
      "Epoch 99, Loss: 9.444431396631094\n",
      "Epoch 100, Loss: 8.734939171717716\n",
      "Epoch 101, Loss: 7.628451402370747\n",
      "Epoch 102, Loss: 7.309585699668298\n",
      "Epoch 103, Loss: 7.586307543974656\n",
      "Epoch 104, Loss: 7.636940057461079\n",
      "Epoch 105, Loss: 8.16597060056833\n",
      "Epoch 106, Loss: 7.4810985418466425\n",
      "Epoch 107, Loss: 7.5706867804894085\n",
      "Epoch 108, Loss: 7.578223393513606\n",
      "Epoch 109, Loss: 7.47311205130357\n",
      "Epoch 110, Loss: 7.474933899365938\n",
      "Epoch 111, Loss: 7.685438339526836\n",
      "Epoch 112, Loss: 8.076632774793184\n",
      "Epoch 113, Loss: 8.47495119388287\n",
      "Epoch 114, Loss: 7.832524886498084\n",
      "Epoch 115, Loss: 7.830316286820632\n",
      "Epoch 116, Loss: 8.197048388994657\n",
      "Epoch 117, Loss: 8.07750520339379\n",
      "Epoch 118, Loss: 9.465450800382174\n",
      "Epoch 119, Loss: 15.07874716245211\n",
      "Epoch 120, Loss: 18.719490418067345\n",
      "Epoch 121, Loss: 13.836179586557241\n",
      "Epoch 122, Loss: 14.042283076506395\n",
      "Epoch 123, Loss: 12.153678784003624\n",
      "Epoch 124, Loss: 9.772216888574453\n",
      "Epoch 125, Loss: 9.944919237723717\n",
      "Epoch 126, Loss: 10.227810823000395\n",
      "Epoch 127, Loss: 9.738804285342876\n",
      "Epoch 128, Loss: 10.792864175943228\n",
      "Epoch 129, Loss: 13.521910355641293\n",
      "Epoch 130, Loss: 11.195914030075073\n",
      "Epoch 131, Loss: 9.129669592930721\n",
      "Epoch 132, Loss: 7.941932201385498\n",
      "Epoch 133, Loss: 8.250912739680363\n",
      "Epoch 134, Loss: 8.781134953865639\n",
      "Epoch 135, Loss: 8.043279812886166\n",
      "Epoch 136, Loss: 7.634934021876409\n",
      "Epoch 137, Loss: 8.744960216375498\n",
      "Epoch 138, Loss: 8.920807379942675\n",
      "Epoch 139, Loss: 8.497461649087759\n",
      "Epoch 140, Loss: 8.304118926708515\n",
      "Epoch 141, Loss: 7.417926128094013\n",
      "Epoch 142, Loss: 6.996323200372549\n",
      "Epoch 143, Loss: 6.867489374600924\n",
      "Epoch 144, Loss: 6.891232857337365\n",
      "Epoch 145, Loss: 7.367786169052124\n",
      "Epoch 146, Loss: 7.013856851137602\n",
      "Epoch 147, Loss: 6.666603932013879\n",
      "Epoch 148, Loss: 6.793805305774395\n",
      "Epoch 149, Loss: 7.116430796109713\n",
      "Epoch 150, Loss: 7.091594530985906\n",
      "Epoch 151, Loss: 6.586764647410466\n",
      "Epoch 152, Loss: 6.505729748652532\n",
      "Epoch 153, Loss: 7.106523513793945\n",
      "Epoch 154, Loss: 7.555608309232271\n",
      "Epoch 155, Loss: 7.739708221875704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:15,985] Trial 6 finished with value: 20.632456993139712 and parameters: {'latent_dim_z1': 18, 'latent_dim_z2': 17, 'hidden_dim': 117, 'epochs': 160, 'causal_reg': 0.1416053056120301, 'learning_rate': 0.0036385016891138733}. Best is trial 6 with value: 20.632456993139712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156, Loss: 8.856557130813599\n",
      "Epoch 157, Loss: 12.290579098921556\n",
      "Epoch 158, Loss: 11.812976837158203\n",
      "Epoch 159, Loss: 11.519617539185743\n",
      "Epoch 160, Loss: 11.395978175676786\n",
      "Epoch 1, Loss: 924.4794252835787\n",
      "Epoch 2, Loss: 424.20443021334137\n",
      "Epoch 3, Loss: 324.2605538001427\n",
      "Epoch 4, Loss: 277.27280484713043\n",
      "Epoch 5, Loss: 221.6039593036358\n",
      "Epoch 6, Loss: 197.62886194082407\n",
      "Epoch 7, Loss: 159.05768291766827\n",
      "Epoch 8, Loss: 148.60320516733023\n",
      "Epoch 9, Loss: 124.08161427424504\n",
      "Epoch 10, Loss: 142.65465604341946\n",
      "Epoch 11, Loss: 109.7120120708759\n",
      "Epoch 12, Loss: 95.00982563312238\n",
      "Epoch 13, Loss: 87.74257924006535\n",
      "Epoch 14, Loss: 76.7709820820735\n",
      "Epoch 15, Loss: 73.15455990571242\n",
      "Epoch 16, Loss: 59.235885033240685\n",
      "Epoch 17, Loss: 49.02436557182899\n",
      "Epoch 18, Loss: 46.327100643744835\n",
      "Epoch 19, Loss: 41.18991129214947\n",
      "Epoch 20, Loss: 36.44444788419283\n",
      "Epoch 21, Loss: 32.19851757929875\n",
      "Epoch 22, Loss: 25.393784743088943\n",
      "Epoch 23, Loss: 24.06796583762536\n",
      "Epoch 24, Loss: 27.76861176123986\n",
      "Epoch 25, Loss: 24.80488553413978\n",
      "Epoch 26, Loss: 25.580116051893967\n",
      "Epoch 27, Loss: 22.406916104830227\n",
      "Epoch 28, Loss: 19.852773042825554\n",
      "Epoch 29, Loss: 16.93744707107544\n",
      "Epoch 30, Loss: 15.520809723780705\n",
      "Epoch 31, Loss: 14.63707160949707\n",
      "Epoch 32, Loss: 14.589213004479042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:16,865] Trial 7 finished with value: 104.43808312599477 and parameters: {'latent_dim_z1': 46, 'latent_dim_z2': 39, 'hidden_dim': 216, 'epochs': 38, 'causal_reg': 0.32967613243939053, 'learning_rate': 0.0022170634177443352}. Best is trial 6 with value: 20.632456993139712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Loss: 14.663765760568472\n",
      "Epoch 34, Loss: 13.15205011001\n",
      "Epoch 35, Loss: 14.071596805865948\n",
      "Epoch 36, Loss: 13.859160533318153\n",
      "Epoch 37, Loss: 15.959946228907658\n",
      "Epoch 38, Loss: 14.723664137033316\n",
      "Epoch 1, Loss: 2597.1077833909253\n",
      "Epoch 2, Loss: 1847.3551424466646\n",
      "Epoch 3, Loss: 1404.8384798490083\n",
      "Epoch 4, Loss: 1090.33986722506\n",
      "Epoch 5, Loss: 928.7102625920222\n",
      "Epoch 6, Loss: 837.130130474384\n",
      "Epoch 7, Loss: 783.3211810772235\n",
      "Epoch 8, Loss: 737.9629845252404\n",
      "Epoch 9, Loss: 701.7197969876803\n",
      "Epoch 10, Loss: 670.3377412649302\n",
      "Epoch 11, Loss: 641.0921431321365\n",
      "Epoch 12, Loss: 614.3772571270282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:17,373] Trial 8 finished with value: 810.2294996168229 and parameters: {'latent_dim_z1': 57, 'latent_dim_z2': 60, 'hidden_dim': 135, 'epochs': 22, 'causal_reg': 0.8601236160363663, 'learning_rate': 0.00011215297555183706}. Best is trial 6 with value: 20.632456993139712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 588.7187329805814\n",
      "Epoch 14, Loss: 566.6624708909255\n",
      "Epoch 15, Loss: 544.6314984835111\n",
      "Epoch 16, Loss: 523.7827019324669\n",
      "Epoch 17, Loss: 503.72502077542816\n",
      "Epoch 18, Loss: 483.36143845778247\n",
      "Epoch 19, Loss: 465.23658282940204\n",
      "Epoch 20, Loss: 447.4756733820989\n",
      "Epoch 21, Loss: 430.35680506779596\n",
      "Epoch 22, Loss: 416.8052966778095\n",
      "Epoch 1, Loss: 2639.4288893479566\n",
      "Epoch 2, Loss: 2350.6741168682393\n",
      "Epoch 3, Loss: 2166.74899996244\n",
      "Epoch 4, Loss: 2032.8026310847356\n",
      "Epoch 5, Loss: 1917.7483520507812\n",
      "Epoch 6, Loss: 1809.1062387319712\n",
      "Epoch 7, Loss: 1701.3329350398137\n",
      "Epoch 8, Loss: 1591.8308598445012\n",
      "Epoch 9, Loss: 1485.9134333683894\n",
      "Epoch 10, Loss: 1388.2354278564453\n",
      "Epoch 11, Loss: 1303.4573951134314\n",
      "Epoch 12, Loss: 1229.502232478215\n",
      "Epoch 13, Loss: 1166.3841846172627\n",
      "Epoch 14, Loss: 1114.1440957876353\n",
      "Epoch 15, Loss: 1070.2759375939002\n",
      "Epoch 16, Loss: 1031.1913769061748\n",
      "Epoch 17, Loss: 998.9558563232422\n",
      "Epoch 18, Loss: 970.6253697321965\n",
      "Epoch 19, Loss: 946.159658578726\n",
      "Epoch 20, Loss: 923.507939852201\n",
      "Epoch 21, Loss: 903.8275897686298\n",
      "Epoch 22, Loss: 886.0966984675481\n",
      "Epoch 23, Loss: 870.1292489858774\n",
      "Epoch 24, Loss: 855.5287076509916\n",
      "Epoch 25, Loss: 841.3632208017202\n",
      "Epoch 26, Loss: 828.7018127441406\n",
      "Epoch 27, Loss: 816.4351513202374\n",
      "Epoch 28, Loss: 805.3186187744141\n",
      "Epoch 29, Loss: 794.8627483661359\n",
      "Epoch 30, Loss: 784.4342991755559\n",
      "Epoch 31, Loss: 774.7075917170598\n",
      "Epoch 32, Loss: 766.4033790001503\n",
      "Epoch 33, Loss: 756.5902762779823\n",
      "Epoch 34, Loss: 748.3314924973708\n",
      "Epoch 35, Loss: 740.0504784217247\n",
      "Epoch 36, Loss: 732.4447332528921\n",
      "Epoch 37, Loss: 724.3203782301682\n",
      "Epoch 38, Loss: 716.9558844933143\n",
      "Epoch 39, Loss: 709.6007643479568\n",
      "Epoch 40, Loss: 703.0813311063326\n",
      "Epoch 41, Loss: 695.9258475670448\n",
      "Epoch 42, Loss: 689.8234194242037\n",
      "Epoch 43, Loss: 682.6690081082858\n",
      "Epoch 44, Loss: 677.1887424175555\n",
      "Epoch 45, Loss: 670.6072998046875\n",
      "Epoch 46, Loss: 664.3846247746394\n",
      "Epoch 47, Loss: 658.3905234703651\n",
      "Epoch 48, Loss: 652.5919582660382\n",
      "Epoch 49, Loss: 647.6644838773287\n",
      "Epoch 50, Loss: 642.0078524076022\n",
      "Epoch 51, Loss: 635.8374657264122\n",
      "Epoch 52, Loss: 630.3972531832181\n",
      "Epoch 53, Loss: 625.4830791766827\n",
      "Epoch 54, Loss: 619.9149076021635\n",
      "Epoch 55, Loss: 615.0100860595703\n",
      "Epoch 56, Loss: 610.093132019043\n",
      "Epoch 57, Loss: 604.7527289757362\n",
      "Epoch 58, Loss: 600.3118644127479\n",
      "Epoch 59, Loss: 595.3364885770358\n",
      "Epoch 60, Loss: 591.3621069101187\n",
      "Epoch 61, Loss: 585.2159969623273\n",
      "Epoch 62, Loss: 580.9350339449369\n",
      "Epoch 63, Loss: 575.9425459641677\n",
      "Epoch 64, Loss: 571.4171477097732\n",
      "Epoch 65, Loss: 567.5610480675331\n",
      "Epoch 66, Loss: 562.8682708740234\n",
      "Epoch 67, Loss: 558.471928523137\n",
      "Epoch 68, Loss: 553.2131083561824\n",
      "Epoch 69, Loss: 548.8559670081505\n",
      "Epoch 70, Loss: 544.7746229905349\n",
      "Epoch 71, Loss: 540.3581748375526\n",
      "Epoch 72, Loss: 536.16553908128\n",
      "Epoch 73, Loss: 532.159174992488\n",
      "Epoch 74, Loss: 527.7090548001803\n",
      "Epoch 75, Loss: 524.1744807316707\n",
      "Epoch 76, Loss: 520.2604604867788\n",
      "Epoch 77, Loss: 515.862436734713\n",
      "Epoch 78, Loss: 511.96410487248346\n",
      "Epoch 79, Loss: 507.91491347092847\n",
      "Epoch 80, Loss: 503.4876550527719\n",
      "Epoch 81, Loss: 499.88929572472205\n",
      "Epoch 82, Loss: 496.3486985426683\n",
      "Epoch 83, Loss: 492.08646275446966\n",
      "Epoch 84, Loss: 488.60096388596753\n",
      "Epoch 85, Loss: 484.7683393038236\n",
      "Epoch 86, Loss: 480.87232501690204\n",
      "Epoch 87, Loss: 477.0451877300556\n",
      "Epoch 88, Loss: 473.4663584782527\n",
      "Epoch 89, Loss: 470.13439735999475\n",
      "Epoch 90, Loss: 466.0572005051833\n",
      "Epoch 91, Loss: 462.7882021390475\n",
      "Epoch 92, Loss: 459.17271892841046\n",
      "Epoch 93, Loss: 455.55284001277045\n",
      "Epoch 94, Loss: 452.30072197547327\n",
      "Epoch 95, Loss: 448.9304856520433\n",
      "Epoch 96, Loss: 445.35515946608325\n",
      "Epoch 97, Loss: 442.008796105018\n",
      "Epoch 98, Loss: 438.60875056340143\n",
      "Epoch 99, Loss: 435.8154208843525\n",
      "Epoch 100, Loss: 432.26739795391376\n",
      "Epoch 101, Loss: 429.0241529024564\n",
      "Epoch 102, Loss: 425.2757759094238\n",
      "Epoch 103, Loss: 422.08317213792066\n",
      "Epoch 104, Loss: 418.92299593411957\n",
      "Epoch 105, Loss: 415.7871451744667\n",
      "Epoch 106, Loss: 412.9714108980619\n",
      "Epoch 107, Loss: 409.1326458270733\n",
      "Epoch 108, Loss: 406.41720111553485\n",
      "Epoch 109, Loss: 403.16253427358777\n",
      "Epoch 110, Loss: 399.84124168982873\n",
      "Epoch 111, Loss: 397.21131486159106\n",
      "Epoch 112, Loss: 393.7584070058969\n",
      "Epoch 113, Loss: 390.71641305776745\n",
      "Epoch 114, Loss: 387.6452349149264\n",
      "Epoch 115, Loss: 384.76688971886267\n",
      "Epoch 116, Loss: 381.66341282771185\n",
      "Epoch 117, Loss: 379.2400219257061\n",
      "Epoch 118, Loss: 375.7106217604417\n",
      "Epoch 119, Loss: 372.5647359994742\n",
      "Epoch 120, Loss: 369.75088559664215\n",
      "Epoch 121, Loss: 366.7181050227239\n",
      "Epoch 122, Loss: 363.78360572228064\n",
      "Epoch 123, Loss: 360.7912169236403\n",
      "Epoch 124, Loss: 358.30424411480243\n",
      "Epoch 125, Loss: 355.1762337317833\n",
      "Epoch 126, Loss: 352.5584109379695\n",
      "Epoch 127, Loss: 350.52198674128607\n",
      "Epoch 128, Loss: 346.8056831359863\n",
      "Epoch 129, Loss: 343.6389051584097\n",
      "Epoch 130, Loss: 341.01920582697943\n",
      "Epoch 131, Loss: 338.55891477144684\n",
      "Epoch 132, Loss: 335.71840873131384\n",
      "Epoch 133, Loss: 332.66359358567456\n",
      "Epoch 134, Loss: 330.3092281634991\n",
      "Epoch 135, Loss: 327.62725536639874\n",
      "Epoch 136, Loss: 324.65886453481824\n",
      "Epoch 137, Loss: 321.9892143836388\n",
      "Epoch 138, Loss: 319.2871539776142\n",
      "Epoch 139, Loss: 316.46593064528247\n",
      "Epoch 140, Loss: 314.02887667142426\n",
      "Epoch 141, Loss: 311.11229236309345\n",
      "Epoch 142, Loss: 309.1899459545429\n",
      "Epoch 143, Loss: 305.93778668917145\n",
      "Epoch 144, Loss: 303.39420142540564\n",
      "Epoch 145, Loss: 300.85769154475287\n",
      "Epoch 146, Loss: 298.1823384211614\n",
      "Epoch 147, Loss: 295.4816565880409\n",
      "Epoch 148, Loss: 292.9658003586989\n",
      "Epoch 149, Loss: 290.5525178175706\n",
      "Epoch 150, Loss: 288.80369919996997\n",
      "Epoch 151, Loss: 285.7607724116399\n",
      "Epoch 152, Loss: 283.0083307119516\n",
      "Epoch 153, Loss: 280.36246901292066\n",
      "Epoch 154, Loss: 278.2406190725473\n",
      "Epoch 155, Loss: 275.43317090548004\n",
      "Epoch 156, Loss: 273.12143736619214\n",
      "Epoch 157, Loss: 270.8391239459698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:20,713] Trial 9 finished with value: 608.3467259813854 and parameters: {'latent_dim_z1': 53, 'latent_dim_z2': 55, 'hidden_dim': 17, 'epochs': 165, 'causal_reg': 0.48057572938084586, 'learning_rate': 2.031499102318875e-05}. Best is trial 6 with value: 20.632456993139712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158, Loss: 268.1744173490084\n",
      "Epoch 159, Loss: 265.9233568631686\n",
      "Epoch 160, Loss: 263.17945113548865\n",
      "Epoch 161, Loss: 261.02738453791693\n",
      "Epoch 162, Loss: 258.40470534104566\n",
      "Epoch 163, Loss: 256.41549037053034\n",
      "Epoch 164, Loss: 254.09705118032602\n",
      "Epoch 165, Loss: 252.12822694044846\n",
      "Epoch 1, Loss: 3180.883735069862\n",
      "Epoch 2, Loss: 1864.0150334284856\n",
      "Epoch 3, Loss: 1335.4598212608923\n",
      "Epoch 4, Loss: 1145.901162954477\n",
      "Epoch 5, Loss: 1037.5842801607573\n",
      "Epoch 6, Loss: 958.8256307748647\n",
      "Epoch 7, Loss: 893.1065474290115\n",
      "Epoch 8, Loss: 837.780275785006\n",
      "Epoch 9, Loss: 780.0062913161057\n",
      "Epoch 10, Loss: 730.0885485135592\n",
      "Epoch 11, Loss: 685.9813690185547\n",
      "Epoch 12, Loss: 645.3086899977463\n",
      "Epoch 13, Loss: 615.143437899076\n",
      "Epoch 14, Loss: 571.4182492769681\n",
      "Epoch 15, Loss: 539.6045379638672\n",
      "Epoch 16, Loss: 500.1000982431265\n",
      "Epoch 17, Loss: 470.9479775061974\n",
      "Epoch 18, Loss: 446.8193570650541\n",
      "Epoch 19, Loss: 413.0381179222694\n",
      "Epoch 20, Loss: 387.20437974196216\n",
      "Epoch 21, Loss: 358.83886073185846\n",
      "Epoch 22, Loss: 330.8011832604042\n",
      "Epoch 23, Loss: 307.62306477473334\n",
      "Epoch 24, Loss: 286.2558410351093\n",
      "Epoch 25, Loss: 264.8156182215764\n",
      "Epoch 26, Loss: 246.42830834021936\n",
      "Epoch 27, Loss: 231.3241682786208\n",
      "Epoch 28, Loss: 210.93057544414813\n",
      "Epoch 29, Loss: 195.91530345036432\n",
      "Epoch 30, Loss: 180.93096967843863\n",
      "Epoch 31, Loss: 168.52104135660025\n",
      "Epoch 32, Loss: 156.81564213679388\n",
      "Epoch 33, Loss: 145.5865082373986\n",
      "Epoch 34, Loss: 133.7865059192364\n",
      "Epoch 35, Loss: 123.83416924109825\n",
      "Epoch 36, Loss: 115.51812377342812\n",
      "Epoch 37, Loss: 107.81407077495868\n",
      "Epoch 38, Loss: 100.66572937598595\n",
      "Epoch 39, Loss: 95.9274813578679\n",
      "Epoch 40, Loss: 88.615265919612\n",
      "Epoch 41, Loss: 81.54096618065468\n",
      "Epoch 42, Loss: 75.99717360276442\n",
      "Epoch 43, Loss: 72.00293805049016\n",
      "Epoch 44, Loss: 66.27213874230019\n",
      "Epoch 45, Loss: 61.962534831120415\n",
      "Epoch 46, Loss: 59.29604324927697\n",
      "Epoch 47, Loss: 54.79473920968863\n",
      "Epoch 48, Loss: 51.43756602360652\n",
      "Epoch 49, Loss: 48.35427695054274\n",
      "Epoch 50, Loss: 45.50803411923922\n",
      "Epoch 51, Loss: 42.94720194889949\n",
      "Epoch 52, Loss: 40.7301721572876\n",
      "Epoch 53, Loss: 39.02896184187669\n",
      "Epoch 54, Loss: 36.77113195566031\n",
      "Epoch 55, Loss: 35.01410465974074\n",
      "Epoch 56, Loss: 33.174787741440994\n",
      "Epoch 57, Loss: 31.886171120863693\n",
      "Epoch 58, Loss: 30.68751698273879\n",
      "Epoch 59, Loss: 29.118316063514122\n",
      "Epoch 60, Loss: 27.678718713613655\n",
      "Epoch 61, Loss: 26.326406735640305\n",
      "Epoch 62, Loss: 25.510345679063064\n",
      "Epoch 63, Loss: 24.5630610539363\n",
      "Epoch 64, Loss: 23.533331100757305\n",
      "Epoch 65, Loss: 23.00639981489915\n",
      "Epoch 66, Loss: 21.842272391686073\n",
      "Epoch 67, Loss: 21.008951517251823\n",
      "Epoch 68, Loss: 20.335277557373047\n",
      "Epoch 69, Loss: 19.665335251734806\n",
      "Epoch 70, Loss: 19.073080209585335\n",
      "Epoch 71, Loss: 18.376093717721794\n",
      "Epoch 72, Loss: 17.83079382089468\n",
      "Epoch 73, Loss: 17.32009983062744\n",
      "Epoch 74, Loss: 16.894697996286247\n",
      "Epoch 75, Loss: 16.419908633598915\n",
      "Epoch 76, Loss: 15.94759306540856\n",
      "Epoch 77, Loss: 15.656640492952787\n",
      "Epoch 78, Loss: 15.397818565368652\n",
      "Epoch 79, Loss: 14.88221131838285\n",
      "Epoch 80, Loss: 14.46939310660729\n",
      "Epoch 81, Loss: 14.15468480036809\n",
      "Epoch 82, Loss: 13.824447595156157\n",
      "Epoch 83, Loss: 13.47767397073599\n",
      "Epoch 84, Loss: 13.192314661466158\n",
      "Epoch 85, Loss: 12.96747640463022\n",
      "Epoch 86, Loss: 12.718073147993822\n",
      "Epoch 87, Loss: 12.46987038392287\n",
      "Epoch 88, Loss: 12.343402404051561\n",
      "Epoch 89, Loss: 12.134402972001295\n",
      "Epoch 90, Loss: 11.833618017343374\n",
      "Epoch 91, Loss: 11.633671613839956\n",
      "Epoch 92, Loss: 11.433173363025372\n",
      "Epoch 93, Loss: 11.272025988652157\n",
      "Epoch 94, Loss: 11.06057931826665\n",
      "Epoch 95, Loss: 10.893547718341534\n",
      "Epoch 96, Loss: 10.668671094454252\n",
      "Epoch 97, Loss: 10.542898049721352\n",
      "Epoch 98, Loss: 10.41833415398231\n",
      "Epoch 99, Loss: 10.276782567684467\n",
      "Epoch 100, Loss: 10.150817797734188\n",
      "Epoch 101, Loss: 10.0010307935568\n",
      "Epoch 102, Loss: 9.890551530397856\n",
      "Epoch 103, Loss: 9.71124322597797\n",
      "Epoch 104, Loss: 9.632580372003408\n",
      "Epoch 105, Loss: 9.495425664461576\n",
      "Epoch 106, Loss: 9.384526802943302\n",
      "Epoch 107, Loss: 9.311140739000761\n",
      "Epoch 108, Loss: 9.157369008431068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:23,449] Trial 10 finished with value: 210.6453843942056 and parameters: {'latent_dim_z1': 78, 'latent_dim_z2': 54, 'hidden_dim': 100, 'epochs': 112, 'causal_reg': 0.731151887350757, 'learning_rate': 0.00017327551762095833}. Best is trial 6 with value: 20.632456993139712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109, Loss: 9.065824582026554\n",
      "Epoch 110, Loss: 8.98746187870319\n",
      "Epoch 111, Loss: 8.888404607772827\n",
      "Epoch 112, Loss: 8.861848280980038\n",
      "Epoch 1, Loss: 410.1650349543645\n",
      "Epoch 2, Loss: 224.02328212444598\n",
      "Epoch 3, Loss: 142.47716287466196\n",
      "Epoch 4, Loss: 113.73464943812444\n",
      "Epoch 5, Loss: 99.59077233534593\n",
      "Epoch 6, Loss: 88.02912558042087\n",
      "Epoch 7, Loss: 80.20302002246564\n",
      "Epoch 8, Loss: 73.76616962139423\n",
      "Epoch 9, Loss: 68.5596259190486\n",
      "Epoch 10, Loss: 62.47338823171762\n",
      "Epoch 11, Loss: 58.29633617401123\n",
      "Epoch 12, Loss: 52.775143696711616\n",
      "Epoch 13, Loss: 49.61495406811054\n",
      "Epoch 14, Loss: 45.070889472961426\n",
      "Epoch 15, Loss: 41.6558256149292\n",
      "Epoch 16, Loss: 39.50716752272386\n",
      "Epoch 17, Loss: 37.96898005558894\n",
      "Epoch 18, Loss: 35.61446652045617\n",
      "Epoch 19, Loss: 30.488949408897987\n",
      "Epoch 20, Loss: 28.36471594296969\n",
      "Epoch 21, Loss: 26.443859760577862\n",
      "Epoch 22, Loss: 25.059178792513332\n",
      "Epoch 23, Loss: 24.99809191777156\n",
      "Epoch 24, Loss: 21.844364166259766\n",
      "Epoch 25, Loss: 20.881351764385517\n",
      "Epoch 26, Loss: 19.967105792118954\n",
      "Epoch 27, Loss: 19.45328165934636\n",
      "Epoch 28, Loss: 18.557792993692253\n",
      "Epoch 29, Loss: 17.905809072347786\n",
      "Epoch 30, Loss: 17.4503311927502\n",
      "Epoch 31, Loss: 16.144246046359722\n",
      "Epoch 32, Loss: 14.263264289269081\n",
      "Epoch 33, Loss: 13.017986554365892\n",
      "Epoch 34, Loss: 12.147951126098633\n",
      "Epoch 35, Loss: 11.713266079242413\n",
      "Epoch 36, Loss: 12.71329872424786\n",
      "Epoch 37, Loss: 11.485367444845346\n",
      "Epoch 38, Loss: 10.483286747565636\n",
      "Epoch 39, Loss: 10.40497325016902\n",
      "Epoch 40, Loss: 10.921337604522705\n",
      "Epoch 41, Loss: 10.454185174061703\n",
      "Epoch 42, Loss: 9.94235977759728\n",
      "Epoch 43, Loss: 9.361084681290846\n",
      "Epoch 44, Loss: 8.73913783293504\n",
      "Epoch 45, Loss: 8.373519218884981\n",
      "Epoch 46, Loss: 8.364308999134945\n",
      "Epoch 47, Loss: 8.443152812811045\n",
      "Epoch 48, Loss: 8.110685843687792\n",
      "Epoch 49, Loss: 7.993154489077055\n",
      "Epoch 50, Loss: 7.868473254717314\n",
      "Epoch 51, Loss: 7.6086790194878215\n",
      "Epoch 52, Loss: 7.4468641097729025\n",
      "Epoch 53, Loss: 7.482457417708177\n",
      "Epoch 54, Loss: 7.620965957641602\n",
      "Epoch 55, Loss: 7.828422234608577\n",
      "Epoch 56, Loss: 7.5557218331557054\n",
      "Epoch 57, Loss: 7.314004567953257\n",
      "Epoch 58, Loss: 7.2627327808967\n",
      "Epoch 59, Loss: 7.208859095206628\n",
      "Epoch 60, Loss: 7.0434800661527195\n",
      "Epoch 61, Loss: 6.779255023369422\n",
      "Epoch 62, Loss: 6.83012214073768\n",
      "Epoch 63, Loss: 7.779934387940627\n",
      "Epoch 64, Loss: 8.174940255972055\n",
      "Epoch 65, Loss: 7.380544148958647\n",
      "Epoch 66, Loss: 6.95098902628972\n",
      "Epoch 67, Loss: 6.952785290204561\n",
      "Epoch 68, Loss: 6.8693177149846\n",
      "Epoch 69, Loss: 6.650498408537644\n",
      "Epoch 70, Loss: 6.596068767400888\n",
      "Epoch 71, Loss: 6.4631275947277365\n",
      "Epoch 72, Loss: 6.450960104282085\n",
      "Epoch 73, Loss: 6.6672202073610745\n",
      "Epoch 74, Loss: 6.8028749869419975\n",
      "Epoch 75, Loss: 6.757005783227774\n",
      "Epoch 76, Loss: 6.778381384336031\n",
      "Epoch 77, Loss: 7.1205597290625935\n",
      "Epoch 78, Loss: 6.696657712642963\n",
      "Epoch 79, Loss: 6.379469468043401\n",
      "Epoch 80, Loss: 6.455531156980074\n",
      "Epoch 81, Loss: 6.646257345493023\n",
      "Epoch 82, Loss: 7.151443774883564\n",
      "Epoch 83, Loss: 8.008656189991878\n",
      "Epoch 84, Loss: 7.1746326226454515\n",
      "Epoch 85, Loss: 6.960985825611995\n",
      "Epoch 86, Loss: 6.834200033774743\n",
      "Epoch 87, Loss: 6.727471039845393\n",
      "Epoch 88, Loss: 6.770111028964703\n",
      "Epoch 89, Loss: 8.015942555207472\n",
      "Epoch 90, Loss: 8.78522984798138\n",
      "Epoch 91, Loss: 11.45876224224384\n",
      "Epoch 92, Loss: 9.944237122168907\n",
      "Epoch 93, Loss: 8.745319769932674\n",
      "Epoch 94, Loss: 8.02571727679326\n",
      "Epoch 95, Loss: 7.199291871144221\n",
      "Epoch 96, Loss: 6.681608328452477\n",
      "Epoch 97, Loss: 6.472570841129009\n",
      "Epoch 98, Loss: 6.356674836232112\n",
      "Epoch 99, Loss: 6.3673781981835\n",
      "Epoch 100, Loss: 6.206204065909753\n",
      "Epoch 101, Loss: 6.133655859873845\n",
      "Epoch 102, Loss: 6.130497327217689\n",
      "Epoch 103, Loss: 6.144953012466431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:25,572] Trial 11 finished with value: 24.22857839191741 and parameters: {'latent_dim_z1': 11, 'latent_dim_z2': 29, 'hidden_dim': 82, 'epochs': 108, 'causal_reg': 0.002627634671318324, 'learning_rate': 0.0007805268428404073}. Best is trial 6 with value: 20.632456993139712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104, Loss: 6.03150985791133\n",
      "Epoch 105, Loss: 5.997542454646184\n",
      "Epoch 106, Loss: 5.9817421069512005\n",
      "Epoch 107, Loss: 6.0016437493837795\n",
      "Epoch 108, Loss: 5.93793012545659\n",
      "Epoch 1, Loss: 429.7496132483849\n",
      "Epoch 2, Loss: 228.95322961073654\n",
      "Epoch 3, Loss: 158.66382774939905\n",
      "Epoch 4, Loss: 132.82417179987982\n",
      "Epoch 5, Loss: 114.70836932842548\n",
      "Epoch 6, Loss: 101.84798856881949\n",
      "Epoch 7, Loss: 91.61913813077487\n",
      "Epoch 8, Loss: 84.53413801926833\n",
      "Epoch 9, Loss: 78.90428190964919\n",
      "Epoch 10, Loss: 72.8601171053373\n",
      "Epoch 11, Loss: 65.39003915053148\n",
      "Epoch 12, Loss: 60.18130683898926\n",
      "Epoch 13, Loss: 55.76638016333947\n",
      "Epoch 14, Loss: 50.10265365013709\n",
      "Epoch 15, Loss: 47.093512755173904\n",
      "Epoch 16, Loss: 43.55191604907696\n",
      "Epoch 17, Loss: 40.850305850689224\n",
      "Epoch 18, Loss: 38.00469970703125\n",
      "Epoch 19, Loss: 35.28783541459303\n",
      "Epoch 20, Loss: 31.157332493708683\n",
      "Epoch 21, Loss: 29.644883449261005\n",
      "Epoch 22, Loss: 28.591879697946403\n",
      "Epoch 23, Loss: 26.460870082561787\n",
      "Epoch 24, Loss: 25.33834171295166\n",
      "Epoch 25, Loss: 24.280610488011288\n",
      "Epoch 26, Loss: 23.569580444922813\n",
      "Epoch 27, Loss: 22.255892130044792\n",
      "Epoch 28, Loss: 20.43777531843919\n",
      "Epoch 29, Loss: 19.334315079909103\n",
      "Epoch 30, Loss: 17.368956547517044\n",
      "Epoch 31, Loss: 16.82814722794753\n",
      "Epoch 32, Loss: 16.88193915440486\n",
      "Epoch 33, Loss: 18.547408177302433\n",
      "Epoch 34, Loss: 15.339232004605806\n",
      "Epoch 35, Loss: 14.859135004190298\n",
      "Epoch 36, Loss: 14.206526756286621\n",
      "Epoch 37, Loss: 13.505900639754076\n",
      "Epoch 38, Loss: 13.44044696367704\n",
      "Epoch 39, Loss: 12.363176052386944\n",
      "Epoch 40, Loss: 11.732831918276274\n",
      "Epoch 41, Loss: 11.22557806968689\n",
      "Epoch 42, Loss: 11.691400674673227\n",
      "Epoch 43, Loss: 11.220735696645884\n",
      "Epoch 44, Loss: 11.143902778625488\n",
      "Epoch 45, Loss: 10.665329144551205\n",
      "Epoch 46, Loss: 9.930649005449736\n",
      "Epoch 47, Loss: 9.968910474043627\n",
      "Epoch 48, Loss: 9.600613795793974\n",
      "Epoch 49, Loss: 9.166872097895695\n",
      "Epoch 50, Loss: 8.970616065538847\n",
      "Epoch 51, Loss: 8.998342275619507\n",
      "Epoch 52, Loss: 8.723285106512217\n",
      "Epoch 53, Loss: 8.321623453727135\n",
      "Epoch 54, Loss: 8.061960587134728\n",
      "Epoch 55, Loss: 8.695145020118126\n",
      "Epoch 56, Loss: 10.390707327769352\n",
      "Epoch 57, Loss: 8.796208931849552\n",
      "Epoch 58, Loss: 8.358612537384033\n",
      "Epoch 59, Loss: 8.318880319595337\n",
      "Epoch 60, Loss: 7.883785027724046\n",
      "Epoch 61, Loss: 7.965296525221604\n",
      "Epoch 62, Loss: 7.997763340289776\n",
      "Epoch 63, Loss: 7.848378860033476\n",
      "Epoch 64, Loss: 8.249211219640879\n",
      "Epoch 65, Loss: 8.222659349441528\n",
      "Epoch 66, Loss: 8.117675469471859\n",
      "Epoch 67, Loss: 7.821544262079092\n",
      "Epoch 68, Loss: 7.361697967235859\n",
      "Epoch 69, Loss: 7.333927374619704\n",
      "Epoch 70, Loss: 7.30208683013916\n",
      "Epoch 71, Loss: 7.652047065588144\n",
      "Epoch 72, Loss: 7.234041782525869\n",
      "Epoch 73, Loss: 6.960180374292227\n",
      "Epoch 74, Loss: 6.706378881747906\n",
      "Epoch 75, Loss: 6.580960365442129\n",
      "Epoch 76, Loss: 6.568959419543926\n",
      "Epoch 77, Loss: 7.280369245089018\n",
      "Epoch 78, Loss: 7.571065627611601\n",
      "Epoch 79, Loss: 7.465719644839947\n",
      "Epoch 80, Loss: 7.242053857216468\n",
      "Epoch 81, Loss: 7.03358765748831\n",
      "Epoch 82, Loss: 7.868835394199078\n",
      "Epoch 83, Loss: 10.145090616666353\n",
      "Epoch 84, Loss: 9.45299975688641\n",
      "Epoch 85, Loss: 7.951393090761625\n",
      "Epoch 86, Loss: 7.111747448260967\n",
      "Epoch 87, Loss: 7.509125892932598\n",
      "Epoch 88, Loss: 7.0096232157487135\n",
      "Epoch 89, Loss: 7.329917412537795\n",
      "Epoch 90, Loss: 7.093668222427368\n",
      "Epoch 91, Loss: 6.809207017605122\n",
      "Epoch 92, Loss: 6.701173507250273\n",
      "Epoch 93, Loss: 6.542415490517249\n",
      "Epoch 94, Loss: 6.353857187124399\n",
      "Epoch 95, Loss: 6.281090351251455\n",
      "Epoch 96, Loss: 6.213252746141874\n",
      "Epoch 97, Loss: 6.257585837290837\n",
      "Epoch 98, Loss: 6.2713437630580025\n",
      "Epoch 99, Loss: 6.4185291070204515\n",
      "Epoch 100, Loss: 6.2880279284257155\n",
      "Epoch 101, Loss: 6.26044251368596\n",
      "Epoch 102, Loss: 6.135096036470854\n",
      "Epoch 103, Loss: 6.751717549103957\n",
      "Epoch 104, Loss: 7.44447370675894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:27,688] Trial 12 finished with value: 26.89605859764009 and parameters: {'latent_dim_z1': 12, 'latent_dim_z2': 29, 'hidden_dim': 85, 'epochs': 107, 'causal_reg': 0.021608975046453798, 'learning_rate': 0.0006964480752392756}. Best is trial 6 with value: 20.632456993139712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105, Loss: 7.301432737937341\n",
      "Epoch 106, Loss: 7.141944261697622\n",
      "Epoch 107, Loss: 7.54680925149184\n",
      "Epoch 1, Loss: 591.0972536527194\n",
      "Epoch 2, Loss: 260.0697130056528\n",
      "Epoch 3, Loss: 179.37255653968225\n",
      "Epoch 4, Loss: 175.98898740915152\n",
      "Epoch 5, Loss: 148.5098739037147\n",
      "Epoch 6, Loss: 119.75097370147705\n",
      "Epoch 7, Loss: 117.86606245774489\n",
      "Epoch 8, Loss: 101.67926069406363\n",
      "Epoch 9, Loss: 106.77330075777493\n",
      "Epoch 10, Loss: 99.12114158043495\n",
      "Epoch 11, Loss: 81.32180844820462\n",
      "Epoch 12, Loss: 74.18405598860521\n",
      "Epoch 13, Loss: 75.00823152982272\n",
      "Epoch 14, Loss: 85.84306394136868\n",
      "Epoch 15, Loss: 73.42259641794058\n",
      "Epoch 16, Loss: 56.57614315473116\n",
      "Epoch 17, Loss: 58.80703838054951\n",
      "Epoch 18, Loss: 56.90095116541936\n",
      "Epoch 19, Loss: 53.72902507048387\n",
      "Epoch 20, Loss: 55.44451273404635\n",
      "Epoch 21, Loss: 43.17574009528527\n",
      "Epoch 22, Loss: 48.001110810499924\n",
      "Epoch 23, Loss: 44.94137323819674\n",
      "Epoch 24, Loss: 44.0673163487361\n",
      "Epoch 25, Loss: 42.438856785114\n",
      "Epoch 26, Loss: 39.32344326606164\n",
      "Epoch 27, Loss: 32.85437231797438\n",
      "Epoch 28, Loss: 36.14634587214543\n",
      "Epoch 29, Loss: 47.43267125349779\n",
      "Epoch 30, Loss: 51.82976553990291\n",
      "Epoch 31, Loss: 67.18562962458684\n",
      "Epoch 32, Loss: 53.987199783325195\n",
      "Epoch 33, Loss: 32.50858915769137\n",
      "Epoch 34, Loss: 28.395413472102238\n",
      "Epoch 35, Loss: 28.561671550457294\n",
      "Epoch 36, Loss: 25.911657149975117\n",
      "Epoch 37, Loss: 25.125986869518574\n",
      "Epoch 38, Loss: 21.793540110954872\n",
      "Epoch 39, Loss: 26.231453198653\n",
      "Epoch 40, Loss: 26.766046340648945\n",
      "Epoch 41, Loss: 20.66691229893611\n",
      "Epoch 42, Loss: 19.088086421673115\n",
      "Epoch 43, Loss: 18.238649184887226\n",
      "Epoch 44, Loss: 19.089983463287354\n",
      "Epoch 45, Loss: 18.020144792703483\n",
      "Epoch 46, Loss: 19.35310781919039\n",
      "Epoch 47, Loss: 24.835876574883095\n",
      "Epoch 48, Loss: 23.263666299673226\n",
      "Epoch 49, Loss: 23.725310178903435\n",
      "Epoch 50, Loss: 21.1467044536884\n",
      "Epoch 51, Loss: 16.935158637853768\n",
      "Epoch 52, Loss: 20.829402520106388\n",
      "Epoch 53, Loss: 19.483215001913216\n",
      "Epoch 54, Loss: 18.955563141749455\n",
      "Epoch 55, Loss: 25.024542625133808\n",
      "Epoch 56, Loss: 18.257441208912777\n",
      "Epoch 57, Loss: 14.066871936504658\n",
      "Epoch 58, Loss: 12.487189146188589\n",
      "Epoch 59, Loss: 14.125985255608192\n",
      "Epoch 60, Loss: 12.49700823196998\n",
      "Epoch 61, Loss: 13.7143840606396\n",
      "Epoch 62, Loss: 13.990294309762808\n",
      "Epoch 63, Loss: 12.271702766418457\n",
      "Epoch 64, Loss: 12.345236943318294\n",
      "Epoch 65, Loss: 14.829568496117226\n",
      "Epoch 66, Loss: 15.736923767970158\n",
      "Epoch 67, Loss: 27.730911841759315\n",
      "Epoch 68, Loss: 24.167237281799316\n",
      "Epoch 69, Loss: 16.141466214106632\n",
      "Epoch 70, Loss: 16.669670471778282\n",
      "Epoch 71, Loss: 12.960639330056997\n",
      "Epoch 72, Loss: 14.31703831599309\n",
      "Epoch 73, Loss: 12.414779938184298\n",
      "Epoch 74, Loss: 11.764437638796293\n",
      "Epoch 75, Loss: 13.7133700297429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:29,157] Trial 13 finished with value: 52.18825377964298 and parameters: {'latent_dim_z1': 24, 'latent_dim_z2': 25, 'hidden_dim': 71, 'epochs': 76, 'causal_reg': 0.01734988558075143, 'learning_rate': 0.0058783963719017535}. Best is trial 6 with value: 20.632456993139712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, Loss: 15.304073333740234\n",
      "Epoch 1, Loss: 751.7726041353666\n",
      "Epoch 2, Loss: 417.4767946096567\n",
      "Epoch 3, Loss: 288.94740735567535\n",
      "Epoch 4, Loss: 238.64861503014197\n",
      "Epoch 5, Loss: 208.7255997290978\n",
      "Epoch 6, Loss: 186.63022276071402\n",
      "Epoch 7, Loss: 165.87474822998047\n",
      "Epoch 8, Loss: 150.33227627093976\n",
      "Epoch 9, Loss: 134.71693999950702\n",
      "Epoch 10, Loss: 121.84968963036171\n",
      "Epoch 11, Loss: 107.79503499544583\n",
      "Epoch 12, Loss: 94.48434888399564\n",
      "Epoch 13, Loss: 88.2060294518104\n",
      "Epoch 14, Loss: 76.94855822049655\n",
      "Epoch 15, Loss: 70.48372474083534\n",
      "Epoch 16, Loss: 63.00129707042988\n",
      "Epoch 17, Loss: 57.53316989311805\n",
      "Epoch 18, Loss: 52.92922812241774\n",
      "Epoch 19, Loss: 51.174686651963455\n",
      "Epoch 20, Loss: 44.2290975864117\n",
      "Epoch 21, Loss: 37.732281097998985\n",
      "Epoch 22, Loss: 37.90536792461689\n",
      "Epoch 23, Loss: 32.13105971996601\n",
      "Epoch 24, Loss: 29.56784593141996\n",
      "Epoch 25, Loss: 27.960694166330192\n",
      "Epoch 26, Loss: 23.954698764360867\n",
      "Epoch 27, Loss: 21.927054075094368\n",
      "Epoch 28, Loss: 19.806760127727802\n",
      "Epoch 29, Loss: 18.932741935436542\n",
      "Epoch 30, Loss: 17.83180020405696\n",
      "Epoch 31, Loss: 16.483637956472542\n",
      "Epoch 32, Loss: 15.456324430612417\n",
      "Epoch 33, Loss: 15.088141771463247\n",
      "Epoch 34, Loss: 13.799956871913029\n",
      "Epoch 35, Loss: 13.215132566598745\n",
      "Epoch 36, Loss: 12.404295389468853\n",
      "Epoch 37, Loss: 12.530227807851938\n",
      "Epoch 38, Loss: 11.653680031116192\n",
      "Epoch 39, Loss: 11.01550685442411\n",
      "Epoch 40, Loss: 10.393674960503212\n",
      "Epoch 41, Loss: 9.906228689047007\n",
      "Epoch 42, Loss: 10.10395310475276\n",
      "Epoch 43, Loss: 10.039516834112314\n",
      "Epoch 44, Loss: 9.584173624332134\n",
      "Epoch 45, Loss: 8.872963391817533\n",
      "Epoch 46, Loss: 8.373191943535438\n",
      "Epoch 47, Loss: 8.399092014019306\n",
      "Epoch 48, Loss: 8.838314056396484\n",
      "Epoch 49, Loss: 8.899193782072802\n",
      "Epoch 50, Loss: 8.266384033056406\n",
      "Epoch 51, Loss: 8.119207547261166\n",
      "Epoch 52, Loss: 8.106618129290068\n",
      "Epoch 53, Loss: 8.448886009363028\n",
      "Epoch 54, Loss: 8.28163693501399\n",
      "Epoch 55, Loss: 7.8972284977252665\n",
      "Epoch 56, Loss: 8.10466797535236\n",
      "Epoch 57, Loss: 7.843209266662598\n",
      "Epoch 58, Loss: 7.529719701180091\n",
      "Epoch 59, Loss: 7.521991949815017\n",
      "Epoch 60, Loss: 7.969703619296734\n",
      "Epoch 61, Loss: 7.909259631083562\n",
      "Epoch 62, Loss: 7.716837387818557\n",
      "Epoch 63, Loss: 7.2794012656578655\n",
      "Epoch 64, Loss: 7.056493594096257\n",
      "Epoch 65, Loss: 7.019338296009944\n",
      "Epoch 66, Loss: 7.348263502120972\n",
      "Epoch 67, Loss: 7.114404916763306\n",
      "Epoch 68, Loss: 7.34581811611469\n",
      "Epoch 69, Loss: 7.1535949523632345\n",
      "Epoch 70, Loss: 7.7122990901653585\n",
      "Epoch 71, Loss: 8.005182981491089\n",
      "Epoch 72, Loss: 8.626013040542603\n",
      "Epoch 73, Loss: 8.52941188445458\n",
      "Epoch 74, Loss: 10.110297569861778\n",
      "Epoch 75, Loss: 10.246292004218468\n",
      "Epoch 76, Loss: 11.358300887621366\n",
      "Epoch 77, Loss: 11.160759375645565\n",
      "Epoch 78, Loss: 11.508380064597496\n",
      "Epoch 79, Loss: 10.657969456452589\n",
      "Epoch 80, Loss: 9.017238451884342\n",
      "Epoch 81, Loss: 9.020522374373217\n",
      "Epoch 82, Loss: 8.344690047777616\n",
      "Epoch 83, Loss: 7.956000163004949\n",
      "Epoch 84, Loss: 7.8901770665095405\n",
      "Epoch 85, Loss: 7.66375363790072\n",
      "Epoch 86, Loss: 7.0366761134221\n",
      "Epoch 87, Loss: 7.354244140478281\n",
      "Epoch 88, Loss: 7.749601070697491\n",
      "Epoch 89, Loss: 7.991077239696796\n",
      "Epoch 90, Loss: 7.937864450307993\n",
      "Epoch 91, Loss: 7.275334394895113\n",
      "Epoch 92, Loss: 7.239409666794997\n",
      "Epoch 93, Loss: 6.867241437618549\n",
      "Epoch 94, Loss: 6.736997200892522\n",
      "Epoch 95, Loss: 6.840782825763409\n",
      "Epoch 96, Loss: 7.583294428311861\n",
      "Epoch 97, Loss: 7.17837511576139\n",
      "Epoch 98, Loss: 6.956672301659217\n",
      "Epoch 99, Loss: 7.954146770330576\n",
      "Epoch 100, Loss: 7.791993929789617\n",
      "Epoch 101, Loss: 8.928085437187782\n",
      "Epoch 102, Loss: 8.680292422954853\n",
      "Epoch 103, Loss: 9.421538994862484\n",
      "Epoch 104, Loss: 9.042552654559795\n",
      "Epoch 105, Loss: 8.379700348927425\n",
      "Epoch 106, Loss: 9.616530913573046\n",
      "Epoch 107, Loss: 9.71730351448059\n",
      "Epoch 108, Loss: 8.892235059004564\n",
      "Epoch 109, Loss: 7.98985583965595\n",
      "Epoch 110, Loss: 7.637599908388578\n",
      "Epoch 111, Loss: 7.93190754376925\n",
      "Epoch 112, Loss: 8.801378855338463\n",
      "Epoch 113, Loss: 8.441590914359459\n",
      "Epoch 114, Loss: 7.310990535295927\n",
      "Epoch 115, Loss: 6.95166121996366\n",
      "Epoch 116, Loss: 7.125969831760113\n",
      "Epoch 117, Loss: 8.157863360184889\n",
      "Epoch 118, Loss: 7.561861735123855\n",
      "Epoch 119, Loss: 7.497077978574312\n",
      "Epoch 120, Loss: 7.048557868370643\n",
      "Epoch 121, Loss: 7.454193555391752\n",
      "Epoch 122, Loss: 7.806574821472168\n",
      "Epoch 123, Loss: 8.96643930215102\n",
      "Epoch 124, Loss: 9.600144899808443\n",
      "Epoch 125, Loss: 8.361906069975634\n",
      "Epoch 126, Loss: 8.133593742664043\n",
      "Epoch 127, Loss: 8.843897746159481\n",
      "Epoch 128, Loss: 7.73599439400893\n",
      "Epoch 129, Loss: 7.749719986548791\n",
      "Epoch 130, Loss: 7.919459067858183\n",
      "Epoch 131, Loss: 7.691200953263503\n",
      "Epoch 132, Loss: 7.866415720719558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:31,995] Trial 14 finished with value: 32.95801705461742 and parameters: {'latent_dim_z1': 24, 'latent_dim_z2': 23, 'hidden_dim': 146, 'epochs': 139, 'causal_reg': 0.17955636452938273, 'learning_rate': 0.0006753031215593292}. Best is trial 6 with value: 20.632456993139712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133, Loss: 8.282993720127987\n",
      "Epoch 134, Loss: 7.662740523998554\n",
      "Epoch 135, Loss: 7.064933941914485\n",
      "Epoch 136, Loss: 6.91454632465656\n",
      "Epoch 137, Loss: 7.253589740166297\n",
      "Epoch 138, Loss: 7.117661971312303\n",
      "Epoch 139, Loss: 6.70256126843966\n",
      "Epoch 1, Loss: 650.5203707768367\n",
      "Epoch 2, Loss: 281.2606617854192\n",
      "Epoch 3, Loss: 206.59990428044244\n",
      "Epoch 4, Loss: 167.89124239408054\n",
      "Epoch 5, Loss: 140.89356583815353\n",
      "Epoch 6, Loss: 121.33064108628493\n",
      "Epoch 7, Loss: 109.8688414647029\n",
      "Epoch 8, Loss: 99.23271076495831\n",
      "Epoch 9, Loss: 84.3067211004404\n",
      "Epoch 10, Loss: 72.73263315054086\n",
      "Epoch 11, Loss: 62.89847051180326\n",
      "Epoch 12, Loss: 57.11741095322829\n",
      "Epoch 13, Loss: 49.49447419093205\n",
      "Epoch 14, Loss: 49.1625609764686\n",
      "Epoch 15, Loss: 42.79436184809758\n",
      "Epoch 16, Loss: 39.24597120285034\n",
      "Epoch 17, Loss: 35.868235734792854\n",
      "Epoch 18, Loss: 34.149502130655144\n",
      "Epoch 19, Loss: 40.76990795135498\n",
      "Epoch 20, Loss: 30.259289227999172\n",
      "Epoch 21, Loss: 25.18577289581299\n",
      "Epoch 22, Loss: 23.19601345062256\n",
      "Epoch 23, Loss: 33.94553558643047\n",
      "Epoch 24, Loss: 25.820442639864407\n",
      "Epoch 25, Loss: 21.113719023191013\n",
      "Epoch 26, Loss: 19.768669862013596\n",
      "Epoch 27, Loss: 19.855275557591366\n",
      "Epoch 28, Loss: 20.343697107755222\n",
      "Epoch 29, Loss: 16.716512863452618\n",
      "Epoch 30, Loss: 14.760108177478497\n",
      "Epoch 31, Loss: 13.488163232803345\n",
      "Epoch 32, Loss: 15.728648259089542\n",
      "Epoch 33, Loss: 14.364434058849628\n",
      "Epoch 34, Loss: 14.822780939248892\n",
      "Epoch 35, Loss: 15.013751873603233\n",
      "Epoch 36, Loss: 14.5270215181204\n",
      "Epoch 37, Loss: 13.253655910491943\n",
      "Epoch 38, Loss: 11.326196597172665\n",
      "Epoch 39, Loss: 11.008071624315702\n",
      "Epoch 40, Loss: 10.30437915141766\n",
      "Epoch 41, Loss: 10.316591519575853\n",
      "Epoch 42, Loss: 11.235614978350126\n",
      "Epoch 43, Loss: 11.470982203116783\n",
      "Epoch 44, Loss: 11.8082754611969\n",
      "Epoch 45, Loss: 10.179762766911434\n",
      "Epoch 46, Loss: 9.232043174596933\n",
      "Epoch 47, Loss: 13.134666442871094\n",
      "Epoch 48, Loss: 15.554344929181612\n",
      "Epoch 49, Loss: 11.156534066567055\n",
      "Epoch 50, Loss: 9.244099910442646\n",
      "Epoch 51, Loss: 9.402881493935219\n",
      "Epoch 52, Loss: 9.223891331599308\n",
      "Epoch 53, Loss: 9.748178078578068\n",
      "Epoch 54, Loss: 9.847216349381666\n",
      "Epoch 55, Loss: 9.928412675857544\n",
      "Epoch 56, Loss: 10.582375929905819\n",
      "Epoch 57, Loss: 10.772314640191885\n",
      "Epoch 58, Loss: 9.798774737578173\n",
      "Epoch 59, Loss: 9.08045352422274\n",
      "Epoch 60, Loss: 10.52504799916194\n",
      "Epoch 61, Loss: 10.248029158665585\n",
      "Epoch 62, Loss: 10.538996733151949\n",
      "Epoch 63, Loss: 9.832215877679678\n",
      "Epoch 64, Loss: 8.398689985275269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:33,585] Trial 15 finished with value: 41.66563550201623 and parameters: {'latent_dim_z1': 23, 'latent_dim_z2': 40, 'hidden_dim': 131, 'epochs': 74, 'causal_reg': 0.5351050282554045, 'learning_rate': 0.0013417207869369737}. Best is trial 6 with value: 20.632456993139712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, Loss: 7.52501471226032\n",
      "Epoch 66, Loss: 9.423265989010151\n",
      "Epoch 67, Loss: 10.09056436098539\n",
      "Epoch 68, Loss: 12.45760699418875\n",
      "Epoch 69, Loss: 11.877981992868277\n",
      "Epoch 70, Loss: 9.55503643476046\n",
      "Epoch 71, Loss: 9.867487852389996\n",
      "Epoch 72, Loss: 9.046090382796068\n",
      "Epoch 73, Loss: 10.782270339819101\n",
      "Epoch 74, Loss: 10.430986422758837\n",
      "Epoch 1, Loss: 372.8598505166861\n",
      "Epoch 2, Loss: 107.86583078824557\n",
      "Epoch 3, Loss: 90.60632016108586\n",
      "Epoch 4, Loss: 71.4526392863347\n",
      "Epoch 5, Loss: 58.71118406149057\n",
      "Epoch 6, Loss: 50.374595422011154\n",
      "Epoch 7, Loss: 54.15372811830961\n",
      "Epoch 8, Loss: 50.45382991203895\n",
      "Epoch 9, Loss: 47.64379090529222\n",
      "Epoch 10, Loss: 37.503695011138916\n",
      "Epoch 11, Loss: 35.50432300567627\n",
      "Epoch 12, Loss: 34.762169067676254\n",
      "Epoch 13, Loss: 28.435096117166374\n",
      "Epoch 14, Loss: 24.63129689143254\n",
      "Epoch 15, Loss: 24.159265151390663\n",
      "Epoch 16, Loss: 30.74380240073571\n",
      "Epoch 17, Loss: 24.43494099837083\n",
      "Epoch 18, Loss: 20.47597103852492\n",
      "Epoch 19, Loss: 25.223811039557823\n",
      "Epoch 20, Loss: 23.29513560808622\n",
      "Epoch 21, Loss: 19.377386349898117\n",
      "Epoch 22, Loss: 20.665691669170673\n",
      "Epoch 23, Loss: 20.648394731374886\n",
      "Epoch 24, Loss: 20.089558858137863\n",
      "Epoch 25, Loss: 16.169312440432034\n",
      "Epoch 26, Loss: 15.873852436359112\n",
      "Epoch 27, Loss: 16.367752295274002\n",
      "Epoch 28, Loss: 13.241932153701782\n",
      "Epoch 29, Loss: 12.360132951002855\n",
      "Epoch 30, Loss: 11.61709477351262\n",
      "Epoch 31, Loss: 12.29830081646259\n",
      "Epoch 32, Loss: 15.629441774808443\n",
      "Epoch 33, Loss: 16.1324573296767\n",
      "Epoch 34, Loss: 15.926155108671923\n",
      "Epoch 35, Loss: 14.971014022827148\n",
      "Epoch 36, Loss: 12.60556670335623\n",
      "Epoch 37, Loss: 14.77411020719088\n",
      "Epoch 38, Loss: 12.723971990438608\n",
      "Epoch 39, Loss: 11.191335971538837\n",
      "Epoch 40, Loss: 10.1436463869535\n",
      "Epoch 41, Loss: 11.36656166956975\n",
      "Epoch 42, Loss: 12.136746846712553\n",
      "Epoch 43, Loss: 13.097716624920185\n",
      "Epoch 44, Loss: 12.032467401944674\n",
      "Epoch 45, Loss: 10.13544777723459\n",
      "Epoch 46, Loss: 9.394169367276705\n",
      "Epoch 47, Loss: 11.70779506976788\n",
      "Epoch 48, Loss: 10.926885879956759\n",
      "Epoch 49, Loss: 14.057072639465332\n",
      "Epoch 50, Loss: 15.040365365835337\n",
      "Epoch 51, Loss: 15.017069082993727\n",
      "Epoch 52, Loss: 11.587555096699642\n",
      "Epoch 53, Loss: 11.00512902553265\n",
      "Epoch 54, Loss: 12.317317284070528\n",
      "Epoch 55, Loss: 10.225505883877094\n",
      "Epoch 56, Loss: 8.997964290472177\n",
      "Epoch 57, Loss: 10.572999734144945\n",
      "Epoch 58, Loss: 10.496998034990751\n",
      "Epoch 59, Loss: 11.314797401428223\n",
      "Epoch 60, Loss: 9.920429779933048\n",
      "Epoch 61, Loss: 8.89504495033851\n",
      "Epoch 62, Loss: 9.20360042498662\n",
      "Epoch 63, Loss: 11.168500203352709\n",
      "Epoch 64, Loss: 11.455061325660118\n",
      "Epoch 65, Loss: 11.922279816407423\n",
      "Epoch 66, Loss: 11.732460517149706\n",
      "Epoch 67, Loss: 11.807252737192007\n",
      "Epoch 68, Loss: 12.408635102785551\n",
      "Epoch 69, Loss: 10.719163051018349\n",
      "Epoch 70, Loss: 11.918487457128672\n",
      "Epoch 71, Loss: 11.470037827124962\n",
      "Epoch 72, Loss: 9.132956798260029\n",
      "Epoch 73, Loss: 12.253836705134464\n",
      "Epoch 74, Loss: 10.979942175058218\n",
      "Epoch 75, Loss: 11.20447221169105\n",
      "Epoch 76, Loss: 10.060417303672203\n",
      "Epoch 77, Loss: 9.795809672429012\n",
      "Epoch 78, Loss: 9.190831881303053\n",
      "Epoch 79, Loss: 8.511519377048199\n",
      "Epoch 80, Loss: 7.899388203254113\n",
      "Epoch 81, Loss: 8.364867540506216\n",
      "Epoch 82, Loss: 9.992712754469652\n",
      "Epoch 83, Loss: 9.526509101574238\n",
      "Epoch 84, Loss: 8.689340059573833\n",
      "Epoch 85, Loss: 8.55103556926434\n",
      "Epoch 86, Loss: 8.46933392378\n",
      "Epoch 87, Loss: 9.405746331581703\n",
      "Epoch 88, Loss: 13.139480994297909\n",
      "Epoch 89, Loss: 12.67660040121812\n",
      "Epoch 90, Loss: 14.10312507702754\n",
      "Epoch 91, Loss: 10.23740005493164\n",
      "Epoch 92, Loss: 9.171789279350868\n",
      "Epoch 93, Loss: 11.893042857830341\n",
      "Epoch 94, Loss: 13.213125467300415\n",
      "Epoch 95, Loss: 11.163980263930101\n",
      "Epoch 96, Loss: 8.628517609376173\n",
      "Epoch 97, Loss: 8.21814485696646\n",
      "Epoch 98, Loss: 8.560449306781475\n",
      "Epoch 99, Loss: 9.902197819489698\n",
      "Epoch 100, Loss: 9.026447241122906\n",
      "Epoch 101, Loss: 8.787401786217323\n",
      "Epoch 102, Loss: 8.317331020648663\n",
      "Epoch 103, Loss: 10.415003391412588\n",
      "Epoch 104, Loss: 11.016476484445425\n",
      "Epoch 105, Loss: 8.745573630699745\n",
      "Epoch 106, Loss: 12.440864067811232\n",
      "Epoch 107, Loss: 14.120946132219755\n",
      "Epoch 108, Loss: 11.023769360322218\n",
      "Epoch 109, Loss: 9.761420011520386\n",
      "Epoch 110, Loss: 11.724630612593431\n",
      "Epoch 111, Loss: 10.108998995560865\n",
      "Epoch 112, Loss: 8.828266950754019\n",
      "Epoch 113, Loss: 7.984345876253569\n",
      "Epoch 114, Loss: 8.4821706368373\n",
      "Epoch 115, Loss: 7.924246421227088\n",
      "Epoch 116, Loss: 7.650586238274207\n",
      "Epoch 117, Loss: 7.469255172289335\n",
      "Epoch 118, Loss: 7.963434347739587\n",
      "Epoch 119, Loss: 7.382700333228478\n",
      "Epoch 120, Loss: 7.368855348000159\n",
      "Epoch 121, Loss: 7.732140376017644\n",
      "Epoch 122, Loss: 7.288457962182852\n",
      "Epoch 123, Loss: 7.047230225342971\n",
      "Epoch 124, Loss: 7.457461448816153\n",
      "Epoch 125, Loss: 8.41587415108314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:36,508] Trial 16 finished with value: 18.69924799078573 and parameters: {'latent_dim_z1': 12, 'latent_dim_z2': 30, 'hidden_dim': 54, 'epochs': 131, 'causal_reg': 0.14218458799862063, 'learning_rate': 0.00780324437166997}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126, Loss: 8.036384124022264\n",
      "Epoch 127, Loss: 7.780317856715276\n",
      "Epoch 128, Loss: 7.345432153114905\n",
      "Epoch 129, Loss: 9.309606240345882\n",
      "Epoch 130, Loss: 15.238444328308105\n",
      "Epoch 131, Loss: 14.320938128691454\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 15:50:39,378] Trial 17 failed with parameters: {'latent_dim_z1': 32, 'latent_dim_z2': 47, 'hidden_dim': 58, 'epochs': 137, 'causal_reg': 0.2026140659312974, 'learning_rate': 0.09505874430728031} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 15:50:39,378] Trial 17 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 1, Loss: 942.1142325768104\n",
      "Epoch 2, Loss: 304.70925962007965\n",
      "Epoch 3, Loss: 227.0260043511024\n",
      "Epoch 4, Loss: 197.56313646756686\n",
      "Epoch 5, Loss: 173.02520077045148\n",
      "Epoch 6, Loss: 154.299016218919\n",
      "Epoch 7, Loss: 150.37442075289212\n",
      "Epoch 8, Loss: 145.24101095933182\n",
      "Epoch 9, Loss: 115.26718697181114\n",
      "Epoch 10, Loss: 105.79760888906625\n",
      "Epoch 11, Loss: 111.36698546776405\n",
      "Epoch 12, Loss: 110.37050951444186\n",
      "Epoch 13, Loss: 111.79932234837459\n",
      "Epoch 14, Loss: 94.7706521841196\n",
      "Epoch 15, Loss: 94.91627861903264\n",
      "Epoch 16, Loss: 79.62905957148625\n",
      "Epoch 17, Loss: 81.17376474233774\n",
      "Epoch 18, Loss: 78.19887322645921\n",
      "Epoch 19, Loss: 71.42346917665921\n",
      "Epoch 20, Loss: 68.46194648742676\n",
      "Epoch 21, Loss: 58.645563712486855\n",
      "Epoch 22, Loss: 65.52576241126427\n",
      "Epoch 23, Loss: 70.20447144141563\n",
      "Epoch 24, Loss: 66.9505763053894\n",
      "Epoch 25, Loss: 76.5167991564824\n",
      "Epoch 26, Loss: 55.175755940950836\n",
      "Epoch 27, Loss: 52.371858009925255\n",
      "Epoch 28, Loss: 46.36091349675105\n",
      "Epoch 29, Loss: 43.45423471010648\n",
      "Epoch 30, Loss: 53.52112491314228\n",
      "Epoch 31, Loss: 73.78318280440111\n",
      "Epoch 32, Loss: 47.209610792306755\n",
      "Epoch 33, Loss: 37.17190096928523\n",
      "Epoch 34, Loss: 37.03105464348426\n",
      "Epoch 35, Loss: 55.0570750603309\n",
      "Epoch 36, Loss: 36.35301843056312\n",
      "Epoch 37, Loss: 48.91032453683707\n",
      "Epoch 38, Loss: 46.3872504234314\n",
      "Epoch 39, Loss: 34.33925199508667\n",
      "Epoch 40, Loss: 27.54249198620136\n",
      "Epoch 41, Loss: 27.58595451941857\n",
      "Epoch 42, Loss: 27.479122198545017\n",
      "Epoch 43, Loss: 25.797505121964676\n",
      "Epoch 44, Loss: 23.073927512535683\n",
      "Epoch 45, Loss: 22.422954706045296\n",
      "Epoch 46, Loss: 29.573538523453934\n",
      "Epoch 47, Loss: 28.70171649639423\n",
      "Epoch 48, Loss: 28.976050230172966\n",
      "Epoch 49, Loss: 35.16716678325947\n",
      "Epoch 50, Loss: 38.641596207251915\n",
      "Epoch 51, Loss: 34.03828628246601\n",
      "Epoch 52, Loss: 26.932855386000412\n",
      "Epoch 53, Loss: 25.721654671889084\n",
      "Epoch 54, Loss: 21.18960901407095\n",
      "Epoch 55, Loss: 27.004441848168007\n",
      "Epoch 56, Loss: 31.295500718630276\n",
      "Epoch 57, Loss: 31.687277207007774\n",
      "Epoch 58, Loss: 26.22157984513503\n",
      "Epoch 59, Loss: 27.194634474240818\n",
      "Epoch 60, Loss: 19.56824695147001\n",
      "Epoch 61, Loss: 16.71811720041128\n",
      "Epoch 62, Loss: 18.939770515148457\n",
      "Epoch 63, Loss: 18.55285252057589\n",
      "Epoch 64, Loss: 19.45899431522076\n",
      "Epoch 65, Loss: 19.546961050767166\n",
      "Epoch 66, Loss: 21.35385612341074\n",
      "Epoch 67, Loss: 34.30346151498648\n",
      "Epoch 68, Loss: 34.222736432002144\n",
      "Epoch 69, Loss: 25.203442170069767\n",
      "Epoch 70, Loss: 23.11121185009296\n",
      "Epoch 71, Loss: 18.264238137465256\n",
      "Epoch 72, Loss: 16.04707618860098\n",
      "Epoch 73, Loss: 20.35032998598539\n",
      "Epoch 74, Loss: 19.208293566336998\n",
      "Epoch 75, Loss: 17.105187269357536\n",
      "Epoch 76, Loss: 16.180769755290104\n",
      "Epoch 77, Loss: 13.557508321908804\n",
      "Epoch 78, Loss: 20.233295293954704\n",
      "Epoch 79, Loss: 19.25619998345008\n",
      "Epoch 80, Loss: 19.457661995520958\n",
      "Epoch 81, Loss: 18.676188175494854\n",
      "Epoch 82, Loss: 17.43275895485511\n",
      "Epoch 83, Loss: 16.026391762953537\n",
      "Epoch 84, Loss: 20.03761049417349\n",
      "Epoch 85, Loss: 21.258684195004978\n",
      "Epoch 86, Loss: 19.797901171904343\n",
      "Epoch 87, Loss: 16.531007840083195\n",
      "Epoch 88, Loss: 15.948365211486816\n",
      "Epoch 89, Loss: 13.84392261505127\n",
      "Epoch 90, Loss: 13.921230279482328\n",
      "Epoch 91, Loss: 14.61988118978647\n",
      "Epoch 92, Loss: 16.232336832926823\n",
      "Epoch 93, Loss: 14.503391485947828\n",
      "Epoch 94, Loss: 13.523534774780273\n",
      "Epoch 95, Loss: 13.86919667170598\n",
      "Epoch 96, Loss: 15.584001981295073\n",
      "Epoch 97, Loss: 25.12623247733483\n",
      "Epoch 98, Loss: 20.473201458270733\n",
      "Epoch 99, Loss: 28.508335406963642\n",
      "Epoch 100, Loss: 30.227472452016976\n",
      "Epoch 101, Loss: 20.426617805774395\n",
      "Epoch 102, Loss: 28.42089264209454\n",
      "Epoch 103, Loss: 19.21649195597722\n",
      "Epoch 104, Loss: 13.61590653199416\n",
      "Epoch 105, Loss: 14.296307050264799\n",
      "Epoch 106, Loss: 31.558107669536884\n",
      "Epoch 107, Loss: 20.61641942537748\n",
      "Epoch 108, Loss: 15.775300282698412\n",
      "Epoch 109, Loss: 14.124826247875507\n",
      "Epoch 110, Loss: 15.618786628429707\n",
      "Epoch 111, Loss: 12.83605903845567\n",
      "Epoch 112, Loss: 15.811043922717754\n",
      "Epoch 113, Loss: 11.637415078970102\n",
      "Epoch 114, Loss: 11.347935364796566\n",
      "Epoch 115, Loss: 13.349040031433105\n",
      "Epoch 116, Loss: 14.304232193873478\n",
      "Epoch 117, Loss: 12.498238728596615\n",
      "Epoch 118, Loss: 11.823143317149235\n",
      "Epoch 119, Loss: 9.468885385073149\n",
      "Epoch 120, Loss: 9.197161674499512\n",
      "Epoch 121, Loss: 10.650462315632748\n",
      "Epoch 122, Loss: 11.508235124441294\n",
      "Epoch 123, Loss: 11.677306945507343\n",
      "Epoch 124, Loss: 17.378834302608784\n",
      "Epoch 125, Loss: 18.09046151087834\n",
      "Epoch 126, Loss: 19.1691837310791\n",
      "Epoch 127, Loss: 20.46023416519165\n",
      "Epoch 128, Loss: 29.233237156501183\n",
      "Epoch 129, Loss: 19.571419844260582\n",
      "Epoch 130, Loss: 17.628376703995926\n",
      "Epoch 131, Loss: 17.790139216643112\n",
      "Epoch 132, Loss: 15.412653097739586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:42,112] Trial 18 finished with value: 47.29364395565883 and parameters: {'latent_dim_z1': 31, 'latent_dim_z2': 45, 'hidden_dim': 45, 'epochs': 134, 'causal_reg': 0.4195275477875213, 'learning_rate': 0.009012404652225884}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133, Loss: 15.114514607649584\n",
      "Epoch 134, Loss: 12.193031384394718\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 15:50:45,466] Trial 19 failed with parameters: {'latent_dim_z1': 69, 'latent_dim_z2': 17, 'hidden_dim': 53, 'epochs': 161, 'causal_reg': 0.1544697381127116, 'learning_rate': 0.09223538316871989} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 15:50:45,467] Trial 19 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161, Loss: nan\n",
      "Epoch 1, Loss: 3.2972266267138286e+32\n",
      "Epoch 2, Loss: 3.3711048042816844e+32\n",
      "Epoch 3, Loss: 3.3711047959703194e+32\n",
      "Epoch 4, Loss: 3.3711050836359975e+32\n",
      "Epoch 5, Loss: 3.3711047363293275e+32\n",
      "Epoch 6, Loss: 3.371104718570165e+32\n",
      "Epoch 7, Loss: 3.3711047759201555e+32\n",
      "Epoch 8, Loss: 3.371104867416545e+32\n",
      "Epoch 9, Loss: 3.3711048318739264e+32\n",
      "Epoch 10, Loss: 3.371104618783496e+32\n",
      "Epoch 11, Loss: 3.371104845582185e+32\n",
      "Epoch 12, Loss: 3.371104864130414e+32\n",
      "Epoch 13, Loss: 3.371104780392185e+32\n",
      "Epoch 14, Loss: 3.371104772940609e+32\n",
      "Epoch 15, Loss: 3.37110501387325e+32\n",
      "Epoch 16, Loss: 3.371104786625937e+32\n",
      "Epoch 17, Loss: 3.3711046538997396e+32\n",
      "Epoch 18, Loss: 3.371104924396438e+32\n",
      "Epoch 19, Loss: 3.3711048518213845e+32\n",
      "Epoch 20, Loss: 3.371104766440647e+32\n",
      "Epoch 21, Loss: 3.371104906433736e+32\n",
      "Epoch 22, Loss: 3.371104854796929e+32\n",
      "Epoch 23, Loss: 3.371104861393258e+32\n",
      "Epoch 24, Loss: 3.3711047076155004e+32\n",
      "Epoch 25, Loss: 3.371104875433217e+32\n",
      "Epoch 26, Loss: 3.371104864043232e+32\n",
      "Epoch 27, Loss: 3.371104688882032e+32\n",
      "Epoch 28, Loss: 3.3711048148924786e+32\n",
      "Epoch 29, Loss: 3.371104782162392e+32\n",
      "Epoch 30, Loss: 3.371104752899726e+32\n",
      "Epoch 31, Loss: 3.3711046556722166e+32\n",
      "Epoch 32, Loss: 3.3711048186217855e+32\n",
      "Epoch 33, Loss: 3.371104741095966e+32\n",
      "Epoch 34, Loss: 3.371104821951054e+32\n",
      "Epoch 35, Loss: 3.371104994914537e+32\n",
      "Epoch 36, Loss: 3.371104909043071e+32\n",
      "Epoch 37, Loss: 3.371104771780905e+32\n",
      "Epoch 38, Loss: 3.3711048778161954e+32\n",
      "Epoch 39, Loss: 3.371104927793363e+32\n",
      "Epoch 40, Loss: 3.371104889153507e+32\n",
      "Epoch 41, Loss: 3.371104831021176e+32\n",
      "Epoch 42, Loss: 3.3711047197756805e+32\n",
      "Epoch 43, Loss: 3.3711048647832925e+32\n",
      "Epoch 44, Loss: 3.371104833065416e+32\n",
      "Epoch 45, Loss: 3.371104901126523e+32\n",
      "Epoch 46, Loss: 3.371104735265206e+32\n",
      "Epoch 47, Loss: 3.3711047144933855e+32\n",
      "Epoch 48, Loss: 3.371104820330465e+32\n",
      "Epoch 49, Loss: 3.371104850613548e+32\n",
      "Epoch 50, Loss: 3.371104859183054e+32\n",
      "Epoch 51, Loss: 3.3711048238845284e+32\n",
      "Epoch 52, Loss: 3.371104864322941e+32\n",
      "Epoch 53, Loss: 3.371104958556856e+32\n",
      "Epoch 54, Loss: 3.371104913785076e+32\n",
      "Epoch 55, Loss: 3.371104690014945e+32\n",
      "Epoch 56, Loss: 3.371104918265081e+32\n",
      "Epoch 57, Loss: 3.371104838491483e+32\n",
      "Epoch 58, Loss: 3.371104985903899e+32\n",
      "Epoch 59, Loss: 3.371104805321428e+32\n",
      "Epoch 60, Loss: 3.371104720505654e+32\n",
      "Epoch 61, Loss: 3.3711051449087026e+32\n",
      "Epoch 62, Loss: 3.371104839815562e+32\n",
      "Epoch 63, Loss: 3.3711047648212886e+32\n",
      "Epoch 64, Loss: 3.371104811588326e+32\n",
      "Epoch 65, Loss: 3.371104845590117e+32\n",
      "Epoch 66, Loss: 3.3711048107912846e+32\n",
      "Epoch 67, Loss: 3.371104649759997e+32\n",
      "Epoch 68, Loss: 3.3711047063797616e+32\n",
      "Epoch 69, Loss: 3.37110479955225e+32\n",
      "Epoch 70, Loss: 3.3711048880818935e+32\n",
      "Epoch 71, Loss: 3.3711048391945024e+32\n",
      "Epoch 72, Loss: 3.3711046971225614e+32\n",
      "Epoch 73, Loss: 3.371104684736722e+32\n",
      "Epoch 74, Loss: 3.371104758795731e+32\n",
      "Epoch 75, Loss: 3.3711048041073194e+32\n",
      "Epoch 76, Loss: 3.371104704905157e+32\n",
      "Epoch 77, Loss: 3.37110502280556e+32\n",
      "Epoch 78, Loss: 3.37110493539819e+32\n",
      "Epoch 79, Loss: 3.371104751731735e+32\n",
      "Epoch 80, Loss: 3.37110488354513e+32\n",
      "Epoch 81, Loss: 3.3711047489514415e+32\n",
      "Epoch 82, Loss: 3.37110478049912e+32\n",
      "Epoch 83, Loss: 3.3711048434653495e+32\n",
      "Epoch 84, Loss: 3.3711048461230995e+32\n",
      "Epoch 85, Loss: 3.3711049855006816e+32\n",
      "Epoch 86, Loss: 3.3711047364594195e+32\n",
      "Epoch 87, Loss: 3.3711048749936735e+32\n",
      "Epoch 88, Loss: 3.371104795519878e+32\n",
      "Epoch 89, Loss: 3.37110477541077e+32\n",
      "Epoch 90, Loss: 3.3711048450694214e+32\n",
      "Epoch 91, Loss: 3.371104944769022e+32\n",
      "Epoch 92, Loss: 3.371104858752365e+32\n",
      "Epoch 93, Loss: 3.3711048640194215e+32\n",
      "Epoch 94, Loss: 3.371105003729016e+32\n",
      "Epoch 95, Loss: 3.371104829491856e+32\n",
      "Epoch 96, Loss: 3.37110487183332e+32\n",
      "Epoch 97, Loss: 3.3711048629325675e+32\n",
      "Epoch 98, Loss: 3.371105272003024e+32\n",
      "Epoch 99, Loss: 3.371104767406359e+32\n",
      "Epoch 100, Loss: 3.371104911550466e+32\n",
      "Epoch 101, Loss: 3.371104847963574e+32\n",
      "Epoch 102, Loss: 3.3711046365722445e+32\n",
      "Epoch 103, Loss: 3.3711048398901435e+32\n",
      "Epoch 104, Loss: 3.371104710688013e+32\n",
      "Epoch 105, Loss: 3.371104836054129e+32\n",
      "Epoch 106, Loss: 3.371104877689055e+32\n",
      "Epoch 107, Loss: 3.371104833865494e+32\n",
      "Epoch 108, Loss: 3.37110489511121e+32\n",
      "Epoch 109, Loss: 3.371104775958383e+32\n",
      "Epoch 110, Loss: 3.371104830786874e+32\n",
      "Epoch 111, Loss: 3.371104746445068e+32\n",
      "Epoch 112, Loss: 3.371104642604883e+32\n",
      "Epoch 113, Loss: 3.371104893541676e+32\n",
      "Epoch 114, Loss: 3.371105039292068e+32\n",
      "Epoch 115, Loss: 3.3711047591653756e+32\n",
      "Epoch 116, Loss: 3.3711046368074974e+32\n",
      "Epoch 117, Loss: 3.3711048622239985e+32\n",
      "Epoch 118, Loss: 3.371104696535444e+32\n",
      "Epoch 119, Loss: 3.371104740568106e+32\n",
      "Epoch 120, Loss: 3.371104446576953e+32\n",
      "Epoch 121, Loss: 3.3711048427898766e+32\n",
      "Epoch 122, Loss: 3.371104730936975e+32\n",
      "Epoch 123, Loss: 3.371104876933476e+32\n",
      "Epoch 124, Loss: 3.3711047955230564e+32\n",
      "Epoch 125, Loss: 3.3711049766464714e+32\n",
      "Epoch 126, Loss: 3.371104985842145e+32\n",
      "Epoch 127, Loss: 3.371104661335651e+32\n",
      "Epoch 128, Loss: 3.371104672274967e+32\n",
      "Epoch 129, Loss: 3.37110486857852e+32\n",
      "Epoch 130, Loss: 3.371104896498604e+32\n",
      "Epoch 131, Loss: 3.3711049061321176e+32\n",
      "Epoch 132, Loss: 3.371104820007165e+32\n",
      "Epoch 133, Loss: 3.371104817600574e+32\n",
      "Epoch 134, Loss: 3.371104791881745e+32\n",
      "Epoch 135, Loss: 3.371104686270427e+32\n",
      "Epoch 136, Loss: 3.37110477690104e+32\n",
      "Epoch 137, Loss: 3.371104790296668e+32\n",
      "Epoch 138, Loss: 3.3711048343612854e+32\n",
      "Epoch 139, Loss: 3.3711048647497706e+32\n",
      "Epoch 140, Loss: 3.371104826590779e+32\n",
      "Epoch 141, Loss: 3.3711047873983156e+32\n",
      "Epoch 142, Loss: 3.371104776629504e+32\n",
      "Epoch 143, Loss: 3.371104723177537e+32\n",
      "Epoch 144, Loss: 3.371104929940223e+32\n",
      "Epoch 145, Loss: 3.371104852675043e+32\n",
      "Epoch 146, Loss: 3.3711047459259485e+32\n",
      "Epoch 147, Loss: 3.371104853333478e+32\n",
      "Epoch 148, Loss: 3.371104749049975e+32\n",
      "Epoch 149, Loss: 3.371104890028962e+32\n",
      "Epoch 150, Loss: 3.3711048029848495e+32\n",
      "Epoch 151, Loss: 3.371105080009765e+32\n",
      "Epoch 152, Loss: 3.371104818880097e+32\n",
      "Epoch 153, Loss: 3.3711048999840736e+32\n",
      "Epoch 154, Loss: 3.37110492339793e+32\n",
      "Epoch 155, Loss: 3.371104678668864e+32\n",
      "Epoch 156, Loss: 3.371104921834554e+32\n",
      "Epoch 157, Loss: 3.371104945478626e+32\n",
      "Epoch 158, Loss: 3.371104930098636e+32\n",
      "Epoch 159, Loss: 3.3711047872121454e+32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:49,013] Trial 20 finished with value: 3.370662440479038e+32 and parameters: {'latent_dim_z1': 74, 'latent_dim_z2': 18, 'hidden_dim': 54, 'epochs': 167, 'causal_reg': 0.18077378114679082, 'learning_rate': 0.057930002281084705}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160, Loss: 3.37110474391338e+32\n",
      "Epoch 161, Loss: 3.371104815863288e+32\n",
      "Epoch 162, Loss: 3.371104655256001e+32\n",
      "Epoch 163, Loss: 3.371104910491339e+32\n",
      "Epoch 164, Loss: 3.371104911602003e+32\n",
      "Epoch 165, Loss: 3.371104862791805e+32\n",
      "Epoch 166, Loss: 3.3711048611406796e+32\n",
      "Epoch 167, Loss: 3.3711046849256874e+32\n",
      "Epoch 1, Loss: 659.3674533550555\n",
      "Epoch 2, Loss: 269.77607169518103\n",
      "Epoch 3, Loss: 173.59253487220178\n",
      "Epoch 4, Loss: 144.45353552011343\n",
      "Epoch 5, Loss: 110.7064715898954\n",
      "Epoch 6, Loss: 102.86564225416917\n",
      "Epoch 7, Loss: 99.21751550527719\n",
      "Epoch 8, Loss: 95.59215743725116\n",
      "Epoch 9, Loss: 76.54802285707913\n",
      "Epoch 10, Loss: 68.76133148486798\n",
      "Epoch 11, Loss: 64.5882992377648\n",
      "Epoch 12, Loss: 62.373402228722206\n",
      "Epoch 13, Loss: 65.10540419358473\n",
      "Epoch 14, Loss: 89.20908370384804\n",
      "Epoch 15, Loss: 65.21846492473895\n",
      "Epoch 16, Loss: 56.74222007164588\n",
      "Epoch 17, Loss: 44.98313881800725\n",
      "Epoch 18, Loss: 40.888309258681076\n",
      "Epoch 19, Loss: 41.923148962167595\n",
      "Epoch 20, Loss: 53.59745542819683\n",
      "Epoch 21, Loss: 40.58695888519287\n",
      "Epoch 22, Loss: 40.41277309564444\n",
      "Epoch 23, Loss: 43.35867338914137\n",
      "Epoch 24, Loss: 49.014790498293365\n",
      "Epoch 25, Loss: 36.6536581332867\n",
      "Epoch 26, Loss: 35.51064568299513\n",
      "Epoch 27, Loss: 43.31656984182504\n",
      "Epoch 28, Loss: 45.658423093649056\n",
      "Epoch 29, Loss: 28.79208913216224\n",
      "Epoch 30, Loss: 25.726863347567043\n",
      "Epoch 31, Loss: 23.488130789536697\n",
      "Epoch 32, Loss: 33.56076038800753\n",
      "Epoch 33, Loss: 25.513048758873573\n",
      "Epoch 34, Loss: 23.421533988072323\n",
      "Epoch 35, Loss: 21.464645018944374\n",
      "Epoch 36, Loss: 24.304562201866737\n",
      "Epoch 37, Loss: 24.522291146791897\n",
      "Epoch 38, Loss: 26.230736805842472\n",
      "Epoch 39, Loss: 25.34690468127911\n",
      "Epoch 40, Loss: 36.636266194857086\n",
      "Epoch 41, Loss: 20.95021262535682\n",
      "Epoch 42, Loss: 19.63918486008277\n",
      "Epoch 43, Loss: 23.250558559711163\n",
      "Epoch 44, Loss: 19.874833290393536\n",
      "Epoch 45, Loss: 20.626711478600136\n",
      "Epoch 46, Loss: 19.12442058783311\n",
      "Epoch 47, Loss: 15.62998181123\n",
      "Epoch 48, Loss: 16.470743105961727\n",
      "Epoch 49, Loss: 19.051503218137302\n",
      "Epoch 50, Loss: 17.86945368693425\n",
      "Epoch 51, Loss: 16.1679306947268\n",
      "Epoch 52, Loss: 13.947279856755184\n",
      "Epoch 53, Loss: 14.98161847774799\n",
      "Epoch 54, Loss: 14.70429647885836\n",
      "Epoch 55, Loss: 13.052234521278969\n",
      "Epoch 56, Loss: 14.281379516308125\n",
      "Epoch 57, Loss: 26.891704449286827\n",
      "Epoch 58, Loss: 34.25535583496094\n",
      "Epoch 59, Loss: 21.754828489743748\n",
      "Epoch 60, Loss: 17.04365011361929\n",
      "Epoch 61, Loss: 14.744462875219492\n",
      "Epoch 62, Loss: 11.611673721900353\n",
      "Epoch 63, Loss: 10.726337249462421\n",
      "Epoch 64, Loss: 10.18498132779048\n",
      "Epoch 65, Loss: 10.903177884908823\n",
      "Epoch 66, Loss: 10.78228207734915\n",
      "Epoch 67, Loss: 11.544347469623272\n",
      "Epoch 68, Loss: 9.59325805077186\n",
      "Epoch 69, Loss: 14.707832666543814\n",
      "Epoch 70, Loss: 10.249914481089665\n",
      "Epoch 71, Loss: 13.497463299677921\n",
      "Epoch 72, Loss: 12.325937307797945\n",
      "Epoch 73, Loss: 12.482375915233906\n",
      "Epoch 74, Loss: 18.20470358775212\n",
      "Epoch 75, Loss: 12.885749431756826\n",
      "Epoch 76, Loss: 12.802232944048368\n",
      "Epoch 77, Loss: 12.599830040564903\n",
      "Epoch 78, Loss: 11.828654234225933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:50,568] Trial 21 finished with value: 46.253142153044415 and parameters: {'latent_dim_z1': 18, 'latent_dim_z2': 11, 'hidden_dim': 113, 'epochs': 79, 'causal_reg': 0.6415281359816745, 'learning_rate': 0.011570617517307362}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, Loss: 13.725145394985493\n",
      "Epoch 1, Loss: 733.9806577242338\n",
      "Epoch 2, Loss: 318.796149620643\n",
      "Epoch 3, Loss: 232.88955424382135\n",
      "Epoch 4, Loss: 193.10185359074518\n",
      "Epoch 5, Loss: 205.6619155590351\n",
      "Epoch 6, Loss: 158.35149588951697\n",
      "Epoch 7, Loss: 125.72770632230319\n",
      "Epoch 8, Loss: 110.91867857712965\n",
      "Epoch 9, Loss: 86.60165009131798\n",
      "Epoch 10, Loss: 74.76513466468224\n",
      "Epoch 11, Loss: 67.36177767240085\n",
      "Epoch 12, Loss: 59.320068286015434\n",
      "Epoch 13, Loss: 52.49913743826059\n",
      "Epoch 14, Loss: 67.91891882969783\n",
      "Epoch 15, Loss: 59.10410022735596\n",
      "Epoch 16, Loss: 51.13250813117394\n",
      "Epoch 17, Loss: 35.903746604919434\n",
      "Epoch 18, Loss: 28.828435531029335\n",
      "Epoch 19, Loss: 24.098365783691406\n",
      "Epoch 20, Loss: 22.699516186347374\n",
      "Epoch 21, Loss: 21.585475591512825\n",
      "Epoch 22, Loss: 18.48062856380756\n",
      "Epoch 23, Loss: 15.19874147268442\n",
      "Epoch 24, Loss: 14.74188602887667\n",
      "Epoch 25, Loss: 16.404590111512405\n",
      "Epoch 26, Loss: 16.284640899071327\n",
      "Epoch 27, Loss: 14.367013216018677\n",
      "Epoch 28, Loss: 16.428028950324425\n",
      "Epoch 29, Loss: 18.36043119430542\n",
      "Epoch 30, Loss: 19.12385870860173\n",
      "Epoch 31, Loss: 19.31143955083994\n",
      "Epoch 32, Loss: 21.0289107469412\n",
      "Epoch 33, Loss: 37.423883364750786\n",
      "Epoch 34, Loss: 44.98550378359281\n",
      "Epoch 35, Loss: 31.915118070749138\n",
      "Epoch 36, Loss: 25.57564086180467\n",
      "Epoch 37, Loss: 20.83864798912635\n",
      "Epoch 38, Loss: 17.82389209820674\n",
      "Epoch 39, Loss: 14.659592646818895\n",
      "Epoch 40, Loss: 12.739425622499907\n",
      "Epoch 41, Loss: 11.10773442341731\n",
      "Epoch 42, Loss: 11.721056644733135\n",
      "Epoch 43, Loss: 9.78440315906818\n",
      "Epoch 44, Loss: 10.583367347717285\n",
      "Epoch 45, Loss: 10.64213884793795\n",
      "Epoch 46, Loss: 9.713013282189003\n",
      "Epoch 47, Loss: 8.82597121825585\n",
      "Epoch 48, Loss: 7.882121507938091\n",
      "Epoch 49, Loss: 7.6996762202336235\n",
      "Epoch 50, Loss: 7.878808553402241\n",
      "Epoch 51, Loss: 8.083226680755615\n",
      "Epoch 52, Loss: 7.995380291571984\n",
      "Epoch 53, Loss: 8.964506827867949\n",
      "Epoch 54, Loss: 8.712135975177471\n",
      "Epoch 55, Loss: 9.877288286502544\n",
      "Epoch 56, Loss: 10.15212191068209\n",
      "Epoch 57, Loss: 10.204232601019052\n",
      "Epoch 58, Loss: 10.996564443294819\n",
      "Epoch 59, Loss: 12.801931674663837\n",
      "Epoch 60, Loss: 16.550294105823223\n",
      "Epoch 61, Loss: 12.449923460300152\n",
      "Epoch 62, Loss: 11.307940831551186\n",
      "Epoch 63, Loss: 11.590138582082895\n",
      "Epoch 64, Loss: 9.970276759221004\n",
      "Epoch 65, Loss: 9.40352858029879\n",
      "Epoch 66, Loss: 8.86474266419044\n",
      "Epoch 67, Loss: 9.114603537779589\n",
      "Epoch 68, Loss: 11.638027411240797\n",
      "Epoch 69, Loss: 13.781113202755268\n",
      "Epoch 70, Loss: 17.557790151009193\n",
      "Epoch 71, Loss: 17.108262887367836\n",
      "Epoch 72, Loss: 17.070678545878483\n",
      "Epoch 73, Loss: 15.837169390458326\n",
      "Epoch 74, Loss: 13.956607855283297\n",
      "Epoch 75, Loss: 13.153841642233042\n",
      "Epoch 76, Loss: 13.398401168676523\n",
      "Epoch 77, Loss: 11.5526777780973\n",
      "Epoch 78, Loss: 9.494762604053204\n",
      "Epoch 79, Loss: 8.528743413778452\n",
      "Epoch 80, Loss: 8.65802995975201\n",
      "Epoch 81, Loss: 9.581948372033926\n",
      "Epoch 82, Loss: 12.261492142310509\n",
      "Epoch 83, Loss: 14.455132301037128\n",
      "Epoch 84, Loss: 14.42475230877216\n",
      "Epoch 85, Loss: 14.60015003497784\n",
      "Epoch 86, Loss: 13.920405076100277\n",
      "Epoch 87, Loss: 14.708553919425377\n",
      "Epoch 88, Loss: 14.089815121430616\n",
      "Epoch 89, Loss: 15.002068501252394\n",
      "Epoch 90, Loss: 12.544414153465858\n",
      "Epoch 91, Loss: 10.642860742715689\n",
      "Epoch 92, Loss: 8.628063770440908\n",
      "Epoch 93, Loss: 7.900142376239483\n",
      "Epoch 94, Loss: 7.243524423012366\n",
      "Epoch 95, Loss: 6.663322338691125\n",
      "Epoch 96, Loss: 6.450892796883216\n",
      "Epoch 97, Loss: 6.385286697974572\n",
      "Epoch 98, Loss: 6.34743829873892\n",
      "Epoch 99, Loss: 6.4273325846745415\n",
      "Epoch 100, Loss: 6.495711528337919\n",
      "Epoch 101, Loss: 6.577130042589628\n",
      "Epoch 102, Loss: 6.85204634299645\n",
      "Epoch 103, Loss: 6.679432337100689\n",
      "Epoch 104, Loss: 6.596420893302331\n",
      "Epoch 105, Loss: 6.622700984661396\n",
      "Epoch 106, Loss: 6.601899752250085\n",
      "Epoch 107, Loss: 6.539225853406466\n",
      "Epoch 108, Loss: 6.939861976183378\n",
      "Epoch 109, Loss: 7.9639057746300335\n",
      "Epoch 110, Loss: 9.113086407001202\n",
      "Epoch 111, Loss: 11.476916808348436\n",
      "Epoch 112, Loss: 13.038845979250395\n",
      "Epoch 113, Loss: 13.897441167097826\n",
      "Epoch 114, Loss: 15.417229615725004\n",
      "Epoch 115, Loss: 17.622997944171612\n",
      "Epoch 116, Loss: 27.24265322318444\n",
      "Epoch 117, Loss: 30.484899264115555\n",
      "Epoch 118, Loss: 19.784478187561035\n",
      "Epoch 119, Loss: 24.211492648491493\n",
      "Epoch 120, Loss: 26.85797302539532\n",
      "Epoch 121, Loss: 24.727000823387733\n",
      "Epoch 122, Loss: 16.79528383108286\n",
      "Epoch 123, Loss: 12.623746211712177\n",
      "Epoch 124, Loss: 10.018788906244131\n",
      "Epoch 125, Loss: 9.723199697641226\n",
      "Epoch 126, Loss: 8.418636156962467\n",
      "Epoch 127, Loss: 7.912033906349769\n",
      "Epoch 128, Loss: 7.378334687306331\n",
      "Epoch 129, Loss: 6.738631395193247\n",
      "Epoch 130, Loss: 6.463422628549429\n",
      "Epoch 131, Loss: 6.52495866555434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:50:53,818] Trial 22 finished with value: 31.443291421706032 and parameters: {'latent_dim_z1': 33, 'latent_dim_z2': 80, 'hidden_dim': 166, 'epochs': 133, 'causal_reg': 0.3362097843428847, 'learning_rate': 0.0026118562983243622}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132, Loss: 6.96338545359098\n",
      "Epoch 133, Loss: 6.824436132724468\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 15:50:57,628] Trial 23 failed with parameters: {'latent_dim_z1': 66, 'latent_dim_z2': 34, 'hidden_dim': 56, 'epochs': 172, 'causal_reg': 0.11916803597905667, 'learning_rate': 0.08197488614923218} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 15:50:57,629] Trial 23 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 1, Loss: 2298.049520639273\n",
      "Epoch 2, Loss: 1289.2884674072266\n",
      "Epoch 3, Loss: 924.9726427518405\n",
      "Epoch 4, Loss: 793.9688262939453\n",
      "Epoch 5, Loss: 716.4526731050931\n",
      "Epoch 6, Loss: 654.1277483426608\n",
      "Epoch 7, Loss: 596.6847399198092\n",
      "Epoch 8, Loss: 547.6185836791992\n",
      "Epoch 9, Loss: 507.53709822434644\n",
      "Epoch 10, Loss: 465.60729158841644\n",
      "Epoch 11, Loss: 433.31189610407904\n",
      "Epoch 12, Loss: 401.9410975529597\n",
      "Epoch 13, Loss: 367.9958347907433\n",
      "Epoch 14, Loss: 339.1237299992488\n",
      "Epoch 15, Loss: 312.800405062162\n",
      "Epoch 16, Loss: 295.6841248732347\n",
      "Epoch 17, Loss: 278.9812704233023\n",
      "Epoch 18, Loss: 250.42756330049954\n",
      "Epoch 19, Loss: 233.80000451894907\n",
      "Epoch 20, Loss: 219.66834464439978\n",
      "Epoch 21, Loss: 203.9791180537297\n",
      "Epoch 22, Loss: 182.69179740318884\n",
      "Epoch 23, Loss: 166.7379655104417\n",
      "Epoch 24, Loss: 158.61767490093524\n",
      "Epoch 25, Loss: 144.23163751455454\n",
      "Epoch 26, Loss: 134.47264730013333\n",
      "Epoch 27, Loss: 119.69437188368578\n",
      "Epoch 28, Loss: 110.8381010202261\n",
      "Epoch 29, Loss: 103.27325057983398\n",
      "Epoch 30, Loss: 96.7163251730112\n",
      "Epoch 31, Loss: 88.57417341379019\n",
      "Epoch 32, Loss: 82.07121130136343\n",
      "Epoch 33, Loss: 76.3366446861854\n",
      "Epoch 34, Loss: 72.97512964101938\n",
      "Epoch 35, Loss: 65.5611643424401\n",
      "Epoch 36, Loss: 61.23911960308369\n",
      "Epoch 37, Loss: 54.92195327465351\n",
      "Epoch 38, Loss: 50.962053519028885\n",
      "Epoch 39, Loss: 50.158138495225174\n",
      "Epoch 40, Loss: 44.07236737471361\n",
      "Epoch 41, Loss: 41.87298866418692\n",
      "Epoch 42, Loss: 39.146347742814285\n",
      "Epoch 43, Loss: 37.44393124947181\n",
      "Epoch 44, Loss: 34.53249975351187\n",
      "Epoch 45, Loss: 32.442987148578354\n",
      "Epoch 46, Loss: 30.38216671576867\n",
      "Epoch 47, Loss: 28.35007271399865\n",
      "Epoch 48, Loss: 27.410112307621883\n",
      "Epoch 49, Loss: 25.24588445516733\n",
      "Epoch 50, Loss: 24.488904622884895\n",
      "Epoch 51, Loss: 23.083443568303036\n",
      "Epoch 52, Loss: 21.580171254964974\n",
      "Epoch 53, Loss: 20.785209325643685\n",
      "Epoch 54, Loss: 19.965860806978664\n",
      "Epoch 55, Loss: 19.12796456997211\n",
      "Epoch 56, Loss: 18.360659012427696\n",
      "Epoch 57, Loss: 17.585972492511456\n",
      "Epoch 58, Loss: 16.94870083148663\n",
      "Epoch 59, Loss: 16.35230434857882\n",
      "Epoch 60, Loss: 15.561948005969708\n",
      "Epoch 61, Loss: 15.098451100862944\n",
      "Epoch 62, Loss: 14.575777017153227\n",
      "Epoch 63, Loss: 13.971646382258488\n",
      "Epoch 64, Loss: 13.604652734903189\n",
      "Epoch 65, Loss: 13.329576272230883\n",
      "Epoch 66, Loss: 12.978126819317158\n",
      "Epoch 67, Loss: 12.584088765657865\n",
      "Epoch 68, Loss: 12.345964651841383\n",
      "Epoch 69, Loss: 12.046219789064848\n",
      "Epoch 70, Loss: 11.790565967559814\n",
      "Epoch 71, Loss: 11.493414897185106\n",
      "Epoch 72, Loss: 11.217735088788546\n",
      "Epoch 73, Loss: 11.032902607550987\n",
      "Epoch 74, Loss: 10.896038293838501\n",
      "Epoch 75, Loss: 10.63278658573444\n",
      "Epoch 76, Loss: 10.40987990452693\n",
      "Epoch 77, Loss: 10.232190774037289\n",
      "Epoch 78, Loss: 10.05410284262437\n",
      "Epoch 79, Loss: 9.943871626487145\n",
      "Epoch 80, Loss: 9.788199754861685\n",
      "Epoch 81, Loss: 9.623188165517954\n",
      "Epoch 82, Loss: 9.498915085425743\n",
      "Epoch 83, Loss: 9.360282072654137\n",
      "Epoch 84, Loss: 9.246552265607393\n",
      "Epoch 85, Loss: 9.120917925467857\n",
      "Epoch 86, Loss: 9.00329114840581\n",
      "Epoch 87, Loss: 8.882766191775982\n",
      "Epoch 88, Loss: 8.744189794246967\n",
      "Epoch 89, Loss: 8.69274322803204\n",
      "Epoch 90, Loss: 8.606462551997257\n",
      "Epoch 91, Loss: 8.520191614444439\n",
      "Epoch 92, Loss: 8.475097417831421\n",
      "Epoch 93, Loss: 8.421606063842773\n",
      "Epoch 94, Loss: 8.343531590241652\n",
      "Epoch 95, Loss: 8.336184666706966\n",
      "Epoch 96, Loss: 8.29461066539471\n",
      "Epoch 97, Loss: 8.316377217953022\n",
      "Epoch 98, Loss: 8.405027536245493\n",
      "Epoch 99, Loss: 9.07639791415288\n",
      "Epoch 100, Loss: 9.749350401071402\n",
      "Epoch 101, Loss: 9.457219619017382\n",
      "Epoch 102, Loss: 10.959032242114727\n",
      "Epoch 103, Loss: 10.232484872524555\n",
      "Epoch 104, Loss: 9.593128039286686\n",
      "Epoch 105, Loss: 9.215491881737343\n",
      "Epoch 106, Loss: 8.881460684996386\n",
      "Epoch 107, Loss: 8.93783376767085\n",
      "Epoch 108, Loss: 8.752644025362455\n",
      "Epoch 109, Loss: 8.600829637967623\n",
      "Epoch 110, Loss: 8.478335600632887\n",
      "Epoch 111, Loss: 8.309586231525127\n",
      "Epoch 112, Loss: 8.605940965505747\n",
      "Epoch 113, Loss: 8.713227418752817\n",
      "Epoch 114, Loss: 9.118617626336905\n",
      "Epoch 115, Loss: 9.503705006379347\n",
      "Epoch 116, Loss: 9.342151495126577\n",
      "Epoch 117, Loss: 11.051638511510996\n",
      "Epoch 118, Loss: 10.986196297865648\n",
      "Epoch 119, Loss: 13.056754809159498\n",
      "Epoch 120, Loss: 13.392584103804369\n",
      "Epoch 121, Loss: 12.90377818621122\n",
      "Epoch 122, Loss: 13.15683152125432\n",
      "Epoch 123, Loss: 12.460878628950853\n",
      "Epoch 124, Loss: 11.408560807888325\n",
      "Epoch 125, Loss: 11.233877768883339\n",
      "Epoch 126, Loss: 9.948418672268208\n",
      "Epoch 127, Loss: 8.402438237116886\n",
      "Epoch 128, Loss: 7.984686117905837\n",
      "Epoch 129, Loss: 7.651008385878343\n",
      "Epoch 130, Loss: 7.421346609409039\n",
      "Epoch 131, Loss: 7.364401138745821\n",
      "Epoch 132, Loss: 7.335055589675903\n",
      "Epoch 133, Loss: 7.2181032070746785\n",
      "Epoch 134, Loss: 7.124789329675528\n",
      "Epoch 135, Loss: 7.081744505808904\n",
      "Epoch 136, Loss: 7.097425149037288\n",
      "Epoch 137, Loss: 6.920110097298255\n",
      "Epoch 138, Loss: 6.887998324174148\n",
      "Epoch 139, Loss: 6.874628984011137\n",
      "Epoch 140, Loss: 6.871496934157151\n",
      "Epoch 141, Loss: 6.901916723984939\n",
      "Epoch 142, Loss: 7.114766340989333\n",
      "Epoch 143, Loss: 7.372385263442993\n",
      "Epoch 144, Loss: 7.497878037966215\n",
      "Epoch 145, Loss: 9.440166491728563\n",
      "Epoch 146, Loss: 12.545621450130756\n",
      "Epoch 147, Loss: 14.095570674309364\n",
      "Epoch 148, Loss: 19.51747388106126\n",
      "Epoch 149, Loss: 15.57062079356267\n",
      "Epoch 150, Loss: 11.100542490298931\n",
      "Epoch 151, Loss: 10.730888788516705\n",
      "Epoch 152, Loss: 9.444498502291166\n",
      "Epoch 153, Loss: 8.15647499377911\n",
      "Epoch 154, Loss: 7.885103610845713\n",
      "Epoch 155, Loss: 7.8572205030001125\n",
      "Epoch 156, Loss: 7.621800055870643\n",
      "Epoch 157, Loss: 7.51154980292687\n",
      "Epoch 158, Loss: 7.676265771572407\n",
      "Epoch 159, Loss: 7.677632515247051\n",
      "Epoch 160, Loss: 7.478867017305815\n",
      "Epoch 161, Loss: 7.080050505124605\n",
      "Epoch 162, Loss: 7.661553841370803\n",
      "Epoch 163, Loss: 8.848021727341871\n",
      "Epoch 164, Loss: 8.16505529330327\n",
      "Epoch 165, Loss: 8.145268495266254\n",
      "Epoch 166, Loss: 7.882392039665809\n",
      "Epoch 167, Loss: 7.445232207958515\n",
      "Epoch 168, Loss: 7.546750233723567\n",
      "Epoch 169, Loss: 7.347180714974036\n",
      "Epoch 170, Loss: 7.150790764735295\n",
      "Epoch 171, Loss: 7.438758923457219\n",
      "Epoch 172, Loss: 7.577174076667199\n",
      "Epoch 173, Loss: 7.283939178173359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:01,627] Trial 24 finished with value: 87.28655115493545 and parameters: {'latent_dim_z1': 65, 'latent_dim_z2': 34, 'hidden_dim': 60, 'epochs': 179, 'causal_reg': 0.10774047937647424, 'learning_rate': 0.0002726068046180417}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174, Loss: 7.138273550913884\n",
      "Epoch 175, Loss: 6.983365132258489\n",
      "Epoch 176, Loss: 6.902558785218459\n",
      "Epoch 177, Loss: 6.792207075999333\n",
      "Epoch 178, Loss: 6.6746978759765625\n",
      "Epoch 179, Loss: 6.916988372802734\n",
      "Epoch 1, Loss: 457.2588495107797\n",
      "Epoch 2, Loss: 313.4376484797551\n",
      "Epoch 3, Loss: 206.88491968008188\n",
      "Epoch 4, Loss: 139.7512372823862\n",
      "Epoch 5, Loss: 111.0397272843581\n",
      "Epoch 6, Loss: 98.23014743511493\n",
      "Epoch 7, Loss: 91.11905413407545\n",
      "Epoch 8, Loss: 83.16827766711896\n",
      "Epoch 9, Loss: 77.20585602980394\n",
      "Epoch 10, Loss: 72.5859288435716\n",
      "Epoch 11, Loss: 69.02269950279823\n",
      "Epoch 12, Loss: 65.15180602440468\n",
      "Epoch 13, Loss: 62.18941857264592\n",
      "Epoch 14, Loss: 59.8246614749615\n",
      "Epoch 15, Loss: 55.627920444195084\n",
      "Epoch 16, Loss: 53.824913024902344\n",
      "Epoch 17, Loss: 51.10699598605816\n",
      "Epoch 18, Loss: 48.935333105234\n",
      "Epoch 19, Loss: 47.55935859680176\n",
      "Epoch 20, Loss: 45.24168821481558\n",
      "Epoch 21, Loss: 42.59420747023363\n",
      "Epoch 22, Loss: 40.265478097475494\n",
      "Epoch 23, Loss: 39.71207383962778\n",
      "Epoch 24, Loss: 38.286100094134994\n",
      "Epoch 25, Loss: 36.46874317756066\n",
      "Epoch 26, Loss: 34.431458766643814\n",
      "Epoch 27, Loss: 32.12342680417574\n",
      "Epoch 28, Loss: 30.557180184584396\n",
      "Epoch 29, Loss: 29.31873321533203\n",
      "Epoch 30, Loss: 28.10979755108173\n",
      "Epoch 31, Loss: 27.180302619934082\n",
      "Epoch 32, Loss: 25.353230219620926\n",
      "Epoch 33, Loss: 24.83786205145029\n",
      "Epoch 34, Loss: 24.543757401979885\n",
      "Epoch 35, Loss: 23.686287183028\n",
      "Epoch 36, Loss: 22.075517067542442\n",
      "Epoch 37, Loss: 20.777307803814228\n",
      "Epoch 38, Loss: 20.069618922013504\n",
      "Epoch 39, Loss: 20.01406137759869\n",
      "Epoch 40, Loss: 19.543639549842247\n",
      "Epoch 41, Loss: 18.25008304302509\n",
      "Epoch 42, Loss: 17.3979590856112\n",
      "Epoch 43, Loss: 17.41887070582463\n",
      "Epoch 44, Loss: 16.97940628345196\n",
      "Epoch 45, Loss: 16.296238789191612\n",
      "Epoch 46, Loss: 15.801500100355883\n",
      "Epoch 47, Loss: 15.3077783034398\n",
      "Epoch 48, Loss: 15.045650995694674\n",
      "Epoch 49, Loss: 14.318867096534142\n",
      "Epoch 50, Loss: 13.732234514676607\n",
      "Epoch 51, Loss: 13.69210092837994\n",
      "Epoch 52, Loss: 13.190514271075909\n",
      "Epoch 53, Loss: 12.895028884594257\n",
      "Epoch 54, Loss: 12.38949229167058\n",
      "Epoch 55, Loss: 12.710938765452457\n",
      "Epoch 56, Loss: 12.454150951825655\n",
      "Epoch 57, Loss: 11.826079075153057\n",
      "Epoch 58, Loss: 11.231909990310669\n",
      "Epoch 59, Loss: 11.197493113004244\n",
      "Epoch 60, Loss: 11.057368498582106\n",
      "Epoch 61, Loss: 10.67563040439899\n",
      "Epoch 62, Loss: 10.625752980892475\n",
      "Epoch 63, Loss: 10.880234021406908\n",
      "Epoch 64, Loss: 10.04115108343271\n",
      "Epoch 65, Loss: 9.976288612072285\n",
      "Epoch 66, Loss: 9.913312508509708\n",
      "Epoch 67, Loss: 9.606190553078285\n",
      "Epoch 68, Loss: 10.71054902443519\n",
      "Epoch 69, Loss: 9.567570264522846\n",
      "Epoch 70, Loss: 9.431536252682026\n",
      "Epoch 71, Loss: 9.194690300868107\n",
      "Epoch 72, Loss: 9.970893676464375\n",
      "Epoch 73, Loss: 8.767028515155499\n",
      "Epoch 74, Loss: 8.402084699043861\n",
      "Epoch 75, Loss: 8.26373291015625\n",
      "Epoch 76, Loss: 8.098686071542593\n",
      "Epoch 77, Loss: 7.920921637461736\n",
      "Epoch 78, Loss: 8.151941537857056\n",
      "Epoch 79, Loss: 8.564535397749681\n",
      "Epoch 80, Loss: 8.22482809653649\n",
      "Epoch 81, Loss: 7.717348208794227\n",
      "Epoch 82, Loss: 7.658843297224778\n",
      "Epoch 83, Loss: 7.804170058323787\n",
      "Epoch 84, Loss: 7.649787939511812\n",
      "Epoch 85, Loss: 7.518676959551298\n",
      "Epoch 86, Loss: 7.479048747282762\n",
      "Epoch 87, Loss: 7.738430114892813\n",
      "Epoch 88, Loss: 7.161513603650606\n",
      "Epoch 89, Loss: 7.074217136089619\n",
      "Epoch 90, Loss: 7.074578028458816\n",
      "Epoch 91, Loss: 7.081484244419978\n",
      "Epoch 92, Loss: 7.667553241436298\n",
      "Epoch 93, Loss: 7.288245402849638\n",
      "Epoch 94, Loss: 7.034018424841074\n",
      "Epoch 95, Loss: 6.85286048742441\n",
      "Epoch 96, Loss: 6.681204300660354\n",
      "Epoch 97, Loss: 6.633083912042471\n",
      "Epoch 98, Loss: 6.6060710320105915\n",
      "Epoch 99, Loss: 6.741853145452646\n",
      "Epoch 100, Loss: 6.932055253248948\n",
      "Epoch 101, Loss: 6.799652686485877\n",
      "Epoch 102, Loss: 6.636031701014592\n",
      "Epoch 103, Loss: 6.725724990551289\n",
      "Epoch 104, Loss: 6.517574310302734\n",
      "Epoch 105, Loss: 6.377659981067364\n",
      "Epoch 106, Loss: 6.354082731100229\n",
      "Epoch 107, Loss: 6.382747411727905\n",
      "Epoch 108, Loss: 6.39485423381512\n",
      "Epoch 109, Loss: 6.525927580319918\n",
      "Epoch 110, Loss: 6.690336520855244\n",
      "Epoch 111, Loss: 6.69969470684345\n",
      "Epoch 112, Loss: 6.530925640693078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:04,035] Trial 25 finished with value: 29.445430413729117 and parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 26, 'hidden_dim': 101, 'epochs': 120, 'causal_reg': 0.08260239988385842, 'learning_rate': 0.0005615321845657018}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113, Loss: 6.329646550692045\n",
      "Epoch 114, Loss: 6.427039238122793\n",
      "Epoch 115, Loss: 6.418987842706533\n",
      "Epoch 116, Loss: 6.350160690454336\n",
      "Epoch 117, Loss: 6.631949388063871\n",
      "Epoch 118, Loss: 6.365468263626099\n",
      "Epoch 119, Loss: 6.356593480476966\n",
      "Epoch 120, Loss: 6.2450992327470045\n",
      "Epoch 1, Loss: 313.8218565720778\n",
      "Epoch 2, Loss: 133.42274431081918\n",
      "Epoch 3, Loss: 94.87429985633263\n",
      "Epoch 4, Loss: 79.25652540647067\n",
      "Epoch 5, Loss: 69.47459404285138\n",
      "Epoch 6, Loss: 62.297867334806\n",
      "Epoch 7, Loss: 53.91582569709191\n",
      "Epoch 8, Loss: 51.007856148939865\n",
      "Epoch 9, Loss: 46.99153562692496\n",
      "Epoch 10, Loss: 41.66737677500798\n",
      "Epoch 11, Loss: 39.41311146662785\n",
      "Epoch 12, Loss: 38.540316801804764\n",
      "Epoch 13, Loss: 34.439975885244515\n",
      "Epoch 14, Loss: 34.67828952349149\n",
      "Epoch 15, Loss: 29.139588796175442\n",
      "Epoch 16, Loss: 30.561153338505672\n",
      "Epoch 17, Loss: 26.499242012317364\n",
      "Epoch 18, Loss: 23.196413590357853\n",
      "Epoch 19, Loss: 23.320784642146183\n",
      "Epoch 20, Loss: 22.939162107614372\n",
      "Epoch 21, Loss: 22.563274053426888\n",
      "Epoch 22, Loss: 19.29572714292086\n",
      "Epoch 23, Loss: 21.017848289929905\n",
      "Epoch 24, Loss: 20.51986422905555\n",
      "Epoch 25, Loss: 19.815945808704083\n",
      "Epoch 26, Loss: 16.51068482032189\n",
      "Epoch 27, Loss: 17.133393581096943\n",
      "Epoch 28, Loss: 15.221502230717586\n",
      "Epoch 29, Loss: 15.40990161895752\n",
      "Epoch 30, Loss: 16.03544227893536\n",
      "Epoch 31, Loss: 16.125301397763767\n",
      "Epoch 32, Loss: 15.076563688424917\n",
      "Epoch 33, Loss: 13.864002888019268\n",
      "Epoch 34, Loss: 13.309066093884981\n",
      "Epoch 35, Loss: 14.903492230635424\n",
      "Epoch 36, Loss: 15.060585242051344\n",
      "Epoch 37, Loss: 14.289954882401686\n",
      "Epoch 38, Loss: 12.427107810974121\n",
      "Epoch 39, Loss: 14.643880385618944\n",
      "Epoch 40, Loss: 11.97629266518813\n",
      "Epoch 41, Loss: 10.965993716166569\n",
      "Epoch 42, Loss: 10.999133660243107\n",
      "Epoch 43, Loss: 12.214233325077938\n",
      "Epoch 44, Loss: 10.006404876708984\n",
      "Epoch 45, Loss: 9.645396544383122\n",
      "Epoch 46, Loss: 9.700237971085768\n",
      "Epoch 47, Loss: 9.702978464273306\n",
      "Epoch 48, Loss: 9.01350292792687\n",
      "Epoch 49, Loss: 8.978353775464571\n",
      "Epoch 50, Loss: 8.735068192848793\n",
      "Epoch 51, Loss: 9.296775304354155\n",
      "Epoch 52, Loss: 9.833776675737822\n",
      "Epoch 53, Loss: 8.211910101083609\n",
      "Epoch 54, Loss: 8.398634470426119\n",
      "Epoch 55, Loss: 8.62024033986605\n",
      "Epoch 56, Loss: 9.775131078866812\n",
      "Epoch 57, Loss: 9.788431534400353\n",
      "Epoch 58, Loss: 9.162281623253456\n",
      "Epoch 59, Loss: 8.464721257870014\n",
      "Epoch 60, Loss: 7.7952635288238525\n",
      "Epoch 61, Loss: 7.862051468629104\n",
      "Epoch 62, Loss: 8.016838844005878\n",
      "Epoch 63, Loss: 7.98478652880742\n",
      "Epoch 64, Loss: 7.801500687232385\n",
      "Epoch 65, Loss: 10.357360142927904\n",
      "Epoch 66, Loss: 10.626302242279053\n",
      "Epoch 67, Loss: 9.9133316553556\n",
      "Epoch 68, Loss: 9.71127201960637\n",
      "Epoch 69, Loss: 8.76251272054819\n",
      "Epoch 70, Loss: 10.94705893443181\n",
      "Epoch 71, Loss: 10.711269195263203\n",
      "Epoch 72, Loss: 8.908238062491783\n",
      "Epoch 73, Loss: 7.769042473572951\n",
      "Epoch 74, Loss: 7.9326285398923435\n",
      "Epoch 75, Loss: 7.857319098252517\n",
      "Epoch 76, Loss: 8.854083758134108\n",
      "Epoch 77, Loss: 8.333736052879921\n",
      "Epoch 78, Loss: 7.430865837977483\n",
      "Epoch 79, Loss: 7.0268037135784445\n",
      "Epoch 80, Loss: 7.035405507454505\n",
      "Epoch 81, Loss: 6.970590499731211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:05,760] Trial 26 finished with value: 22.409690941704635 and parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 29, 'hidden_dim': 80, 'epochs': 90, 'causal_reg': 0.2552309466452298, 'learning_rate': 0.0016571191750000974}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, Loss: 6.82986305310176\n",
      "Epoch 83, Loss: 6.6080911709712105\n",
      "Epoch 84, Loss: 6.484750729340774\n",
      "Epoch 85, Loss: 6.559868830900926\n",
      "Epoch 86, Loss: 6.931735258836013\n",
      "Epoch 87, Loss: 9.040748999669002\n",
      "Epoch 88, Loss: 8.712751461909367\n",
      "Epoch 89, Loss: 10.041491160025963\n",
      "Epoch 90, Loss: 10.883065462112427\n",
      "Epoch 1, Loss: 543.4385088407076\n",
      "Epoch 2, Loss: 212.82706686166617\n",
      "Epoch 3, Loss: 193.76203008798453\n",
      "Epoch 4, Loss: 137.98788745586688\n",
      "Epoch 5, Loss: 118.30165041410007\n",
      "Epoch 6, Loss: 109.39986588404729\n",
      "Epoch 7, Loss: 95.98428689516507\n",
      "Epoch 8, Loss: 88.09862855764536\n",
      "Epoch 9, Loss: 75.05399249150203\n",
      "Epoch 10, Loss: 66.36383988307072\n",
      "Epoch 11, Loss: 58.328445654649\n",
      "Epoch 12, Loss: 55.31104656366202\n",
      "Epoch 13, Loss: 50.2027390553401\n",
      "Epoch 14, Loss: 44.490227479201096\n",
      "Epoch 15, Loss: 42.646066665649414\n",
      "Epoch 16, Loss: 40.028621453505295\n",
      "Epoch 17, Loss: 36.98190036186805\n",
      "Epoch 18, Loss: 34.36565465193529\n",
      "Epoch 19, Loss: 33.72342744240394\n",
      "Epoch 20, Loss: 31.1628307562608\n",
      "Epoch 21, Loss: 29.104194072576668\n",
      "Epoch 22, Loss: 24.157388320335976\n",
      "Epoch 23, Loss: 22.9933579151447\n",
      "Epoch 24, Loss: 24.38506500537579\n",
      "Epoch 25, Loss: 30.630303566272442\n",
      "Epoch 26, Loss: 22.916843304267296\n",
      "Epoch 27, Loss: 24.532183317037727\n",
      "Epoch 28, Loss: 21.455007406381462\n",
      "Epoch 29, Loss: 17.519767027634842\n",
      "Epoch 30, Loss: 18.028345511509823\n",
      "Epoch 31, Loss: 15.04140487084022\n",
      "Epoch 32, Loss: 14.577896118164062\n",
      "Epoch 33, Loss: 15.712258155529316\n",
      "Epoch 34, Loss: 14.010646013113169\n",
      "Epoch 35, Loss: 15.050059025104229\n",
      "Epoch 36, Loss: 17.14527731675368\n",
      "Epoch 37, Loss: 15.645981641916128\n",
      "Epoch 38, Loss: 15.202091437119703\n",
      "Epoch 39, Loss: 13.692094252659725\n",
      "Epoch 40, Loss: 11.548099737900953\n",
      "Epoch 41, Loss: 10.447657511784481\n",
      "Epoch 42, Loss: 11.686068094693697\n",
      "Epoch 43, Loss: 13.549909775073711\n",
      "Epoch 44, Loss: 12.81849193572998\n",
      "Epoch 45, Loss: 13.774857337658222\n",
      "Epoch 46, Loss: 14.796891725980318\n",
      "Epoch 47, Loss: 13.162487011689405\n",
      "Epoch 48, Loss: 11.996750024648813\n",
      "Epoch 49, Loss: 11.670955786338219\n",
      "Epoch 50, Loss: 10.325899069125835\n",
      "Epoch 51, Loss: 10.984434036108164\n",
      "Epoch 52, Loss: 10.659656249559843\n",
      "Epoch 53, Loss: 11.105846331669735\n",
      "Epoch 54, Loss: 10.506686650789701\n",
      "Epoch 55, Loss: 10.695381806446957\n",
      "Epoch 56, Loss: 14.208681766803448\n",
      "Epoch 57, Loss: 16.459563181950497\n",
      "Epoch 58, Loss: 10.920102687982412\n",
      "Epoch 59, Loss: 10.021719749157246\n",
      "Epoch 60, Loss: 9.653965601554283\n",
      "Epoch 61, Loss: 11.382226026975191\n",
      "Epoch 62, Loss: 11.32493180495042\n",
      "Epoch 63, Loss: 10.130818587083082\n",
      "Epoch 64, Loss: 10.867443066376905\n",
      "Epoch 65, Loss: 9.858371331141544\n",
      "Epoch 66, Loss: 10.171340245466967\n",
      "Epoch 67, Loss: 9.14949239217318\n",
      "Epoch 68, Loss: 10.070190227948702\n",
      "Epoch 69, Loss: 9.39887153185331\n",
      "Epoch 70, Loss: 9.24778888775752\n",
      "Epoch 71, Loss: 8.463775909863985\n",
      "Epoch 72, Loss: 8.749732384314903\n",
      "Epoch 73, Loss: 9.902861980291513\n",
      "Epoch 74, Loss: 9.670505340282734\n",
      "Epoch 75, Loss: 10.64142839725201\n",
      "Epoch 76, Loss: 8.281397562760572\n",
      "Epoch 77, Loss: 7.346559597895696\n",
      "Epoch 78, Loss: 8.545772827588594\n",
      "Epoch 79, Loss: 14.708037779881405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:07,382] Trial 27 finished with value: 34.26219733307317 and parameters: {'latent_dim_z1': 19, 'latent_dim_z2': 22, 'hidden_dim': 39, 'epochs': 86, 'causal_reg': 0.23806866728692844, 'learning_rate': 0.002361393329727525}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, Loss: 12.02743704502399\n",
      "Epoch 81, Loss: 9.016777075254\n",
      "Epoch 82, Loss: 8.422612997201773\n",
      "Epoch 83, Loss: 8.454404023977427\n",
      "Epoch 84, Loss: 9.168785076874952\n",
      "Epoch 85, Loss: 9.433836405093853\n",
      "Epoch 86, Loss: 10.860584332392765\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 15:51:10,327] Trial 28 failed with parameters: {'latent_dim_z1': 29, 'latent_dim_z2': 34, 'hidden_dim': 11, 'epochs': 152, 'causal_reg': 0.27701009680091704, 'learning_rate': 0.09580277243932803} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 15:51:10,327] Trial 28 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 1, Loss: 557.6289033156174\n",
      "Epoch 2, Loss: 246.27519255418045\n",
      "Epoch 3, Loss: 172.72037594134991\n",
      "Epoch 4, Loss: 160.72905613825873\n",
      "Epoch 5, Loss: 122.9814029106727\n",
      "Epoch 6, Loss: 119.74785408606895\n",
      "Epoch 7, Loss: 98.96784202869122\n",
      "Epoch 8, Loss: 81.08681473365196\n",
      "Epoch 9, Loss: 73.33914771446815\n",
      "Epoch 10, Loss: 69.48513713249794\n",
      "Epoch 11, Loss: 65.67694766704852\n",
      "Epoch 12, Loss: 53.58638447981615\n",
      "Epoch 13, Loss: 49.2561281644381\n",
      "Epoch 14, Loss: 44.752428128169136\n",
      "Epoch 15, Loss: 39.92720691974346\n",
      "Epoch 16, Loss: 33.73565695835994\n",
      "Epoch 17, Loss: 35.105772458589996\n",
      "Epoch 18, Loss: 31.38615993353037\n",
      "Epoch 19, Loss: 37.27720003861647\n",
      "Epoch 20, Loss: 36.8333136851971\n",
      "Epoch 21, Loss: 44.56097434117244\n",
      "Epoch 22, Loss: 31.52443526341365\n",
      "Epoch 23, Loss: 21.50321025114793\n",
      "Epoch 24, Loss: 18.5702983049246\n",
      "Epoch 25, Loss: 16.826997316800632\n",
      "Epoch 26, Loss: 21.750267542325535\n",
      "Epoch 27, Loss: 20.097373742323654\n",
      "Epoch 28, Loss: 22.16454245493962\n",
      "Epoch 29, Loss: 18.241818868196926\n",
      "Epoch 30, Loss: 17.97756004333496\n",
      "Epoch 31, Loss: 18.662325162153977\n",
      "Epoch 32, Loss: 25.00598195882944\n",
      "Epoch 33, Loss: 20.155859580406776\n",
      "Epoch 34, Loss: 16.48536095252404\n",
      "Epoch 35, Loss: 16.865169323407688\n",
      "Epoch 36, Loss: 14.754642523252047\n",
      "Epoch 37, Loss: 12.831455707550049\n",
      "Epoch 38, Loss: 12.110607275596031\n",
      "Epoch 39, Loss: 13.439855025364803\n",
      "Epoch 40, Loss: 11.70508309511038\n",
      "Epoch 41, Loss: 13.071810759030855\n",
      "Epoch 42, Loss: 11.874743773387028\n",
      "Epoch 43, Loss: 13.478513295833881\n",
      "Epoch 44, Loss: 11.267640132170458\n",
      "Epoch 45, Loss: 10.103531727424034\n",
      "Epoch 46, Loss: 11.772196164497963\n",
      "Epoch 47, Loss: 13.349515291360708\n",
      "Epoch 48, Loss: 16.007515577169563\n",
      "Epoch 49, Loss: 15.96275755075308\n",
      "Epoch 50, Loss: 31.710573783287636\n",
      "Epoch 51, Loss: 32.41933246759268\n",
      "Epoch 52, Loss: 26.183950974391056\n",
      "Epoch 53, Loss: 22.146597128648025\n",
      "Epoch 54, Loss: 14.826758476404043\n",
      "Epoch 55, Loss: 10.209682703018188\n",
      "Epoch 56, Loss: 10.156044299785908\n",
      "Epoch 57, Loss: 10.465869793525108\n",
      "Epoch 58, Loss: 9.67878404030433\n",
      "Epoch 59, Loss: 8.935890986369206\n",
      "Epoch 60, Loss: 9.172862676473764\n",
      "Epoch 61, Loss: 8.676901560563307\n",
      "Epoch 62, Loss: 7.87137561578017\n",
      "Epoch 63, Loss: 7.327603431848379\n",
      "Epoch 64, Loss: 7.161378035178552\n",
      "Epoch 65, Loss: 6.875483329479511\n",
      "Epoch 66, Loss: 8.341541180243858\n",
      "Epoch 67, Loss: 12.618317787463848\n",
      "Epoch 68, Loss: 23.104307431441086\n",
      "Epoch 69, Loss: 15.680447101593018\n",
      "Epoch 70, Loss: 11.78394996202909\n",
      "Epoch 71, Loss: 10.753558800770687\n",
      "Epoch 72, Loss: 9.442558985490065\n",
      "Epoch 73, Loss: 10.02840045782236\n",
      "Epoch 74, Loss: 11.24450764289269\n",
      "Epoch 75, Loss: 9.904395855390108\n",
      "Epoch 76, Loss: 9.446104819958027\n",
      "Epoch 77, Loss: 12.602111486288218\n",
      "Epoch 78, Loss: 12.611666697722216\n",
      "Epoch 79, Loss: 15.36493440774771\n",
      "Epoch 80, Loss: 14.983043560614952\n",
      "Epoch 81, Loss: 15.56812047958374\n",
      "Epoch 82, Loss: 13.919076406038725\n",
      "Epoch 83, Loss: 10.556103999798115\n",
      "Epoch 84, Loss: 9.901089741633488\n",
      "Epoch 85, Loss: 9.798650961655836\n",
      "Epoch 86, Loss: 8.808218680895292\n",
      "Epoch 87, Loss: 8.357134177134586\n",
      "Epoch 88, Loss: 7.606040312693669\n",
      "Epoch 89, Loss: 7.9517044470860405\n",
      "Epoch 90, Loss: 8.743379978033213\n",
      "Epoch 91, Loss: 9.399134232447697\n",
      "Epoch 92, Loss: 15.760463402821468\n",
      "Epoch 93, Loss: 22.2952220256512\n",
      "Epoch 94, Loss: 18.812526042644794\n",
      "Epoch 95, Loss: 19.447370914312508\n",
      "Epoch 96, Loss: 24.17858101771428\n",
      "Epoch 97, Loss: 20.924930535829983\n",
      "Epoch 98, Loss: 27.075497480539177\n",
      "Epoch 99, Loss: 23.50331588891836\n",
      "Epoch 100, Loss: 21.732438674339882\n",
      "Epoch 101, Loss: 18.974141927865837\n",
      "Epoch 102, Loss: 15.476234105917124\n",
      "Epoch 103, Loss: 12.75078876201923\n",
      "Epoch 104, Loss: 13.312480669755201\n",
      "Epoch 105, Loss: 11.864660024642944\n",
      "Epoch 106, Loss: 9.654503877346333\n",
      "Epoch 107, Loss: 8.824771422606249\n",
      "Epoch 108, Loss: 9.351098885902992\n",
      "Epoch 109, Loss: 7.92801281122061\n",
      "Epoch 110, Loss: 7.825377666033232\n",
      "Epoch 111, Loss: 7.449178769038274\n",
      "Epoch 112, Loss: 8.153478384017944\n",
      "Epoch 113, Loss: 7.688534241456252\n",
      "Epoch 114, Loss: 7.2417357334723835\n",
      "Epoch 115, Loss: 7.348860612282386\n",
      "Epoch 116, Loss: 8.594766671840961\n",
      "Epoch 117, Loss: 7.970292513187115\n",
      "Epoch 118, Loss: 8.19840200130756\n",
      "Epoch 119, Loss: 7.734123853536753\n",
      "Epoch 120, Loss: 7.8570780754089355\n",
      "Epoch 121, Loss: 9.562623739242554\n",
      "Epoch 122, Loss: 8.674341128422665\n",
      "Epoch 123, Loss: 7.7046951514024\n",
      "Epoch 124, Loss: 7.0630920850313625\n",
      "Epoch 125, Loss: 7.094459808789766\n",
      "Epoch 126, Loss: 8.389110583525438\n",
      "Epoch 127, Loss: 8.96432034785931\n",
      "Epoch 128, Loss: 9.192074427237877\n",
      "Epoch 129, Loss: 8.374153228906485\n",
      "Epoch 130, Loss: 7.813082676667434\n",
      "Epoch 131, Loss: 12.3662213912377\n",
      "Epoch 132, Loss: 20.19408732194167\n",
      "Epoch 133, Loss: 15.78614198244535\n",
      "Epoch 134, Loss: 15.281846981782179\n",
      "Epoch 135, Loss: 12.36583449290349\n",
      "Epoch 136, Loss: 9.67380272425138\n",
      "Epoch 137, Loss: 9.838050016990074\n",
      "Epoch 138, Loss: 10.22208463228666\n",
      "Epoch 139, Loss: 10.23087296119103\n",
      "Epoch 140, Loss: 9.219342415149395\n",
      "Epoch 141, Loss: 10.984776900364803\n",
      "Epoch 142, Loss: 20.371410883389988\n",
      "Epoch 143, Loss: 16.238034340051506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:13,505] Trial 29 finished with value: 26.152241766052757 and parameters: {'latent_dim_z1': 27, 'latent_dim_z2': 36, 'hidden_dim': 105, 'epochs': 149, 'causal_reg': 0.42154403916340244, 'learning_rate': 0.005117285202640379}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144, Loss: 13.714266098462618\n",
      "Epoch 145, Loss: 12.124386585675753\n",
      "Epoch 146, Loss: 13.049572871281551\n",
      "Epoch 147, Loss: 19.02749307339008\n",
      "Epoch 148, Loss: 15.894344678291908\n",
      "Epoch 149, Loss: 14.382075731570904\n",
      "Epoch 1, Loss: 9033.614727313701\n",
      "Epoch 2, Loss: 589.0077737661509\n",
      "Epoch 3, Loss: 447.1358971228966\n",
      "Epoch 4, Loss: 387.3262780996469\n",
      "Epoch 5, Loss: 332.09312908466046\n",
      "Epoch 6, Loss: 296.3772236750676\n",
      "Epoch 7, Loss: 272.7896602337177\n",
      "Epoch 8, Loss: 259.5786033043495\n",
      "Epoch 9, Loss: 248.63415028498724\n",
      "Epoch 10, Loss: 243.44769609891452\n",
      "Epoch 11, Loss: 236.4770701481746\n",
      "Epoch 12, Loss: 233.38571049616888\n",
      "Epoch 13, Loss: 228.94471505972055\n",
      "Epoch 14, Loss: 226.1567508990948\n",
      "Epoch 15, Loss: 221.27851838331955\n",
      "Epoch 16, Loss: 218.41671796945425\n",
      "Epoch 17, Loss: 208.74099115224985\n",
      "Epoch 18, Loss: 208.7707898066594\n",
      "Epoch 19, Loss: 205.2875603895921\n",
      "Epoch 20, Loss: 200.5230219914363\n",
      "Epoch 21, Loss: 191.84100459172174\n",
      "Epoch 22, Loss: 187.2862095466027\n",
      "Epoch 23, Loss: 179.58777794471155\n",
      "Epoch 24, Loss: 174.5620466379019\n",
      "Epoch 25, Loss: 168.53758767934946\n",
      "Epoch 26, Loss: 164.085143162654\n",
      "Epoch 27, Loss: 160.12756171593298\n",
      "Epoch 28, Loss: 146.43811284578763\n",
      "Epoch 29, Loss: 139.70254369882437\n",
      "Epoch 30, Loss: 134.35942283043494\n",
      "Epoch 31, Loss: 124.83388577974759\n",
      "Epoch 32, Loss: 121.04317312974196\n",
      "Epoch 33, Loss: 114.75801350520207\n",
      "Epoch 34, Loss: 106.25668782454271\n",
      "Epoch 35, Loss: 100.70568128732535\n",
      "Epoch 36, Loss: 95.58180559598483\n",
      "Epoch 37, Loss: 92.42346037351169\n",
      "Epoch 38, Loss: 90.51090798011192\n",
      "Epoch 39, Loss: 80.05352277022142\n",
      "Epoch 40, Loss: 73.93747469095084\n",
      "Epoch 41, Loss: 74.26454976888803\n",
      "Epoch 42, Loss: 70.55981724078839\n",
      "Epoch 43, Loss: 62.50359168419471\n",
      "Epoch 44, Loss: 59.7842455643874\n",
      "Epoch 45, Loss: 56.64700940939096\n",
      "Epoch 46, Loss: 56.87866607079139\n",
      "Epoch 47, Loss: 58.43683800330529\n",
      "Epoch 48, Loss: 51.990376032315766\n",
      "Epoch 49, Loss: 45.986451589144195\n",
      "Epoch 50, Loss: 42.72973695168128\n",
      "Epoch 51, Loss: 37.79497557419997\n",
      "Epoch 52, Loss: 35.01493938152607\n",
      "Epoch 53, Loss: 33.43411196195162\n",
      "Epoch 54, Loss: 34.592761259812576\n",
      "Epoch 55, Loss: 34.83046117195716\n",
      "Epoch 56, Loss: 27.65918177824754\n",
      "Epoch 57, Loss: 32.5711449109591\n",
      "Epoch 58, Loss: 26.255050219022312\n",
      "Epoch 59, Loss: 31.663765760568474\n",
      "Epoch 60, Loss: 27.428992161383995\n",
      "Epoch 61, Loss: 26.525370854597824\n",
      "Epoch 62, Loss: 27.64626381947444\n",
      "Epoch 63, Loss: 23.09149353320782\n",
      "Epoch 64, Loss: 22.091128294284527\n",
      "Epoch 65, Loss: 24.313388897822453\n",
      "Epoch 66, Loss: 23.6215909077571\n",
      "Epoch 67, Loss: 27.720550317030686\n",
      "Epoch 68, Loss: 26.505878118368294\n",
      "Epoch 69, Loss: 19.541658804966854\n",
      "Epoch 70, Loss: 16.46003216963548\n",
      "Epoch 71, Loss: 15.735901905940128\n",
      "Epoch 72, Loss: 14.540526866912842\n",
      "Epoch 73, Loss: 17.832716134878304\n",
      "Epoch 74, Loss: 22.075941966130184\n",
      "Epoch 75, Loss: 23.472144713768593\n",
      "Epoch 76, Loss: 17.70988291960496\n",
      "Epoch 77, Loss: 14.402590366510244\n",
      "Epoch 78, Loss: 14.975025140322172\n",
      "Epoch 79, Loss: 14.857187344477726\n",
      "Epoch 80, Loss: 23.270775464864876\n",
      "Epoch 81, Loss: 19.42331490149865\n",
      "Epoch 82, Loss: 20.6712536078233\n",
      "Epoch 83, Loss: 15.842428354116587\n",
      "Epoch 84, Loss: 15.641923464261568\n",
      "Epoch 85, Loss: 14.955794371091402\n",
      "Epoch 86, Loss: 15.494956438358013\n",
      "Epoch 87, Loss: 13.02264455648569\n",
      "Epoch 88, Loss: 12.360051521888145\n",
      "Epoch 89, Loss: 11.362108524029072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:15,592] Trial 30 finished with value: 200.33045752032538 and parameters: {'latent_dim_z1': 17, 'latent_dim_z2': 46, 'hidden_dim': 163, 'epochs': 92, 'causal_reg': 0.26658607942983376, 'learning_rate': 0.020832040967950046}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Loss: 10.507246420933651\n",
      "Epoch 91, Loss: 9.345055286700909\n",
      "Epoch 92, Loss: 9.739590516457191\n",
      "Epoch 1, Loss: 1727.814740694486\n",
      "Epoch 2, Loss: 1016.4694096491887\n",
      "Epoch 3, Loss: 787.2000914353591\n",
      "Epoch 4, Loss: 651.5608819814829\n",
      "Epoch 5, Loss: 585.4591263991135\n",
      "Epoch 6, Loss: 551.6803406935471\n",
      "Epoch 7, Loss: 538.8474942720853\n",
      "Epoch 8, Loss: 533.6709131094126\n",
      "Epoch 9, Loss: 532.058717580942\n",
      "Epoch 10, Loss: 531.5261969933143\n",
      "Epoch 11, Loss: 531.3222204355093\n",
      "Epoch 12, Loss: 531.3135240994967\n",
      "Epoch 13, Loss: 531.3692210270808\n",
      "Epoch 14, Loss: 531.2523363553561\n",
      "Epoch 15, Loss: 531.1599590594952\n",
      "Epoch 16, Loss: 531.2304998544546\n",
      "Epoch 17, Loss: 531.1800730778621\n",
      "Epoch 18, Loss: 531.1177215576172\n",
      "Epoch 19, Loss: 531.322865999662\n",
      "Epoch 20, Loss: 531.1613470224233\n",
      "Epoch 21, Loss: 531.1413885263296\n",
      "Epoch 22, Loss: 531.1574965256912\n",
      "Epoch 23, Loss: 531.145998441256\n",
      "Epoch 24, Loss: 531.2051943265475\n",
      "Epoch 25, Loss: 531.1843519944412\n",
      "Epoch 26, Loss: 531.5996692364032\n",
      "Epoch 27, Loss: 532.3055684016301\n",
      "Epoch 28, Loss: 532.1632989736704\n",
      "Epoch 29, Loss: 531.0521727341873\n",
      "Epoch 30, Loss: 531.2197500375601\n",
      "Epoch 31, Loss: 531.4820873553937\n",
      "Epoch 32, Loss: 531.478033212515\n",
      "Epoch 33, Loss: 531.541368337778\n",
      "Epoch 34, Loss: 531.6567118718074\n",
      "Epoch 35, Loss: 531.2509466317983\n",
      "Epoch 36, Loss: 531.2109668438251\n",
      "Epoch 37, Loss: 531.4058391864484\n",
      "Epoch 38, Loss: 531.4540763268104\n",
      "Epoch 39, Loss: 531.4132226797251\n",
      "Epoch 40, Loss: 531.2750772329478\n",
      "Epoch 41, Loss: 531.2370341374324\n",
      "Epoch 42, Loss: 531.3701793964093\n",
      "Epoch 43, Loss: 531.0909787691556\n",
      "Epoch 44, Loss: 531.1918088472806\n",
      "Epoch 45, Loss: 531.6149027897761\n",
      "Epoch 46, Loss: 531.3860596876877\n",
      "Epoch 47, Loss: 531.2588947002704\n",
      "Epoch 48, Loss: 531.1478024996244\n",
      "Epoch 49, Loss: 531.1957174447866\n",
      "Epoch 50, Loss: 531.7189207810623\n",
      "Epoch 51, Loss: 531.2086780254657\n",
      "Epoch 52, Loss: 531.1554424579327\n",
      "Epoch 53, Loss: 531.3863707322341\n",
      "Epoch 54, Loss: 531.5423947847806\n",
      "Epoch 55, Loss: 531.2222166794998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:16,957] Trial 31 finished with value: 564.8395551534801 and parameters: {'latent_dim_z1': 37, 'latent_dim_z2': 31, 'hidden_dim': 76, 'epochs': 64, 'causal_reg': 0.10682009653587095, 'learning_rate': 0.012497949993327657}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, Loss: 531.0571500338041\n",
      "Epoch 57, Loss: 531.8597564697266\n",
      "Epoch 58, Loss: 531.1906791100135\n",
      "Epoch 59, Loss: 531.8672391451322\n",
      "Epoch 60, Loss: 531.3518072275015\n",
      "Epoch 61, Loss: 531.1355890127329\n",
      "Epoch 62, Loss: 531.5169079120343\n",
      "Epoch 63, Loss: 531.2667206984299\n",
      "Epoch 64, Loss: 531.6973783052885\n",
      "Epoch 1, Loss: 1163.853522667518\n",
      "Epoch 2, Loss: 940.5656292255109\n",
      "Epoch 3, Loss: 837.746076143705\n",
      "Epoch 4, Loss: 781.6952731792743\n",
      "Epoch 5, Loss: 744.7333743755634\n",
      "Epoch 6, Loss: 716.7365388136643\n",
      "Epoch 7, Loss: 691.6170783409706\n",
      "Epoch 8, Loss: 668.2060276911809\n",
      "Epoch 9, Loss: 644.1694118793195\n",
      "Epoch 10, Loss: 616.2952311589167\n",
      "Epoch 11, Loss: 588.4435025728666\n",
      "Epoch 12, Loss: 560.4949176494891\n",
      "Epoch 13, Loss: 533.8376593956581\n",
      "Epoch 14, Loss: 507.7831074641301\n",
      "Epoch 15, Loss: 484.16632666954627\n",
      "Epoch 16, Loss: 462.2629552987906\n",
      "Epoch 17, Loss: 442.5081123938927\n",
      "Epoch 18, Loss: 423.90780757023737\n",
      "Epoch 19, Loss: 407.44694665762097\n",
      "Epoch 20, Loss: 392.17095653827374\n",
      "Epoch 21, Loss: 378.0295509925255\n",
      "Epoch 22, Loss: 365.1845028217022\n",
      "Epoch 23, Loss: 353.43432969313403\n",
      "Epoch 24, Loss: 342.89982575636645\n",
      "Epoch 25, Loss: 332.6242206280048\n",
      "Epoch 26, Loss: 323.6818190354567\n",
      "Epoch 27, Loss: 315.1662453871507\n",
      "Epoch 28, Loss: 307.49275823739856\n",
      "Epoch 29, Loss: 300.14350333580603\n",
      "Epoch 30, Loss: 293.5808393038236\n",
      "Epoch 31, Loss: 287.493720127986\n",
      "Epoch 32, Loss: 281.692083798922\n",
      "Epoch 33, Loss: 276.2746822650616\n",
      "Epoch 34, Loss: 271.49461394089917\n",
      "Epoch 35, Loss: 266.68474050668567\n",
      "Epoch 36, Loss: 262.3139407818134\n",
      "Epoch 37, Loss: 258.34156036376953\n",
      "Epoch 38, Loss: 254.5548433157114\n",
      "Epoch 39, Loss: 251.1909352816068\n",
      "Epoch 40, Loss: 247.47920579176682\n",
      "Epoch 41, Loss: 244.44273904653696\n",
      "Epoch 42, Loss: 241.54949100200946\n",
      "Epoch 43, Loss: 238.41817767803485\n",
      "Epoch 44, Loss: 235.63791275024414\n",
      "Epoch 45, Loss: 233.20475358229416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:18,059] Trial 32 finished with value: 399.4907679577808 and parameters: {'latent_dim_z1': 20, 'latent_dim_z2': 19, 'hidden_dim': 120, 'epochs': 55, 'causal_reg': 0.22655434430110394, 'learning_rate': 4.225511323490025e-05}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Loss: 230.32942933302658\n",
      "Epoch 47, Loss: 228.0037017235389\n",
      "Epoch 48, Loss: 225.53659541790302\n",
      "Epoch 49, Loss: 223.191281245305\n",
      "Epoch 50, Loss: 220.83212573711688\n",
      "Epoch 51, Loss: 218.66561478834885\n",
      "Epoch 52, Loss: 216.39988987262433\n",
      "Epoch 53, Loss: 214.5624771118164\n",
      "Epoch 54, Loss: 212.39190292358398\n",
      "Epoch 55, Loss: 210.42776841383713\n",
      "Epoch 1, Loss: 746.6620137141301\n",
      "Epoch 2, Loss: 361.9271926879883\n",
      "Epoch 3, Loss: 261.9861413515531\n",
      "Epoch 4, Loss: 213.95690448467548\n",
      "Epoch 5, Loss: 195.98968271108774\n",
      "Epoch 6, Loss: 173.83125202472394\n",
      "Epoch 7, Loss: 152.2307632152851\n",
      "Epoch 8, Loss: 136.1670586512639\n",
      "Epoch 9, Loss: 129.8516558133639\n",
      "Epoch 10, Loss: 111.06496766897348\n",
      "Epoch 11, Loss: 105.44610537015475\n",
      "Epoch 12, Loss: 97.3792046033419\n",
      "Epoch 13, Loss: 83.291977588947\n",
      "Epoch 14, Loss: 80.1135362478403\n",
      "Epoch 15, Loss: 67.40447807312012\n",
      "Epoch 16, Loss: 63.28409260969896\n",
      "Epoch 17, Loss: 62.42244859842154\n",
      "Epoch 18, Loss: 51.28114971747765\n",
      "Epoch 19, Loss: 44.66105094322791\n",
      "Epoch 20, Loss: 47.32842489389273\n",
      "Epoch 21, Loss: 42.628267141488884\n",
      "Epoch 22, Loss: 40.19574077312763\n",
      "Epoch 23, Loss: 36.745533796457146\n",
      "Epoch 24, Loss: 34.207862230447624\n",
      "Epoch 25, Loss: 31.011225627018856\n",
      "Epoch 26, Loss: 34.410399216871994\n",
      "Epoch 27, Loss: 33.04674816131592\n",
      "Epoch 28, Loss: 37.48053315969614\n",
      "Epoch 29, Loss: 37.41614389419556\n",
      "Epoch 30, Loss: 43.27708413050725\n",
      "Epoch 31, Loss: 27.0529842376709\n",
      "Epoch 32, Loss: 21.522824654212364\n",
      "Epoch 33, Loss: 21.726380018087532\n",
      "Epoch 34, Loss: 17.023925157693718\n",
      "Epoch 35, Loss: 15.805577498215895\n",
      "Epoch 36, Loss: 14.687183765264658\n",
      "Epoch 37, Loss: 14.674385951115536\n",
      "Epoch 38, Loss: 14.270823625417856\n",
      "Epoch 39, Loss: 12.476722423846905\n",
      "Epoch 40, Loss: 11.960876024686373\n",
      "Epoch 41, Loss: 11.56543175990765\n",
      "Epoch 42, Loss: 14.777204293471117\n",
      "Epoch 43, Loss: 13.08226959521954\n",
      "Epoch 44, Loss: 14.636892538804274\n",
      "Epoch 45, Loss: 17.114446786733772\n",
      "Epoch 46, Loss: 14.882268612201397\n",
      "Epoch 47, Loss: 14.097119147960957\n",
      "Epoch 48, Loss: 11.885435361128588\n",
      "Epoch 49, Loss: 10.67218068929819\n",
      "Epoch 50, Loss: 10.116654854554396\n",
      "Epoch 51, Loss: 11.829874662252573\n",
      "Epoch 52, Loss: 19.056501608628494\n",
      "Epoch 53, Loss: 15.766862429105318\n",
      "Epoch 54, Loss: 12.272023182648878\n",
      "Epoch 55, Loss: 10.377619963425856\n",
      "Epoch 56, Loss: 9.70550667322599\n",
      "Epoch 57, Loss: 9.422734333918644\n",
      "Epoch 58, Loss: 9.917233888919537\n",
      "Epoch 59, Loss: 10.44413709640503\n",
      "Epoch 60, Loss: 9.59929950420673\n",
      "Epoch 61, Loss: 8.76554037974431\n",
      "Epoch 62, Loss: 10.119764126264132\n",
      "Epoch 63, Loss: 9.835784416932325\n",
      "Epoch 64, Loss: 8.644864925971397\n",
      "Epoch 65, Loss: 8.521071635759794\n",
      "Epoch 66, Loss: 7.619899566356953\n",
      "Epoch 67, Loss: 8.478969940772423\n",
      "Epoch 68, Loss: 8.420050419293917\n",
      "Epoch 69, Loss: 8.352358011099009\n",
      "Epoch 70, Loss: 9.314223601267887\n",
      "Epoch 71, Loss: 9.380836156698374\n",
      "Epoch 72, Loss: 8.869573868238009\n",
      "Epoch 73, Loss: 8.612967472810011\n",
      "Epoch 74, Loss: 10.91564099605267\n",
      "Epoch 75, Loss: 10.680979215181791\n",
      "Epoch 76, Loss: 13.35644188294044\n",
      "Epoch 77, Loss: 11.705571303000816\n",
      "Epoch 78, Loss: 10.12099181688749\n",
      "Epoch 79, Loss: 9.754204456622784\n",
      "Epoch 80, Loss: 8.80507623232328\n",
      "Epoch 81, Loss: 8.836671920923086\n",
      "Epoch 82, Loss: 9.306597617956308\n",
      "Epoch 83, Loss: 11.934174262560331\n",
      "Epoch 84, Loss: 16.61175852555495\n",
      "Epoch 85, Loss: 20.10801986547617\n",
      "Epoch 86, Loss: 13.530167964788584\n",
      "Epoch 87, Loss: 14.245732197394737\n",
      "Epoch 88, Loss: 32.1003569823045\n",
      "Epoch 89, Loss: 24.250406430317806\n",
      "Epoch 90, Loss: 18.555667510399452\n",
      "Epoch 91, Loss: 15.787317789517916\n",
      "Epoch 92, Loss: 14.462057957282433\n",
      "Epoch 93, Loss: 12.961525880373442\n",
      "Epoch 94, Loss: 9.319348592024584\n",
      "Epoch 95, Loss: 8.454847207436195\n",
      "Epoch 96, Loss: 8.280244167034443\n",
      "Epoch 97, Loss: 7.8637150801145115\n",
      "Epoch 98, Loss: 6.982385690395649\n",
      "Epoch 99, Loss: 6.772945385712844\n",
      "Epoch 100, Loss: 6.557857256669265\n",
      "Epoch 101, Loss: 6.207104224425096\n",
      "Epoch 102, Loss: 6.113461292707003\n",
      "Epoch 103, Loss: 6.238323853566096\n",
      "Epoch 104, Loss: 6.045543725673969\n",
      "Epoch 105, Loss: 5.956666121116052\n",
      "Epoch 106, Loss: 5.899835403148945\n",
      "Epoch 107, Loss: 5.801467657089233\n",
      "Epoch 108, Loss: 5.81315356034499\n",
      "Epoch 109, Loss: 5.93108083651616\n",
      "Epoch 110, Loss: 6.1695146560668945\n",
      "Epoch 111, Loss: 6.217607956666213\n",
      "Epoch 112, Loss: 6.616894391866831\n",
      "Epoch 113, Loss: 7.13455840257498\n",
      "Epoch 114, Loss: 7.324007896276621\n",
      "Epoch 115, Loss: 7.271375784507165\n",
      "Epoch 116, Loss: 6.904637245031504\n",
      "Epoch 117, Loss: 7.970130406893217\n",
      "Epoch 118, Loss: 9.975841027039747\n",
      "Epoch 119, Loss: 12.550968390244703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:20,447] Trial 33 finished with value: 37.34660420849657 and parameters: {'latent_dim_z1': 29, 'latent_dim_z2': 15, 'hidden_dim': 24, 'epochs': 124, 'causal_reg': 0.3607522818487461, 'learning_rate': 0.0015016335010379547}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: 10.990953592153696\n",
      "Epoch 121, Loss: 13.807629952063927\n",
      "Epoch 122, Loss: 12.793251514434814\n",
      "Epoch 123, Loss: 12.470665198106031\n",
      "Epoch 124, Loss: 12.454620618086595\n",
      "Epoch 1, Loss: 438.2676080557016\n",
      "Epoch 2, Loss: 167.01099586486816\n",
      "Epoch 3, Loss: 108.74612023280217\n",
      "Epoch 4, Loss: 91.53510387127216\n",
      "Epoch 5, Loss: 81.73166839893048\n",
      "Epoch 6, Loss: 69.90126140301044\n",
      "Epoch 7, Loss: 62.38924958155705\n",
      "Epoch 8, Loss: 53.298554530510536\n",
      "Epoch 9, Loss: 50.378408725445084\n",
      "Epoch 10, Loss: 49.009364494910606\n",
      "Epoch 11, Loss: 45.05272428806011\n",
      "Epoch 12, Loss: 42.25274815926185\n",
      "Epoch 13, Loss: 37.35684453524076\n",
      "Epoch 14, Loss: 37.59826043935922\n",
      "Epoch 15, Loss: 38.05805947230412\n",
      "Epoch 16, Loss: 32.27102628121009\n",
      "Epoch 17, Loss: 28.398732918959396\n",
      "Epoch 18, Loss: 28.473206153282753\n",
      "Epoch 19, Loss: 28.06179615167471\n",
      "Epoch 20, Loss: 24.48161554336548\n",
      "Epoch 21, Loss: 22.295569860018215\n",
      "Epoch 22, Loss: 23.56271142225999\n",
      "Epoch 23, Loss: 22.77741971382728\n",
      "Epoch 24, Loss: 21.42691883674035\n",
      "Epoch 25, Loss: 19.817394660069393\n",
      "Epoch 26, Loss: 17.68939109948965\n",
      "Epoch 27, Loss: 19.076190324930046\n",
      "Epoch 28, Loss: 20.367184143800003\n",
      "Epoch 29, Loss: 16.120872827676628\n",
      "Epoch 30, Loss: 17.255771967080925\n",
      "Epoch 31, Loss: 17.471645245185265\n",
      "Epoch 32, Loss: 14.895013405726505\n",
      "Epoch 33, Loss: 13.503484029036303\n",
      "Epoch 34, Loss: 13.207613248091478\n",
      "Epoch 35, Loss: 16.05607293202327\n",
      "Epoch 36, Loss: 13.399495968451866\n",
      "Epoch 37, Loss: 13.456547480363112\n",
      "Epoch 38, Loss: 12.033888193277212\n",
      "Epoch 39, Loss: 12.061692733031053\n",
      "Epoch 40, Loss: 11.479801269677969\n",
      "Epoch 41, Loss: 11.066196496670063\n",
      "Epoch 42, Loss: 11.968042300297665\n",
      "Epoch 43, Loss: 14.856337327223558\n",
      "Epoch 44, Loss: 11.912436210192167\n",
      "Epoch 45, Loss: 12.27105555167565\n",
      "Epoch 46, Loss: 12.101080674391527\n",
      "Epoch 47, Loss: 15.335219016441933\n",
      "Epoch 48, Loss: 16.304958691963783\n",
      "Epoch 49, Loss: 16.210894327897293\n",
      "Epoch 50, Loss: 15.206720645611103\n",
      "Epoch 51, Loss: 16.140571190760685\n",
      "Epoch 52, Loss: 16.136957517037025\n",
      "Epoch 53, Loss: 13.034222419445332\n",
      "Epoch 54, Loss: 13.602806109648485\n",
      "Epoch 55, Loss: 12.38841647368211\n",
      "Epoch 56, Loss: 10.624107067401592\n",
      "Epoch 57, Loss: 12.138851770987877\n",
      "Epoch 58, Loss: 11.327919446505033\n",
      "Epoch 59, Loss: 14.42548473064716\n",
      "Epoch 60, Loss: 14.112275233635536\n",
      "Epoch 61, Loss: 10.071721131985004\n",
      "Epoch 62, Loss: 8.48583934857295\n",
      "Epoch 63, Loss: 8.32362174987793\n",
      "Epoch 64, Loss: 9.005279852793766\n",
      "Epoch 65, Loss: 8.87859529715318\n",
      "Epoch 66, Loss: 12.538246558262752\n",
      "Epoch 67, Loss: 9.59999704360962\n",
      "Epoch 68, Loss: 8.198135889493502\n",
      "Epoch 69, Loss: 7.942460977114164\n",
      "Epoch 70, Loss: 7.41289995266841\n",
      "Epoch 71, Loss: 8.898721144749569\n",
      "Epoch 72, Loss: 11.604362084315373\n",
      "Epoch 73, Loss: 10.461889028549194\n",
      "Epoch 74, Loss: 11.700220658228947\n",
      "Epoch 75, Loss: 8.910995373359093\n",
      "Epoch 76, Loss: 8.201057727520283\n",
      "Epoch 77, Loss: 8.985967507729164\n",
      "Epoch 78, Loss: 10.541967740425697\n",
      "Epoch 79, Loss: 12.449319380980272\n",
      "Epoch 80, Loss: 12.196812134522657\n",
      "Epoch 81, Loss: 10.772421433375431\n",
      "Epoch 82, Loss: 9.531761921369112\n",
      "Epoch 83, Loss: 9.116328404499935\n",
      "Epoch 84, Loss: 8.590757498374352\n",
      "Epoch 85, Loss: 8.466923126807579\n",
      "Epoch 86, Loss: 8.313948246148916\n",
      "Epoch 87, Loss: 8.278070119711069\n",
      "Epoch 88, Loss: 7.873839671795185\n",
      "Epoch 89, Loss: 7.408545897557185\n",
      "Epoch 90, Loss: 7.140339099443876\n",
      "Epoch 91, Loss: 7.24719498707698\n",
      "Epoch 92, Loss: 7.195704258405245\n",
      "Epoch 93, Loss: 7.65093102821937\n",
      "Epoch 94, Loss: 10.898038680736835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:22,492] Trial 34 finished with value: 25.670941137541558 and parameters: {'latent_dim_z1': 14, 'latent_dim_z2': 43, 'hidden_dim': 65, 'epochs': 96, 'causal_reg': 0.5803281432526084, 'learning_rate': 0.0037298690573701887}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, Loss: 15.121803980607252\n",
      "Epoch 96, Loss: 13.0030486033513\n",
      "Epoch 1, Loss: 7788.669346149151\n",
      "Epoch 2, Loss: 310.0273669316218\n",
      "Epoch 3, Loss: 235.224918952355\n",
      "Epoch 4, Loss: 221.36906755887546\n",
      "Epoch 5, Loss: 220.13066541231595\n",
      "Epoch 6, Loss: 219.7594848045936\n",
      "Epoch 7, Loss: 219.820804302509\n",
      "Epoch 8, Loss: 219.75478920569788\n",
      "Epoch 9, Loss: 219.47911952092096\n",
      "Epoch 10, Loss: 219.37721399160532\n",
      "Epoch 11, Loss: 219.31024522047775\n",
      "Epoch 12, Loss: 220.07716868473932\n",
      "Epoch 13, Loss: 219.30661040086014\n",
      "Epoch 14, Loss: 219.1427820645846\n",
      "Epoch 15, Loss: 219.16033876859225\n",
      "Epoch 16, Loss: 219.1844024658203\n",
      "Epoch 17, Loss: 219.1308244558481\n",
      "Epoch 18, Loss: 219.11759479229266\n",
      "Epoch 19, Loss: 219.1273571894719\n",
      "Epoch 20, Loss: 219.1162332388071\n",
      "Epoch 21, Loss: 218.93901736919696\n",
      "Epoch 22, Loss: 219.19182968139648\n",
      "Epoch 23, Loss: 219.18807689960187\n",
      "Epoch 24, Loss: 219.05850219726562\n",
      "Epoch 25, Loss: 219.2791346036471\n",
      "Epoch 26, Loss: 218.7001753586989\n",
      "Epoch 27, Loss: 219.44648126455454\n",
      "Epoch 28, Loss: 219.2456656235915\n",
      "Epoch 29, Loss: 218.9770839397724\n",
      "Epoch 30, Loss: 219.02086932842548\n",
      "Epoch 31, Loss: 218.94380892240085\n",
      "Epoch 32, Loss: 219.16138106126053\n",
      "Epoch 33, Loss: 218.77145444429837\n",
      "Epoch 34, Loss: 219.08606573251578\n",
      "Epoch 35, Loss: 218.90815177330603\n",
      "Epoch 36, Loss: 219.53219956618088\n",
      "Epoch 37, Loss: 219.60597903911884\n",
      "Epoch 38, Loss: 219.1479342533992\n",
      "Epoch 39, Loss: 219.07974712665265\n",
      "Epoch 40, Loss: 219.1640912569486\n",
      "Epoch 41, Loss: 219.25141231830304\n",
      "Epoch 42, Loss: 219.51269443218524\n",
      "Epoch 43, Loss: 219.26560299213116\n",
      "Epoch 44, Loss: 218.8571037879357\n",
      "Epoch 45, Loss: 219.3078921391414\n",
      "Epoch 46, Loss: 219.06817377530612\n",
      "Epoch 47, Loss: 218.99481729360727\n",
      "Epoch 48, Loss: 219.18431678185095\n",
      "Epoch 49, Loss: 218.83476316011868\n",
      "Epoch 50, Loss: 220.0047390277569\n",
      "Epoch 51, Loss: 219.73448885404147\n",
      "Epoch 52, Loss: 220.3853633587177\n",
      "Epoch 53, Loss: 220.12301694429837\n",
      "Epoch 54, Loss: 219.15094170203577\n",
      "Epoch 55, Loss: 219.43668130727914\n",
      "Epoch 56, Loss: 219.44915390014648\n",
      "Epoch 57, Loss: 219.06065779465897\n",
      "Epoch 58, Loss: 219.22108459472656\n",
      "Epoch 59, Loss: 219.00471995427057\n",
      "Epoch 60, Loss: 218.94639528714694\n",
      "Epoch 61, Loss: 219.42854514488806\n",
      "Epoch 62, Loss: 219.10516826923077\n",
      "Epoch 63, Loss: 219.07754604633038\n",
      "Epoch 64, Loss: 219.49175130403958\n",
      "Epoch 65, Loss: 219.219240628756\n",
      "Epoch 66, Loss: 218.7816162109375\n",
      "Epoch 67, Loss: 219.14811471792368\n",
      "Epoch 68, Loss: 219.03236169081467\n",
      "Epoch 69, Loss: 218.91487473707932\n",
      "Epoch 70, Loss: 219.24598957942084\n",
      "Epoch 71, Loss: 219.02641677856445\n",
      "Epoch 72, Loss: 219.6100585644062\n",
      "Epoch 73, Loss: 218.99318430973932\n",
      "Epoch 74, Loss: 218.92176114595853\n",
      "Epoch 75, Loss: 218.90138127253607\n",
      "Epoch 76, Loss: 219.45352965134842\n",
      "Epoch 77, Loss: 220.28460517296423\n",
      "Epoch 78, Loss: 219.57130373441257\n",
      "Epoch 79, Loss: 219.35006977961615\n",
      "Epoch 80, Loss: 219.15094434298\n",
      "Epoch 81, Loss: 219.58089329646185\n",
      "Epoch 82, Loss: 219.06276556161734\n",
      "Epoch 83, Loss: 219.38628328763522\n",
      "Epoch 84, Loss: 219.21574783325195\n",
      "Epoch 85, Loss: 219.21680215688852\n",
      "Epoch 86, Loss: 219.08737476055438\n",
      "Epoch 87, Loss: 219.20434746375452\n",
      "Epoch 88, Loss: 219.13673753004807\n",
      "Epoch 89, Loss: 219.29470267662634\n",
      "Epoch 90, Loss: 219.3609956594614\n",
      "Epoch 91, Loss: 218.98492255577673\n",
      "Epoch 92, Loss: 218.92304728581354\n",
      "Epoch 93, Loss: 219.1768071101262\n",
      "Epoch 94, Loss: 219.4504262484037\n",
      "Epoch 95, Loss: 218.92996215820312\n",
      "Epoch 96, Loss: 219.78074088463416\n",
      "Epoch 97, Loss: 219.06510719886194\n",
      "Epoch 98, Loss: 219.17276235727164\n",
      "Epoch 99, Loss: 219.15253888643704\n",
      "Epoch 100, Loss: 219.5525131225586\n",
      "Epoch 101, Loss: 219.40644748394305\n",
      "Epoch 102, Loss: 219.33289043719952\n",
      "Epoch 103, Loss: 219.5143086360051\n",
      "Epoch 104, Loss: 219.3115131671612\n",
      "Epoch 105, Loss: 218.90430861253006\n",
      "Epoch 106, Loss: 219.7056667621319\n",
      "Epoch 107, Loss: 219.73904272226187\n",
      "Epoch 108, Loss: 219.07447110689603\n",
      "Epoch 109, Loss: 218.88765100332407\n",
      "Epoch 110, Loss: 219.21005337054913\n",
      "Epoch 111, Loss: 219.15587909405048\n",
      "Epoch 112, Loss: 219.8495812049279\n",
      "Epoch 113, Loss: 219.56756533109225\n",
      "Epoch 114, Loss: 219.73767764751727\n",
      "Epoch 115, Loss: 220.055298438439\n",
      "Epoch 116, Loss: 219.07006454467773\n",
      "Epoch 117, Loss: 219.556578709529\n",
      "Epoch 118, Loss: 219.33235696645883\n",
      "Epoch 119, Loss: 219.09299850463867\n",
      "Epoch 120, Loss: 218.97588407076321\n",
      "Epoch 121, Loss: 219.72228358342096\n",
      "Epoch 122, Loss: 219.7157120337853\n",
      "Epoch 123, Loss: 221.64220927311823\n",
      "Epoch 124, Loss: 219.76990274282602\n",
      "Epoch 125, Loss: 219.92118248572717\n",
      "Epoch 126, Loss: 219.1931994511531\n",
      "Epoch 127, Loss: 218.95791537945087\n",
      "Epoch 128, Loss: 220.10618239182693\n",
      "Epoch 129, Loss: 219.07344436645508\n",
      "Epoch 130, Loss: 219.2408555837778\n",
      "Epoch 131, Loss: 218.9102536714994\n",
      "Epoch 132, Loss: 218.9297752380371\n",
      "Epoch 133, Loss: 219.02561862652118\n",
      "Epoch 134, Loss: 219.05514585054837\n",
      "Epoch 135, Loss: 219.12911561819223\n",
      "Epoch 136, Loss: 219.13906654944788\n",
      "Epoch 137, Loss: 219.38507813673752\n",
      "Epoch 138, Loss: 218.89083216740534\n",
      "Epoch 139, Loss: 219.2037828885592\n",
      "Epoch 140, Loss: 219.24778072650616\n",
      "Epoch 141, Loss: 219.2575833247258\n",
      "Epoch 142, Loss: 219.42537747896634\n",
      "Epoch 143, Loss: 219.15046633206882\n",
      "Epoch 144, Loss: 219.2957787146935\n",
      "Epoch 145, Loss: 219.2904613201435\n",
      "Epoch 146, Loss: 219.04672798743616\n",
      "Epoch 147, Loss: 218.8939891228309\n",
      "Epoch 148, Loss: 219.0379861684946\n",
      "Epoch 149, Loss: 219.33806815514197\n",
      "Epoch 150, Loss: 219.77570548424353\n",
      "Epoch 151, Loss: 219.20809408334586\n",
      "Epoch 152, Loss: 219.76511412400467\n",
      "Epoch 153, Loss: 219.10517384455756\n",
      "Epoch 154, Loss: 219.41675010094275\n",
      "Epoch 155, Loss: 219.30608543982873\n",
      "Epoch 156, Loss: 219.1837026155912\n",
      "Epoch 157, Loss: 219.06274560781625\n",
      "Epoch 158, Loss: 219.70527032705454\n",
      "Epoch 159, Loss: 218.88202637892502\n",
      "Epoch 160, Loss: 219.43282259427585\n",
      "Epoch 161, Loss: 219.5417973445012\n",
      "Epoch 162, Loss: 219.09703944279596\n",
      "Epoch 163, Loss: 219.36051442072943\n",
      "Epoch 164, Loss: 219.30797107403095\n",
      "Epoch 165, Loss: 220.05180212167593\n",
      "Epoch 166, Loss: 219.2651035602276\n",
      "Epoch 167, Loss: 219.94522754962628\n",
      "Epoch 168, Loss: 219.64193138709436\n",
      "Epoch 169, Loss: 219.45918743426984\n",
      "Epoch 170, Loss: 218.87554110013522\n",
      "Epoch 171, Loss: 218.90304506742038\n",
      "Epoch 172, Loss: 219.82834126399115\n",
      "Epoch 173, Loss: 219.4061989417443\n",
      "Epoch 174, Loss: 219.18445381751428\n",
      "Epoch 175, Loss: 219.49922708364633\n",
      "Epoch 176, Loss: 218.90598531869742\n",
      "Epoch 177, Loss: 219.71020243718073\n",
      "Epoch 178, Loss: 220.1638033940242\n",
      "Epoch 179, Loss: 218.79181583111102\n",
      "Epoch 180, Loss: 218.80072901799127\n",
      "Epoch 181, Loss: 219.4127370394193\n",
      "Epoch 182, Loss: 219.27391932560846\n",
      "Epoch 183, Loss: 219.38996975238507\n",
      "Epoch 184, Loss: 218.90687443659857\n",
      "Epoch 185, Loss: 218.99484693087064\n",
      "Epoch 186, Loss: 219.01471974299506\n",
      "Epoch 187, Loss: 219.0110740661621\n",
      "Epoch 188, Loss: 220.10319078885593\n",
      "Epoch 189, Loss: 219.2641096848708\n",
      "Epoch 190, Loss: 219.17305168738733\n",
      "Epoch 191, Loss: 219.82799794123724\n",
      "Epoch 192, Loss: 219.4044562119704\n",
      "Epoch 193, Loss: 219.8942956190843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:26,550] Trial 35 finished with value: 258.29910757577954 and parameters: {'latent_dim_z1': 15, 'latent_dim_z2': 14, 'hidden_dim': 147, 'epochs': 197, 'causal_reg': 0.08845632039086647, 'learning_rate': 0.03293217077205056}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, Loss: 220.54936482356146\n",
      "Epoch 195, Loss: 219.45146912794846\n",
      "Epoch 196, Loss: 219.3160051199106\n",
      "Epoch 197, Loss: 219.90474466177133\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 15:51:28,478] Trial 36 failed with parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 28, 'hidden_dim': 86, 'epochs': 99, 'causal_reg': 0.0042699099751557935, 'learning_rate': 0.09990069761624444} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 15:51:28,478] Trial 36 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 1, Loss: 395.6698790330153\n",
      "Epoch 2, Loss: 190.72028790987454\n",
      "Epoch 3, Loss: 123.32838029127855\n",
      "Epoch 4, Loss: 97.45517466618465\n",
      "Epoch 5, Loss: 85.27116276667668\n",
      "Epoch 6, Loss: 77.17025096599872\n",
      "Epoch 7, Loss: 70.0986424959623\n",
      "Epoch 8, Loss: 65.57111065204327\n",
      "Epoch 9, Loss: 61.17701295705942\n",
      "Epoch 10, Loss: 57.61681035848764\n",
      "Epoch 11, Loss: 54.703862777123085\n",
      "Epoch 12, Loss: 49.81957817077637\n",
      "Epoch 13, Loss: 46.84025999215933\n",
      "Epoch 14, Loss: 45.033044228186974\n",
      "Epoch 15, Loss: 43.2662248978248\n",
      "Epoch 16, Loss: 40.26621040931115\n",
      "Epoch 17, Loss: 41.87060598226694\n",
      "Epoch 18, Loss: 36.708595532637375\n",
      "Epoch 19, Loss: 39.73244355275081\n",
      "Epoch 20, Loss: 34.22831880129301\n",
      "Epoch 21, Loss: 31.771513645465557\n",
      "Epoch 22, Loss: 32.98182417796208\n",
      "Epoch 23, Loss: 30.495284997499905\n",
      "Epoch 24, Loss: 27.526801916269157\n",
      "Epoch 25, Loss: 26.569597757779636\n",
      "Epoch 26, Loss: 27.157700025118313\n",
      "Epoch 27, Loss: 25.31331832592304\n",
      "Epoch 28, Loss: 23.39020923467783\n",
      "Epoch 29, Loss: 21.37794197522677\n",
      "Epoch 30, Loss: 21.045059094062218\n",
      "Epoch 31, Loss: 20.989749394930325\n",
      "Epoch 32, Loss: 20.86610529972957\n",
      "Epoch 33, Loss: 21.102513239933895\n",
      "Epoch 34, Loss: 17.795485019683838\n",
      "Epoch 35, Loss: 17.41022876592783\n",
      "Epoch 36, Loss: 16.629098433714645\n",
      "Epoch 37, Loss: 15.486279560969425\n",
      "Epoch 38, Loss: 15.069859137901894\n",
      "Epoch 39, Loss: 14.536118617424599\n",
      "Epoch 40, Loss: 14.341690466954159\n",
      "Epoch 41, Loss: 13.930651022837711\n",
      "Epoch 42, Loss: 12.837820860055777\n",
      "Epoch 43, Loss: 13.385264726785513\n",
      "Epoch 44, Loss: 12.559508837186373\n",
      "Epoch 45, Loss: 12.059589752784142\n",
      "Epoch 46, Loss: 12.251473316779503\n",
      "Epoch 47, Loss: 11.302554350632887\n",
      "Epoch 48, Loss: 11.217617475069487\n",
      "Epoch 49, Loss: 10.919851871637198\n",
      "Epoch 50, Loss: 10.190703978905312\n",
      "Epoch 51, Loss: 10.57604663188641\n",
      "Epoch 52, Loss: 9.96817896916316\n",
      "Epoch 53, Loss: 12.445991699512188\n",
      "Epoch 54, Loss: 11.958503631445078\n",
      "Epoch 55, Loss: 10.33614576779879\n",
      "Epoch 56, Loss: 9.685786467332106\n",
      "Epoch 57, Loss: 9.825800235454853\n",
      "Epoch 58, Loss: 9.097537114070011\n",
      "Epoch 59, Loss: 9.247078840549175\n",
      "Epoch 60, Loss: 9.348431477179894\n",
      "Epoch 61, Loss: 8.857325352155245\n",
      "Epoch 62, Loss: 8.6616561963008\n",
      "Epoch 63, Loss: 8.379528027314405\n",
      "Epoch 64, Loss: 8.726840019226074\n",
      "Epoch 65, Loss: 8.591206605617817\n",
      "Epoch 66, Loss: 9.184518612348116\n",
      "Epoch 67, Loss: 9.275772113066454\n",
      "Epoch 68, Loss: 9.78185591330895\n",
      "Epoch 69, Loss: 9.18212577012869\n",
      "Epoch 70, Loss: 10.246389572436993\n",
      "Epoch 71, Loss: 9.368538819826567\n",
      "Epoch 72, Loss: 8.194884832088764\n",
      "Epoch 73, Loss: 8.114171725053053\n",
      "Epoch 74, Loss: 7.600160983892588\n",
      "Epoch 75, Loss: 7.357186555862427\n",
      "Epoch 76, Loss: 7.185234986818754\n",
      "Epoch 77, Loss: 7.166874793859629\n",
      "Epoch 78, Loss: 7.324909191865188\n",
      "Epoch 79, Loss: 7.522619852652917\n",
      "Epoch 80, Loss: 7.273916849723229\n",
      "Epoch 81, Loss: 7.250788945418138\n",
      "Epoch 82, Loss: 7.049685001373291\n",
      "Epoch 83, Loss: 7.027305823106032\n",
      "Epoch 84, Loss: 7.011771770624014\n",
      "Epoch 85, Loss: 7.837581176024217\n",
      "Epoch 86, Loss: 7.602059822816115\n",
      "Epoch 87, Loss: 7.408132443061242\n",
      "Epoch 88, Loss: 7.324579917467558\n",
      "Epoch 89, Loss: 7.541658566548274\n",
      "Epoch 90, Loss: 7.421731361976037\n",
      "Epoch 91, Loss: 7.078961280676035\n",
      "Epoch 92, Loss: 6.781226781698374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:30,411] Trial 37 finished with value: 26.71563282660449 and parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 29, 'hidden_dim': 81, 'epochs': 98, 'causal_reg': 0.15041399014915682, 'learning_rate': 0.0010296975612966222}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, Loss: 6.597508283761831\n",
      "Epoch 94, Loss: 6.573695824696467\n",
      "Epoch 95, Loss: 6.64645869915302\n",
      "Epoch 96, Loss: 7.065396639016958\n",
      "Epoch 97, Loss: 7.393670412210318\n",
      "Epoch 98, Loss: 6.975314928935124\n",
      "Epoch 1, Loss: 462.174067570613\n",
      "Epoch 2, Loss: 338.51795489971454\n",
      "Epoch 3, Loss: 256.18262070875903\n",
      "Epoch 4, Loss: 185.02284592848557\n",
      "Epoch 5, Loss: 142.06517894451434\n",
      "Epoch 6, Loss: 119.91573788569524\n",
      "Epoch 7, Loss: 107.42643495706412\n",
      "Epoch 8, Loss: 97.93109908470741\n",
      "Epoch 9, Loss: 92.35839990469125\n",
      "Epoch 10, Loss: 86.85340844667874\n",
      "Epoch 11, Loss: 82.1582670211792\n",
      "Epoch 12, Loss: 77.97642370370718\n",
      "Epoch 13, Loss: 74.1329890031081\n",
      "Epoch 14, Loss: 71.29988919771634\n",
      "Epoch 15, Loss: 68.61919182997484\n",
      "Epoch 16, Loss: 66.23700912182147\n",
      "Epoch 17, Loss: 63.80819093264066\n",
      "Epoch 18, Loss: 62.01258762066181\n",
      "Epoch 19, Loss: 59.358536573556755\n",
      "Epoch 20, Loss: 57.13448781233568\n",
      "Epoch 21, Loss: 55.90091661306528\n",
      "Epoch 22, Loss: 53.47439670562744\n",
      "Epoch 23, Loss: 51.28760205782377\n",
      "Epoch 24, Loss: 50.20333370795617\n",
      "Epoch 25, Loss: 48.46229582566481\n",
      "Epoch 26, Loss: 47.06749938084529\n",
      "Epoch 27, Loss: 45.47419305948111\n",
      "Epoch 28, Loss: 43.133552037752594\n",
      "Epoch 29, Loss: 41.298077033116265\n",
      "Epoch 30, Loss: 40.730315281794624\n",
      "Epoch 31, Loss: 38.81694217828604\n",
      "Epoch 32, Loss: 37.85375653780424\n",
      "Epoch 33, Loss: 36.4422414119427\n",
      "Epoch 34, Loss: 35.594635009765625\n",
      "Epoch 35, Loss: 36.30584239959717\n",
      "Epoch 36, Loss: 33.3992422910837\n",
      "Epoch 37, Loss: 31.63511481651893\n",
      "Epoch 38, Loss: 30.159387808579666\n",
      "Epoch 39, Loss: 29.310485766484188\n",
      "Epoch 40, Loss: 28.151606486393856\n",
      "Epoch 41, Loss: 26.54762337757991\n",
      "Epoch 42, Loss: 26.414928179520828\n",
      "Epoch 43, Loss: 25.070407463954044\n",
      "Epoch 44, Loss: 24.852756940401516\n",
      "Epoch 45, Loss: 23.994616710222683\n",
      "Epoch 46, Loss: 23.049516421097977\n",
      "Epoch 47, Loss: 22.75073660337008\n",
      "Epoch 48, Loss: 21.131654812739445\n",
      "Epoch 49, Loss: 20.503353962531456\n",
      "Epoch 50, Loss: 20.8056618983929\n",
      "Epoch 51, Loss: 20.767705770639274\n",
      "Epoch 52, Loss: 18.954369655022255\n",
      "Epoch 53, Loss: 18.27503991127014\n",
      "Epoch 54, Loss: 17.660668189709003\n",
      "Epoch 55, Loss: 17.364839260394756\n",
      "Epoch 56, Loss: 16.66452268453745\n",
      "Epoch 57, Loss: 16.477457633385292\n",
      "Epoch 58, Loss: 16.15574891750629\n",
      "Epoch 59, Loss: 15.510150212507982\n",
      "Epoch 60, Loss: 16.22216818882869\n",
      "Epoch 61, Loss: 15.021906742682823\n",
      "Epoch 62, Loss: 14.586736092200645\n",
      "Epoch 63, Loss: 14.319131594437819\n",
      "Epoch 64, Loss: 14.071072028233456\n",
      "Epoch 65, Loss: 13.133294692406288\n",
      "Epoch 66, Loss: 12.993347461407001\n",
      "Epoch 67, Loss: 12.9286135160006\n",
      "Epoch 68, Loss: 12.365440185253437\n",
      "Epoch 69, Loss: 12.002941168271578\n",
      "Epoch 70, Loss: 11.843245689685528\n",
      "Epoch 71, Loss: 11.545133443979116\n",
      "Epoch 72, Loss: 11.315389229701115\n",
      "Epoch 73, Loss: 11.152539986830492\n",
      "Epoch 74, Loss: 10.942776955091036\n",
      "Epoch 75, Loss: 11.083413912699772\n",
      "Epoch 76, Loss: 10.782205361586351\n",
      "Epoch 77, Loss: 11.036131455348087\n",
      "Epoch 78, Loss: 11.102404300983135\n",
      "Epoch 79, Loss: 10.237176638383131\n",
      "Epoch 80, Loss: 10.024399463947002\n",
      "Epoch 81, Loss: 10.26545790525583\n",
      "Epoch 82, Loss: 9.647027804301334\n",
      "Epoch 83, Loss: 9.344960909623365\n",
      "Epoch 84, Loss: 9.359652464206402\n",
      "Epoch 85, Loss: 9.606455344420214\n",
      "Epoch 86, Loss: 9.20316718174861\n",
      "Epoch 87, Loss: 8.997940870431753\n",
      "Epoch 88, Loss: 8.896183105615469\n",
      "Epoch 89, Loss: 8.895283864094662\n",
      "Epoch 90, Loss: 8.673121544030996\n",
      "Epoch 91, Loss: 8.59243292074937\n",
      "Epoch 92, Loss: 8.367613480641293\n",
      "Epoch 93, Loss: 8.410139175561758\n",
      "Epoch 94, Loss: 8.512483908579899\n",
      "Epoch 95, Loss: 8.162173674656795\n",
      "Epoch 96, Loss: 8.365054020514854\n",
      "Epoch 97, Loss: 7.912591952543992\n",
      "Epoch 98, Loss: 7.929186876003559\n",
      "Epoch 99, Loss: 7.708804442332341\n",
      "Epoch 100, Loss: 7.697409134644729\n",
      "Epoch 101, Loss: 7.65587935080895\n",
      "Epoch 102, Loss: 7.546050346814669\n",
      "Epoch 103, Loss: 7.519069231473482\n",
      "Epoch 104, Loss: 7.52295838869535\n",
      "Epoch 105, Loss: 7.42462864288917\n",
      "Epoch 106, Loss: 7.406867320720966\n",
      "Epoch 107, Loss: 7.404256820678711\n",
      "Epoch 108, Loss: 7.289420879804171\n",
      "Epoch 109, Loss: 7.23992996949416\n",
      "Epoch 110, Loss: 7.275149143659151\n",
      "Epoch 111, Loss: 7.174145936965942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:32,784] Trial 38 finished with value: 36.543934665499926 and parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 27, 'hidden_dim': 93, 'epochs': 119, 'causal_reg': 0.003615072688253426, 'learning_rate': 0.0003821522379795771}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112, Loss: 7.068207795803364\n",
      "Epoch 113, Loss: 6.980813870063195\n",
      "Epoch 114, Loss: 6.97580414551955\n",
      "Epoch 115, Loss: 6.8770168744600735\n",
      "Epoch 116, Loss: 6.928638531611516\n",
      "Epoch 117, Loss: 6.783544668784509\n",
      "Epoch 118, Loss: 6.774248196528508\n",
      "Epoch 119, Loss: 6.786470229809101\n",
      "Epoch 1, Loss: 560.7245137141301\n",
      "Epoch 2, Loss: 196.20096103961652\n",
      "Epoch 3, Loss: 120.24257117051344\n",
      "Epoch 4, Loss: 96.34421730041504\n",
      "Epoch 5, Loss: 74.0666758463933\n",
      "Epoch 6, Loss: 67.36549978989821\n",
      "Epoch 7, Loss: 77.48285396282489\n",
      "Epoch 8, Loss: 57.05706053513747\n",
      "Epoch 9, Loss: 54.8429903617272\n",
      "Epoch 10, Loss: 48.50412896963266\n",
      "Epoch 11, Loss: 42.40993837209848\n",
      "Epoch 12, Loss: 37.63904329446646\n",
      "Epoch 13, Loss: 36.95777086111215\n",
      "Epoch 14, Loss: 36.78708839416504\n",
      "Epoch 15, Loss: 49.097550025353065\n",
      "Epoch 16, Loss: 39.351667367495025\n",
      "Epoch 17, Loss: 29.502941865187424\n",
      "Epoch 18, Loss: 26.20775791314932\n",
      "Epoch 19, Loss: 26.100784411797157\n",
      "Epoch 20, Loss: 26.935290189889763\n",
      "Epoch 21, Loss: 28.462490888742302\n",
      "Epoch 22, Loss: 26.053860297569862\n",
      "Epoch 23, Loss: 26.514823840214657\n",
      "Epoch 24, Loss: 27.536526056436394\n",
      "Epoch 25, Loss: 23.73650547174307\n",
      "Epoch 26, Loss: 22.022354749532845\n",
      "Epoch 27, Loss: 20.049288566295917\n",
      "Epoch 28, Loss: 18.261914546673115\n",
      "Epoch 29, Loss: 18.090647844167854\n",
      "Epoch 30, Loss: 17.990725957430325\n",
      "Epoch 31, Loss: 17.25495433807373\n",
      "Epoch 32, Loss: 21.44275720302875\n",
      "Epoch 33, Loss: 16.703601360321045\n",
      "Epoch 34, Loss: 16.85713115105262\n",
      "Epoch 35, Loss: 15.927370841686542\n",
      "Epoch 36, Loss: 24.723813643822304\n",
      "Epoch 37, Loss: 17.500456168101383\n",
      "Epoch 38, Loss: 14.196985978346605\n",
      "Epoch 39, Loss: 13.337760668534498\n",
      "Epoch 40, Loss: 19.33789609028743\n",
      "Epoch 41, Loss: 18.758153181809647\n",
      "Epoch 42, Loss: 17.782875757951004\n",
      "Epoch 43, Loss: 14.235359503672672\n",
      "Epoch 44, Loss: 15.379178377298208\n",
      "Epoch 45, Loss: 13.357842592092661\n",
      "Epoch 46, Loss: 12.043020138373741\n",
      "Epoch 47, Loss: 11.601489323836107\n",
      "Epoch 48, Loss: 11.531138860262358\n",
      "Epoch 49, Loss: 12.105544512088482\n",
      "Epoch 50, Loss: 13.845822554368238\n",
      "Epoch 51, Loss: 13.130865793961744\n",
      "Epoch 52, Loss: 11.562687103564922\n",
      "Epoch 53, Loss: 14.14522728553185\n",
      "Epoch 54, Loss: 20.27924761405358\n",
      "Epoch 55, Loss: 16.507280056293194\n",
      "Epoch 56, Loss: 14.971619312579815\n",
      "Epoch 57, Loss: 25.537395367255577\n",
      "Epoch 58, Loss: 21.967868236395027\n",
      "Epoch 59, Loss: 14.898759218362661\n",
      "Epoch 60, Loss: 11.447557082543007\n",
      "Epoch 61, Loss: 10.004254194406363\n",
      "Epoch 62, Loss: 10.120782356995802\n",
      "Epoch 63, Loss: 9.842169761657715\n",
      "Epoch 64, Loss: 10.013611866877628\n",
      "Epoch 65, Loss: 10.996136940442598\n",
      "Epoch 66, Loss: 10.79537477860084\n",
      "Epoch 67, Loss: 10.61243042579064\n",
      "Epoch 68, Loss: 9.563488648487972\n",
      "Epoch 69, Loss: 10.688014287215013\n",
      "Epoch 70, Loss: 11.186919964276827\n",
      "Epoch 71, Loss: 10.641430451319767\n",
      "Epoch 72, Loss: 10.433064790872427\n",
      "Epoch 73, Loss: 12.28483478839581\n",
      "Epoch 74, Loss: 15.959779335902287\n",
      "Epoch 75, Loss: 11.194343163416935\n",
      "Epoch 76, Loss: 11.27296099295983\n",
      "Epoch 77, Loss: 11.371391002948467\n",
      "Epoch 78, Loss: 10.065751552581787\n",
      "Epoch 79, Loss: 9.583337306976318\n",
      "Epoch 80, Loss: 8.91295495400062\n",
      "Epoch 81, Loss: 9.500032626665556\n",
      "Epoch 82, Loss: 11.311558760129488\n",
      "Epoch 83, Loss: 11.334603401330801\n",
      "Epoch 84, Loss: 9.742236210749699\n",
      "Epoch 85, Loss: 9.984823410327618\n",
      "Epoch 86, Loss: 12.83845175229586\n",
      "Epoch 87, Loss: 9.996451231149527\n",
      "Epoch 88, Loss: 9.52371254334083\n",
      "Epoch 89, Loss: 9.320050111183754\n",
      "Epoch 90, Loss: 9.736419952832735\n",
      "Epoch 91, Loss: 12.713518637877245\n",
      "Epoch 92, Loss: 24.268122856433575\n",
      "Epoch 93, Loss: 20.749073395362267\n",
      "Epoch 94, Loss: 20.655034872201774\n",
      "Epoch 95, Loss: 14.805393677491407\n",
      "Epoch 96, Loss: 10.70159545311561\n",
      "Epoch 97, Loss: 9.741135652248676\n",
      "Epoch 98, Loss: 9.295240017083975\n",
      "Epoch 99, Loss: 8.83140541956975\n",
      "Epoch 100, Loss: 9.07487678527832\n",
      "Epoch 101, Loss: 8.945391233150776\n",
      "Epoch 102, Loss: 8.953450111242441\n",
      "Epoch 103, Loss: 8.055047842172476\n",
      "Epoch 104, Loss: 7.856190773156973\n",
      "Epoch 105, Loss: 7.828644624123206\n",
      "Epoch 106, Loss: 8.603631569789005\n",
      "Epoch 107, Loss: 9.232835201116709\n",
      "Epoch 108, Loss: 9.466563078073355\n",
      "Epoch 109, Loss: 11.17028234555171\n",
      "Epoch 110, Loss: 11.13522769854619\n",
      "Epoch 111, Loss: 9.072727019970234\n",
      "Epoch 112, Loss: 8.339760138438297\n",
      "Epoch 113, Loss: 9.005382684560923\n",
      "Epoch 114, Loss: 8.751398141567524\n",
      "Epoch 115, Loss: 8.547460794448853\n",
      "Epoch 116, Loss: 12.422813543906578\n",
      "Epoch 117, Loss: 11.018480190863976\n",
      "Epoch 118, Loss: 8.72351046708914\n",
      "Epoch 119, Loss: 8.305706886144785\n",
      "Epoch 120, Loss: 8.961875768808218\n",
      "Epoch 121, Loss: 10.859984526267418\n",
      "Epoch 122, Loss: 11.760695347419151\n",
      "Epoch 123, Loss: 10.79782221867488\n",
      "Epoch 124, Loss: 8.630511302214403\n",
      "Epoch 125, Loss: 8.868574124116163\n",
      "Epoch 126, Loss: 8.345958636357235\n",
      "Epoch 127, Loss: 7.896708708543044\n",
      "Epoch 128, Loss: 7.710851412553054\n",
      "Epoch 129, Loss: 9.094666737776537\n",
      "Epoch 130, Loss: 8.81471848487854\n",
      "Epoch 131, Loss: 9.225716902659489\n",
      "Epoch 132, Loss: 8.153946069570688\n",
      "Epoch 133, Loss: 7.936911986424373\n",
      "Epoch 134, Loss: 8.535694654171284\n",
      "Epoch 135, Loss: 8.54503244620103\n",
      "Epoch 136, Loss: 8.137256970772377\n",
      "Epoch 137, Loss: 9.164084672927856\n",
      "Epoch 138, Loss: 9.17452570108267\n",
      "Epoch 139, Loss: 7.899991548978365\n",
      "Epoch 140, Loss: 7.3294015114124\n",
      "Epoch 141, Loss: 13.460996921245869\n",
      "Epoch 142, Loss: 14.744583973517784\n",
      "Epoch 143, Loss: 11.32177550976093\n",
      "Epoch 144, Loss: 10.000406705416166\n",
      "Epoch 145, Loss: 9.021154880523682\n",
      "Epoch 146, Loss: 10.819941630730263\n",
      "Epoch 147, Loss: 10.238893123773428\n",
      "Epoch 148, Loss: 8.434972029465895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:35,604] Trial 39 finished with value: 22.458790743920993 and parameters: {'latent_dim_z1': 15, 'latent_dim_z2': 20, 'hidden_dim': 46, 'epochs': 148, 'causal_reg': 0.061364145862912126, 'learning_rate': 0.007375747627942743}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 679.5191544752854\n",
      "Epoch 2, Loss: 239.99062758225662\n",
      "Epoch 3, Loss: 160.37196672879733\n",
      "Epoch 4, Loss: 144.49582246633676\n",
      "Epoch 5, Loss: 119.96147639934833\n",
      "Epoch 6, Loss: 115.02456371600812\n",
      "Epoch 7, Loss: 94.1702072436993\n",
      "Epoch 8, Loss: 89.03245617793156\n",
      "Epoch 9, Loss: 80.50838397099422\n",
      "Epoch 10, Loss: 72.55297205998347\n",
      "Epoch 11, Loss: 68.32537056849553\n",
      "Epoch 12, Loss: 67.65115209726187\n",
      "Epoch 13, Loss: 57.15981476123516\n",
      "Epoch 14, Loss: 68.04598294771634\n",
      "Epoch 15, Loss: 63.903171612666206\n",
      "Epoch 16, Loss: 54.51730111929086\n",
      "Epoch 17, Loss: 43.34706214758066\n",
      "Epoch 18, Loss: 38.820443813617416\n",
      "Epoch 19, Loss: 36.60630292158861\n",
      "Epoch 20, Loss: 37.35348943563608\n",
      "Epoch 21, Loss: 38.63660507935744\n",
      "Epoch 22, Loss: 35.89361487902128\n",
      "Epoch 23, Loss: 37.90708805964543\n",
      "Epoch 24, Loss: 46.995819311875564\n",
      "Epoch 25, Loss: 42.39130291571984\n",
      "Epoch 26, Loss: 39.346376382387604\n",
      "Epoch 27, Loss: 29.819852902339054\n",
      "Epoch 28, Loss: 26.354746378385105\n",
      "Epoch 29, Loss: 35.29138667766865\n",
      "Epoch 30, Loss: 25.886379975538986\n",
      "Epoch 31, Loss: 24.074970172001766\n",
      "Epoch 32, Loss: 24.112144470214844\n",
      "Epoch 33, Loss: 24.57916076366718\n",
      "Epoch 34, Loss: 23.467691568227913\n",
      "Epoch 35, Loss: 28.660069428957424\n",
      "Epoch 36, Loss: 35.81357277356661\n",
      "Epoch 37, Loss: 25.815260887145996\n",
      "Epoch 38, Loss: 33.120758074980515\n",
      "Epoch 39, Loss: 31.572025702549862\n",
      "Epoch 40, Loss: 29.861858257880577\n",
      "Epoch 41, Loss: 38.70730400085449\n",
      "Epoch 42, Loss: 43.07054545329167\n",
      "Epoch 43, Loss: 24.310908721043514\n",
      "Epoch 44, Loss: 21.93715708072369\n",
      "Epoch 45, Loss: 21.532119200779842\n",
      "Epoch 46, Loss: 16.229854070223293\n",
      "Epoch 47, Loss: 20.840623928950382\n",
      "Epoch 48, Loss: 30.35791121996366\n",
      "Epoch 49, Loss: 33.431207143343414\n",
      "Epoch 50, Loss: 21.384875150827263\n",
      "Epoch 51, Loss: 15.503014271075909\n",
      "Epoch 52, Loss: 16.36752726481511\n",
      "Epoch 53, Loss: 15.385745965517485\n",
      "Epoch 54, Loss: 13.741416234236498\n",
      "Epoch 55, Loss: 14.123342073880709\n",
      "Epoch 56, Loss: 14.50026064652663\n",
      "Epoch 57, Loss: 16.333613322331356\n",
      "Epoch 58, Loss: 13.51857878611638\n",
      "Epoch 59, Loss: 15.035181614068838\n",
      "Epoch 60, Loss: 19.38936765377338\n",
      "Epoch 61, Loss: 15.54960247186514\n",
      "Epoch 62, Loss: 14.415188826047457\n",
      "Epoch 63, Loss: 13.369614032598642\n",
      "Epoch 64, Loss: 11.982066007760855\n",
      "Epoch 65, Loss: 11.739643243642954\n",
      "Epoch 66, Loss: 15.481943937448355\n",
      "Epoch 67, Loss: 15.768695519520687\n",
      "Epoch 68, Loss: 14.63253432053786\n",
      "Epoch 69, Loss: 13.821451150454008\n",
      "Epoch 70, Loss: 15.153742936941294\n",
      "Epoch 71, Loss: 14.020421284895678\n",
      "Epoch 72, Loss: 16.766375945164608\n",
      "Epoch 73, Loss: 13.356625373546894\n",
      "Epoch 74, Loss: 11.554230488263643\n",
      "Epoch 75, Loss: 10.821084077541645\n",
      "Epoch 76, Loss: 17.922004002791184\n",
      "Epoch 77, Loss: 15.580706486335167\n",
      "Epoch 78, Loss: 13.327552942129282\n",
      "Epoch 79, Loss: 18.136484732994667\n",
      "Epoch 80, Loss: 17.103622986720158\n",
      "Epoch 81, Loss: 38.28882796947773\n",
      "Epoch 82, Loss: 36.18956928986769\n",
      "Epoch 83, Loss: 20.110155985905575\n",
      "Epoch 84, Loss: 14.46233375255878\n",
      "Epoch 85, Loss: 13.119925792400654\n",
      "Epoch 86, Loss: 13.105473921849178\n",
      "Epoch 87, Loss: 13.486892443436842\n",
      "Epoch 88, Loss: 15.04242730140686\n",
      "Epoch 89, Loss: 12.518015182935274\n",
      "Epoch 90, Loss: 11.616087693434496\n",
      "Epoch 91, Loss: 12.256518345612745\n",
      "Epoch 92, Loss: 10.963910653040958\n",
      "Epoch 93, Loss: 11.414883943704458\n",
      "Epoch 94, Loss: 10.672303474866426\n",
      "Epoch 95, Loss: 10.882890536234928\n",
      "Epoch 96, Loss: 11.63562246469351\n",
      "Epoch 97, Loss: 12.092426245029156\n",
      "Epoch 98, Loss: 12.662278505472036\n",
      "Epoch 99, Loss: 11.311440963011522\n",
      "Epoch 100, Loss: 11.30212349158067\n",
      "Epoch 101, Loss: 12.235928755540113\n",
      "Epoch 102, Loss: 11.129891615647535\n",
      "Epoch 103, Loss: 12.197770485511192\n",
      "Epoch 104, Loss: 15.074778685202965\n",
      "Epoch 105, Loss: 13.67700927074139\n",
      "Epoch 106, Loss: 15.020726919174194\n",
      "Epoch 107, Loss: 12.593571259425236\n",
      "Epoch 108, Loss: 10.266701771662785\n",
      "Epoch 109, Loss: 9.571831996624287\n",
      "Epoch 110, Loss: 11.565682631272535\n",
      "Epoch 111, Loss: 11.184024535692656\n",
      "Epoch 112, Loss: 12.641121057363657\n",
      "Epoch 113, Loss: 14.93245924436129\n",
      "Epoch 114, Loss: 13.942950505476732\n",
      "Epoch 115, Loss: 19.553465916560246\n",
      "Epoch 116, Loss: 16.96684819001418\n",
      "Epoch 117, Loss: 18.65037573300875\n",
      "Epoch 118, Loss: 17.93257520749019\n",
      "Epoch 119, Loss: 13.820606470108032\n",
      "Epoch 120, Loss: 14.954410699697641\n",
      "Epoch 121, Loss: 11.984610575896044\n",
      "Epoch 122, Loss: 14.266869251544659\n",
      "Epoch 123, Loss: 13.073839334341196\n",
      "Epoch 124, Loss: 15.163569652117216\n",
      "Epoch 125, Loss: 12.673071109331572\n",
      "Epoch 126, Loss: 13.168922442656298\n",
      "Epoch 127, Loss: 13.548163469021137\n",
      "Epoch 128, Loss: 16.741130608778732\n",
      "Epoch 129, Loss: 15.153707100794865\n",
      "Epoch 130, Loss: 12.704734343748827\n",
      "Epoch 131, Loss: 11.777671208748451\n",
      "Epoch 132, Loss: 11.54964434183561\n",
      "Epoch 133, Loss: 12.348869140331562\n",
      "Epoch 134, Loss: 13.246760038229136\n",
      "Epoch 135, Loss: 13.021725269464346\n",
      "Epoch 136, Loss: 14.690464936769926\n",
      "Epoch 137, Loss: 11.177627398417545\n",
      "Epoch 138, Loss: 11.191737798544077\n",
      "Epoch 139, Loss: 10.753168692955605\n",
      "Epoch 140, Loss: 9.769401476933407\n",
      "Epoch 141, Loss: 10.187887265132023\n",
      "Epoch 142, Loss: 9.875397187012892\n",
      "Epoch 143, Loss: 10.739122720865103\n",
      "Epoch 144, Loss: 10.789237921054546\n",
      "Epoch 145, Loss: 9.972413723285381\n",
      "Epoch 146, Loss: 10.356354163243221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:38,415] Trial 40 finished with value: 30.847491123370624 and parameters: {'latent_dim_z1': 21, 'latent_dim_z2': 21, 'hidden_dim': 34, 'epochs': 150, 'causal_reg': 0.19117877434014047, 'learning_rate': 0.007820265151419465}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147, Loss: 9.87196865448585\n",
      "Epoch 148, Loss: 9.42025034244244\n",
      "Epoch 149, Loss: 9.970458911015438\n",
      "Epoch 150, Loss: 9.725499043097862\n",
      "Epoch 1, Loss: 953.1708409236028\n",
      "Epoch 2, Loss: 413.2110343346229\n",
      "Epoch 3, Loss: 311.6944979154147\n",
      "Epoch 4, Loss: 262.21023178100586\n",
      "Epoch 5, Loss: 242.98319713885968\n",
      "Epoch 6, Loss: 235.64820920504056\n",
      "Epoch 7, Loss: 234.05827771700345\n",
      "Epoch 8, Loss: 233.10028545673077\n",
      "Epoch 9, Loss: 233.01847457885742\n",
      "Epoch 10, Loss: 232.97210223858173\n",
      "Epoch 11, Loss: 233.0252450796274\n",
      "Epoch 12, Loss: 232.96838554969202\n",
      "Epoch 13, Loss: 232.96672674325796\n",
      "Epoch 14, Loss: 233.08930176955002\n",
      "Epoch 15, Loss: 232.9747446500338\n",
      "Epoch 16, Loss: 232.9316887488732\n",
      "Epoch 17, Loss: 232.9534149169922\n",
      "Epoch 18, Loss: 233.08191387469952\n",
      "Epoch 19, Loss: 232.95402233417218\n",
      "Epoch 20, Loss: 232.999146094689\n",
      "Epoch 21, Loss: 232.91983325664813\n",
      "Epoch 22, Loss: 233.00709973848782\n",
      "Epoch 23, Loss: 232.96127964900091\n",
      "Epoch 24, Loss: 233.16646810678336\n",
      "Epoch 25, Loss: 233.18765258789062\n",
      "Epoch 26, Loss: 233.0209136376014\n",
      "Epoch 27, Loss: 232.97903413038986\n",
      "Epoch 28, Loss: 233.03010412362906\n",
      "Epoch 29, Loss: 233.047610943134\n",
      "Epoch 30, Loss: 233.21975443913385\n",
      "Epoch 31, Loss: 232.9629795367901\n",
      "Epoch 32, Loss: 232.94547887948843\n",
      "Epoch 33, Loss: 233.00789701021634\n",
      "Epoch 34, Loss: 232.98994739239032\n",
      "Epoch 35, Loss: 233.18746889554538\n",
      "Epoch 36, Loss: 232.9513065631573\n",
      "Epoch 37, Loss: 233.06118480975812\n",
      "Epoch 38, Loss: 233.40686416625977\n",
      "Epoch 39, Loss: 233.09535510723407\n",
      "Epoch 40, Loss: 233.5836560175969\n",
      "Epoch 41, Loss: 233.05020053570087\n",
      "Epoch 42, Loss: 233.05101893498346\n",
      "Epoch 43, Loss: 233.2440558213454\n",
      "Epoch 44, Loss: 233.10744212223932\n",
      "Epoch 45, Loss: 233.1312687213604\n",
      "Epoch 46, Loss: 233.12975105872522\n",
      "Epoch 47, Loss: 232.9634642967811\n",
      "Epoch 48, Loss: 233.07652106651892\n",
      "Epoch 49, Loss: 232.9694428077111\n",
      "Epoch 50, Loss: 233.14410341702975\n",
      "Epoch 51, Loss: 233.14814083392804\n",
      "Epoch 52, Loss: 233.10344930795523\n",
      "Epoch 53, Loss: 233.0152156536396\n",
      "Epoch 54, Loss: 233.05365753173828\n",
      "Epoch 55, Loss: 233.20415878295898\n",
      "Epoch 56, Loss: 233.1427283653846\n",
      "Epoch 57, Loss: 233.27659782996545\n",
      "Epoch 58, Loss: 233.11409348707932\n",
      "Epoch 59, Loss: 233.1355226956881\n",
      "Epoch 60, Loss: 233.0552635192871\n",
      "Epoch 61, Loss: 233.32553100585938\n",
      "Epoch 62, Loss: 233.59547483004056\n",
      "Epoch 63, Loss: 233.13847409761868\n",
      "Epoch 64, Loss: 233.17853487454929\n",
      "Epoch 65, Loss: 233.47904146634616\n",
      "Epoch 66, Loss: 233.2135743361253\n",
      "Epoch 67, Loss: 233.50653134859525\n",
      "Epoch 68, Loss: 233.11552693293646\n",
      "Epoch 69, Loss: 233.05055530254657\n",
      "Epoch 70, Loss: 233.06278287447415\n",
      "Epoch 71, Loss: 233.33613469050482\n",
      "Epoch 72, Loss: 233.07875178410455\n",
      "Epoch 73, Loss: 233.12874016394983\n",
      "Epoch 74, Loss: 233.1208278949444\n",
      "Epoch 75, Loss: 232.96348190307617\n",
      "Epoch 76, Loss: 233.4848271883451\n",
      "Epoch 77, Loss: 233.0339578481821\n",
      "Epoch 78, Loss: 233.08375211862418\n",
      "Epoch 79, Loss: 233.0198637155386\n",
      "Epoch 80, Loss: 233.08272171020508\n",
      "Epoch 81, Loss: 233.81978137676532\n",
      "Epoch 82, Loss: 232.90790998018704\n",
      "Epoch 83, Loss: 233.26195731529822\n",
      "Epoch 84, Loss: 233.1763693002554\n",
      "Epoch 85, Loss: 233.41167596670297\n",
      "Epoch 86, Loss: 232.90777206420898\n",
      "Epoch 87, Loss: 233.4798067533053\n",
      "Epoch 88, Loss: 233.32525634765625\n",
      "Epoch 89, Loss: 233.13737399761493\n",
      "Epoch 90, Loss: 233.13580380953275\n",
      "Epoch 91, Loss: 233.61901473999023\n",
      "Epoch 92, Loss: 233.1395744910607\n",
      "Epoch 93, Loss: 232.9701156616211\n",
      "Epoch 94, Loss: 232.91935436542218\n",
      "Epoch 95, Loss: 233.02926518366888\n",
      "Epoch 96, Loss: 233.1052469106821\n",
      "Epoch 97, Loss: 233.36129056490384\n",
      "Epoch 98, Loss: 232.92268753051758\n",
      "Epoch 99, Loss: 233.23983471210187\n",
      "Epoch 100, Loss: 233.31955513587366\n",
      "Epoch 101, Loss: 233.05959232036884\n",
      "Epoch 102, Loss: 233.25797609182504\n",
      "Epoch 103, Loss: 233.22795369074896\n",
      "Epoch 104, Loss: 233.0333395737868\n",
      "Epoch 105, Loss: 233.23652736957257\n",
      "Epoch 106, Loss: 233.19864302415115\n",
      "Epoch 107, Loss: 233.2506774022029\n",
      "Epoch 108, Loss: 233.78431965754584\n",
      "Epoch 109, Loss: 233.88931186382587\n",
      "Epoch 110, Loss: 233.20338146503155\n",
      "Epoch 111, Loss: 232.99100259634164\n",
      "Epoch 112, Loss: 233.3204477750338\n",
      "Epoch 113, Loss: 233.1120708172138\n",
      "Epoch 114, Loss: 233.03489420964166\n",
      "Epoch 115, Loss: 233.113039456881\n",
      "Epoch 116, Loss: 233.26177567702072\n",
      "Epoch 117, Loss: 233.06970831064078\n",
      "Epoch 118, Loss: 233.06631587101862\n",
      "Epoch 119, Loss: 233.14224419227014\n",
      "Epoch 120, Loss: 232.9587992154635\n",
      "Epoch 121, Loss: 233.0753000699557\n",
      "Epoch 122, Loss: 233.16447947575494\n",
      "Epoch 123, Loss: 233.18546177790716\n",
      "Epoch 124, Loss: 232.9435885502742\n",
      "Epoch 125, Loss: 233.16515585092398\n",
      "Epoch 126, Loss: 232.95197618924655\n",
      "Epoch 127, Loss: 233.11525080754205\n",
      "Epoch 128, Loss: 233.24232570941632\n",
      "Epoch 129, Loss: 233.28603480412409\n",
      "Epoch 130, Loss: 233.33974280724158\n",
      "Epoch 131, Loss: 233.64713815542368\n",
      "Epoch 132, Loss: 233.0758845989521\n",
      "Epoch 133, Loss: 233.64199799757736\n",
      "Epoch 134, Loss: 233.01990186251126\n",
      "Epoch 135, Loss: 233.41154597355768\n",
      "Epoch 136, Loss: 233.05954595712515\n",
      "Epoch 137, Loss: 233.28644532423752\n",
      "Epoch 138, Loss: 233.23524856567383\n",
      "Epoch 139, Loss: 233.12413670466498\n",
      "Epoch 140, Loss: 233.08967619675857\n",
      "Epoch 141, Loss: 233.20441818237305\n",
      "Epoch 142, Loss: 233.05321385310248\n",
      "Epoch 143, Loss: 233.11528220543494\n",
      "Epoch 144, Loss: 233.00630099956805\n",
      "Epoch 145, Loss: 233.33965800358698\n",
      "Epoch 146, Loss: 233.21780072725736\n",
      "Epoch 147, Loss: 233.3008725092961\n",
      "Epoch 148, Loss: 233.03494409414438\n",
      "Epoch 149, Loss: 233.74592942457932\n",
      "Epoch 150, Loss: 233.31942807711087\n",
      "Epoch 151, Loss: 233.58049451387845\n",
      "Epoch 152, Loss: 233.49056214552658\n",
      "Epoch 153, Loss: 233.2293557387132\n",
      "Epoch 154, Loss: 233.25424693180963\n",
      "Epoch 155, Loss: 232.99971888615534\n",
      "Epoch 156, Loss: 233.12236932607797\n",
      "Epoch 157, Loss: 232.95746524517352\n",
      "Epoch 158, Loss: 233.05227514413687\n",
      "Epoch 159, Loss: 233.01258996816782\n",
      "Epoch 160, Loss: 233.1330739534818\n",
      "Epoch 161, Loss: 233.07631712693436\n",
      "Epoch 162, Loss: 233.01312989455002\n",
      "Epoch 163, Loss: 233.19734925490158\n",
      "Epoch 164, Loss: 233.03616538414587\n",
      "Epoch 165, Loss: 233.22632246751053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:42,019] Trial 41 finished with value: 239.2341917547987 and parameters: {'latent_dim_z1': 16, 'latent_dim_z2': 14, 'hidden_dim': 253, 'epochs': 168, 'causal_reg': 0.06514878448089273, 'learning_rate': 0.015614947059525736}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166, Loss: 233.11347888066217\n",
      "Epoch 167, Loss: 233.0867726252629\n",
      "Epoch 168, Loss: 233.10336215679462\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 15:51:45,359] Trial 42 failed with parameters: {'latent_dim_z1': 39, 'latent_dim_z2': 10, 'hidden_dim': 11, 'epochs': 182, 'causal_reg': 0.15255645630169495, 'learning_rate': 0.08596140853518588} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 15:51:45,359] Trial 42 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182, Loss: nan\n",
      "Epoch 1, Loss: 592.9411274836614\n",
      "Epoch 2, Loss: 236.54815908578726\n",
      "Epoch 3, Loss: 205.69342657235953\n",
      "Epoch 4, Loss: 160.8317046532264\n",
      "Epoch 5, Loss: 141.56163024902344\n",
      "Epoch 6, Loss: 129.4831292079045\n",
      "Epoch 7, Loss: 123.03578156691331\n",
      "Epoch 8, Loss: 121.72976801945613\n",
      "Epoch 9, Loss: 111.27532196044922\n",
      "Epoch 10, Loss: 99.99355404193585\n",
      "Epoch 11, Loss: 106.17119422325722\n",
      "Epoch 12, Loss: 90.62382573347826\n",
      "Epoch 13, Loss: 80.32402500739464\n",
      "Epoch 14, Loss: 80.178250679603\n",
      "Epoch 15, Loss: 78.12049895066481\n",
      "Epoch 16, Loss: 62.15350136390099\n",
      "Epoch 17, Loss: 65.3142799964318\n",
      "Epoch 18, Loss: 68.3706794151893\n",
      "Epoch 19, Loss: 70.97814405881442\n",
      "Epoch 20, Loss: 60.490866074195274\n",
      "Epoch 21, Loss: 55.486777965839096\n",
      "Epoch 22, Loss: 63.018879450284516\n",
      "Epoch 23, Loss: 57.633813271155724\n",
      "Epoch 24, Loss: 46.214572833134575\n",
      "Epoch 25, Loss: 45.0936640225924\n",
      "Epoch 26, Loss: 64.7444561444796\n",
      "Epoch 27, Loss: 45.73096385368934\n",
      "Epoch 28, Loss: 42.224901272700386\n",
      "Epoch 29, Loss: 40.83569658719576\n",
      "Epoch 30, Loss: 41.100563562833344\n",
      "Epoch 31, Loss: 38.396233778733475\n",
      "Epoch 32, Loss: 32.6208533507127\n",
      "Epoch 33, Loss: 33.74999867952787\n",
      "Epoch 34, Loss: 36.2503484579233\n",
      "Epoch 35, Loss: 34.059737132145806\n",
      "Epoch 36, Loss: 32.733793882223274\n",
      "Epoch 37, Loss: 34.89426682545589\n",
      "Epoch 38, Loss: 29.288337597480187\n",
      "Epoch 39, Loss: 27.44852297122662\n",
      "Epoch 40, Loss: 27.221278777489296\n",
      "Epoch 41, Loss: 25.78525370817918\n",
      "Epoch 42, Loss: 31.15260857802171\n",
      "Epoch 43, Loss: 59.26953975970928\n",
      "Epoch 44, Loss: 45.23642943455623\n",
      "Epoch 45, Loss: 37.25823149314294\n",
      "Epoch 46, Loss: 25.522771175091084\n",
      "Epoch 47, Loss: 22.671875660236065\n",
      "Epoch 48, Loss: 20.655372472909782\n",
      "Epoch 49, Loss: 20.25021868485671\n",
      "Epoch 50, Loss: 23.823196044335\n",
      "Epoch 51, Loss: 21.62622587497418\n",
      "Epoch 52, Loss: 20.400805143209602\n",
      "Epoch 53, Loss: 18.415400651785042\n",
      "Epoch 54, Loss: 19.876520670377293\n",
      "Epoch 55, Loss: 17.369415906759407\n",
      "Epoch 56, Loss: 23.315344993884747\n",
      "Epoch 57, Loss: 21.62424799112173\n",
      "Epoch 58, Loss: 23.41050509306101\n",
      "Epoch 59, Loss: 18.472107373751125\n",
      "Epoch 60, Loss: 19.0318299807035\n",
      "Epoch 61, Loss: 21.391457080841064\n",
      "Epoch 62, Loss: 26.709458974691536\n",
      "Epoch 63, Loss: 28.89949208039504\n",
      "Epoch 64, Loss: 19.348364866696873\n",
      "Epoch 65, Loss: 15.726042454059307\n",
      "Epoch 66, Loss: 17.41157418030959\n",
      "Epoch 67, Loss: 19.020094247964714\n",
      "Epoch 68, Loss: 18.104222554426926\n",
      "Epoch 69, Loss: 15.338039361513578\n",
      "Epoch 70, Loss: 18.719856298886814\n",
      "Epoch 71, Loss: 23.12989854812622\n",
      "Epoch 72, Loss: 17.22356631205632\n",
      "Epoch 73, Loss: 17.24534133764414\n",
      "Epoch 74, Loss: 25.38397689966055\n",
      "Epoch 75, Loss: 24.13067311507005\n",
      "Epoch 76, Loss: 27.469603685232308\n",
      "Epoch 77, Loss: 33.836943296285774\n",
      "Epoch 78, Loss: 20.61549601188073\n",
      "Epoch 79, Loss: 22.22160115608802\n",
      "Epoch 80, Loss: 18.30339527130127\n",
      "Epoch 81, Loss: 20.86051794198843\n",
      "Epoch 82, Loss: 18.26209314052875\n",
      "Epoch 83, Loss: 24.931869616875282\n",
      "Epoch 84, Loss: 27.586726298699013\n",
      "Epoch 85, Loss: 15.30563343488253\n",
      "Epoch 86, Loss: 11.28905333005465\n",
      "Epoch 87, Loss: 10.474165567984947\n",
      "Epoch 88, Loss: 10.258228283662062\n",
      "Epoch 89, Loss: 12.070638143099272\n",
      "Epoch 90, Loss: 12.950417591975285\n",
      "Epoch 91, Loss: 11.971974152785082\n",
      "Epoch 92, Loss: 10.939662071374746\n",
      "Epoch 93, Loss: 12.197783305094791\n",
      "Epoch 94, Loss: 13.16665348639855\n",
      "Epoch 95, Loss: 13.377864030691294\n",
      "Epoch 96, Loss: 11.648151544424204\n",
      "Epoch 97, Loss: 12.007911388690655\n",
      "Epoch 98, Loss: 10.516265704081608\n",
      "Epoch 99, Loss: 10.41720854319059\n",
      "Epoch 100, Loss: 14.066025715607863\n",
      "Epoch 101, Loss: 17.240863910088173\n",
      "Epoch 102, Loss: 13.273623649890606\n",
      "Epoch 103, Loss: 11.191010255080004\n",
      "Epoch 104, Loss: 13.681018664286686\n",
      "Epoch 105, Loss: 12.032763334421011\n",
      "Epoch 106, Loss: 17.62927763278668\n",
      "Epoch 107, Loss: 14.398000130286583\n",
      "Epoch 108, Loss: 17.455060445345364\n",
      "Epoch 109, Loss: 14.319638068859394\n",
      "Epoch 110, Loss: 13.621198287376991\n",
      "Epoch 111, Loss: 12.896718208606426\n",
      "Epoch 112, Loss: 11.093711687968327\n",
      "Epoch 113, Loss: 9.944076152948233\n",
      "Epoch 114, Loss: 9.485355615615845\n",
      "Epoch 115, Loss: 11.340837441957914\n",
      "Epoch 116, Loss: 11.067865224984976\n",
      "Epoch 117, Loss: 11.127108738972591\n",
      "Epoch 118, Loss: 13.256714197305532\n",
      "Epoch 119, Loss: 12.178466154978825\n",
      "Epoch 120, Loss: 12.857676836160513\n",
      "Epoch 121, Loss: 11.679030345036434\n",
      "Epoch 122, Loss: 11.578021709735577\n",
      "Epoch 123, Loss: 15.770721105428843\n",
      "Epoch 124, Loss: 15.85783283527081\n",
      "Epoch 125, Loss: 16.61414469205416\n",
      "Epoch 126, Loss: 17.552574487832878\n",
      "Epoch 127, Loss: 22.293639659881592\n",
      "Epoch 128, Loss: 19.55890233700092\n",
      "Epoch 129, Loss: 15.189963964315561\n",
      "Epoch 130, Loss: 12.97573786515456\n",
      "Epoch 131, Loss: 11.04717542574956\n",
      "Epoch 132, Loss: 9.898884479816143\n",
      "Epoch 133, Loss: 10.051480623391958\n",
      "Epoch 134, Loss: 9.178902882796068\n",
      "Epoch 135, Loss: 8.960028813435482\n",
      "Epoch 136, Loss: 9.120235479795015\n",
      "Epoch 137, Loss: 9.11093937433683\n",
      "Epoch 138, Loss: 10.033341536155113\n",
      "Epoch 139, Loss: 10.28019855572627\n",
      "Epoch 140, Loss: 10.688457268934984\n",
      "Epoch 141, Loss: 13.143736179058369\n",
      "Epoch 142, Loss: 24.29143938651452\n",
      "Epoch 143, Loss: 18.27918712909405\n",
      "Epoch 144, Loss: 21.36879931963407\n",
      "Epoch 145, Loss: 22.51414995927077\n",
      "Epoch 146, Loss: 28.689889394319973\n",
      "Epoch 147, Loss: 16.829900081341084\n",
      "Epoch 148, Loss: 13.160807426159199\n",
      "Epoch 149, Loss: 10.981030409152691\n",
      "Epoch 150, Loss: 10.399026430570162\n",
      "Epoch 151, Loss: 9.70710305067209\n",
      "Epoch 152, Loss: 10.88048008772043\n",
      "Epoch 153, Loss: 12.071002079890324\n",
      "Epoch 154, Loss: 11.092097979325514\n",
      "Epoch 155, Loss: 10.454113281690157\n",
      "Epoch 156, Loss: 10.76075133910546\n",
      "Epoch 157, Loss: 8.761882415184608\n",
      "Epoch 158, Loss: 8.919849322392391\n",
      "Epoch 159, Loss: 9.395629277596107\n",
      "Epoch 160, Loss: 9.293642465884869\n",
      "Epoch 161, Loss: 8.917012691497803\n",
      "Epoch 162, Loss: 10.072154063444872\n",
      "Epoch 163, Loss: 12.116278318258432\n",
      "Epoch 164, Loss: 17.446824000431942\n",
      "Epoch 165, Loss: 16.52645393518301\n",
      "Epoch 166, Loss: 17.77675151824951\n",
      "Epoch 167, Loss: 17.275701156029335\n",
      "Epoch 168, Loss: 15.898400673499474\n",
      "Epoch 169, Loss: 12.62441299511836\n",
      "Epoch 170, Loss: 10.108349341612596\n",
      "Epoch 171, Loss: 9.114580411177416\n",
      "Epoch 172, Loss: 9.124044803472666\n",
      "Epoch 173, Loss: 10.832262974518995\n",
      "Epoch 174, Loss: 15.24514075425955\n",
      "Epoch 175, Loss: 18.542005869058464\n",
      "Epoch 176, Loss: 10.7295872431535\n",
      "Epoch 177, Loss: 9.69596187884991\n",
      "Epoch 178, Loss: 8.159430760603685\n",
      "Epoch 179, Loss: 7.8472466835608845\n",
      "Epoch 180, Loss: 7.871109522306002\n",
      "Epoch 181, Loss: 8.1061828319843\n",
      "Epoch 182, Loss: 7.6169077799870415\n",
      "Epoch 183, Loss: 7.84676454617427\n",
      "Epoch 184, Loss: 7.6073318628164435\n",
      "Epoch 185, Loss: 8.05534526017996\n",
      "Epoch 186, Loss: 8.518373544399555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:48,946] Trial 43 finished with value: 31.258359482532583 and parameters: {'latent_dim_z1': 26, 'latent_dim_z2': 17, 'hidden_dim': 49, 'epochs': 187, 'causal_reg': 0.280926087319874, 'learning_rate': 0.003873847513747985}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187, Loss: 8.850522243059599\n",
      "Epoch 1, Loss: 719525.4142010028\n",
      "Epoch 2, Loss: 1971.5872051532451\n",
      "Epoch 3, Loss: 1594.1833836482122\n",
      "Epoch 4, Loss: 1374.7511174128606\n",
      "Epoch 5, Loss: 1246.3548713097205\n",
      "Epoch 6, Loss: 1122.4585301325872\n",
      "Epoch 7, Loss: 1094.2887807992788\n",
      "Epoch 8, Loss: 1044.6391460712139\n",
      "Epoch 9, Loss: 1025.2445361797627\n",
      "Epoch 10, Loss: 969.4601616492638\n",
      "Epoch 11, Loss: 946.3047907902644\n",
      "Epoch 12, Loss: 926.0683194673978\n",
      "Epoch 13, Loss: 1004.4238762488732\n",
      "Epoch 14, Loss: 959.9541590763972\n",
      "Epoch 15, Loss: 905.0236006516677\n",
      "Epoch 16, Loss: 875.1508953387921\n",
      "Epoch 17, Loss: 865.1673220120944\n",
      "Epoch 18, Loss: 877.3184591440054\n",
      "Epoch 19, Loss: 1010.3624643179087\n",
      "Epoch 20, Loss: 949.1438962496244\n",
      "Epoch 21, Loss: 857.1979334904597\n",
      "Epoch 22, Loss: 805.5353686992938\n",
      "Epoch 23, Loss: 786.993167583759\n",
      "Epoch 24, Loss: 777.8771679217999\n",
      "Epoch 25, Loss: 764.1442272479718\n",
      "Epoch 26, Loss: 754.4585653451772\n",
      "Epoch 27, Loss: 739.7542959359976\n",
      "Epoch 28, Loss: 760.7496478740985\n",
      "Epoch 29, Loss: 753.6568474402794\n",
      "Epoch 30, Loss: 702.7541057880109\n",
      "Epoch 31, Loss: 763.861443739671\n",
      "Epoch 32, Loss: 736.6854981642502\n",
      "Epoch 33, Loss: 749.3682873065655\n",
      "Epoch 34, Loss: 692.9818901648888\n",
      "Epoch 35, Loss: 685.9443670419546\n",
      "Epoch 36, Loss: 663.841658959022\n",
      "Epoch 37, Loss: 670.1930729792668\n",
      "Epoch 38, Loss: 674.7870671198918\n",
      "Epoch 39, Loss: 674.9386256291316\n",
      "Epoch 40, Loss: 647.5741712130033\n",
      "Epoch 41, Loss: 640.227165222168\n",
      "Epoch 42, Loss: 640.5871247511643\n",
      "Epoch 43, Loss: 630.8596261831431\n",
      "Epoch 44, Loss: 663.5484290489784\n",
      "Epoch 45, Loss: 676.1502497746394\n",
      "Epoch 46, Loss: 935.1629744309646\n",
      "Epoch 47, Loss: 1041.8059856708232\n",
      "Epoch 48, Loss: 761.4259802011343\n",
      "Epoch 49, Loss: 638.6494017380935\n",
      "Epoch 50, Loss: 664.6221336951622\n",
      "Epoch 51, Loss: 662.2566375732422\n",
      "Epoch 52, Loss: 631.2462580754207\n",
      "Epoch 53, Loss: 953.7381239670974\n",
      "Epoch 54, Loss: 1355.518049973708\n",
      "Epoch 55, Loss: 2029.3558748685396\n",
      "Epoch 56, Loss: 2659.055119441106\n",
      "Epoch 57, Loss: 1711.940448467548\n",
      "Epoch 58, Loss: 1171.2254591721755\n",
      "Epoch 59, Loss: 911.8241260235126\n",
      "Epoch 60, Loss: 914.5205230712891\n",
      "Epoch 61, Loss: 1027.963133591872\n",
      "Epoch 62, Loss: 881.311015202449\n",
      "Epoch 63, Loss: 630.1546718890851\n",
      "Epoch 64, Loss: 620.2296482966497\n",
      "Epoch 65, Loss: 703.7496232252854\n",
      "Epoch 66, Loss: 906.9079801119291\n",
      "Epoch 67, Loss: 1706.878157395583\n",
      "Epoch 68, Loss: 1763.8503241905798\n",
      "Epoch 69, Loss: 1378.8951639028696\n",
      "Epoch 70, Loss: 1055.4152491642878\n",
      "Epoch 71, Loss: 823.515873835637\n",
      "Epoch 72, Loss: 1114.9103945218599\n",
      "Epoch 73, Loss: 1089.7094920231746\n",
      "Epoch 74, Loss: 707.0369403545673\n",
      "Epoch 75, Loss: 660.4614733182467\n",
      "Epoch 76, Loss: 623.1794926570012\n",
      "Epoch 77, Loss: 647.4904362605168\n",
      "Epoch 78, Loss: 635.8300065260667\n",
      "Epoch 79, Loss: 710.7796085064227\n",
      "Epoch 80, Loss: 599.9050128643329\n",
      "Epoch 81, Loss: 607.516471862793\n",
      "Epoch 82, Loss: 616.9688350970929\n",
      "Epoch 83, Loss: 591.1258439284104\n",
      "Epoch 84, Loss: 603.4583300076998\n",
      "Epoch 85, Loss: 596.5604418241061\n",
      "Epoch 86, Loss: 588.4685586782603\n",
      "Epoch 87, Loss: 603.3132429856521\n",
      "Epoch 88, Loss: 670.1390392596905\n",
      "Epoch 89, Loss: 697.539053109976\n",
      "Epoch 90, Loss: 874.8693530742938\n",
      "Epoch 91, Loss: 1113.8690725473257\n",
      "Epoch 92, Loss: 727.9879907461313\n",
      "Epoch 93, Loss: 655.2711052527794\n",
      "Epoch 94, Loss: 576.656494140625\n",
      "Epoch 95, Loss: 628.8736231877253\n",
      "Epoch 96, Loss: 590.8469913189227\n",
      "Epoch 97, Loss: 583.973612858699\n",
      "Epoch 98, Loss: 600.6763370220477\n",
      "Epoch 99, Loss: 581.7639212975135\n",
      "Epoch 100, Loss: 615.7500041081355\n",
      "Epoch 101, Loss: 593.3604096632737\n",
      "Epoch 102, Loss: 597.7052530141978\n",
      "Epoch 103, Loss: 562.5641039334811\n",
      "Epoch 104, Loss: 574.9125366210938\n",
      "Epoch 105, Loss: 556.2292140080378\n",
      "Epoch 106, Loss: 579.0255795992338\n",
      "Epoch 107, Loss: 853.7308842585637\n",
      "Epoch 108, Loss: 844.2559790978065\n",
      "Epoch 109, Loss: 639.393551166241\n",
      "Epoch 110, Loss: 608.5701998197115\n",
      "Epoch 111, Loss: 563.1173177865835\n",
      "Epoch 112, Loss: 815.6865457388071\n",
      "Epoch 113, Loss: 1163.5119006817158\n",
      "Epoch 114, Loss: 661.5376369769757\n",
      "Epoch 115, Loss: 567.6267242431641\n",
      "Epoch 116, Loss: 943.6712552584135\n",
      "Epoch 117, Loss: 1185.9568575345552\n",
      "Epoch 118, Loss: 706.1156927255483\n",
      "Epoch 119, Loss: 560.0169472327599\n",
      "Epoch 120, Loss: 547.2143942025991\n",
      "Epoch 121, Loss: 534.5607211773212\n",
      "Epoch 122, Loss: 549.7298267071063\n",
      "Epoch 123, Loss: 547.5121489304763\n",
      "Epoch 124, Loss: 539.8221975473257\n",
      "Epoch 125, Loss: 542.0471197275015\n",
      "Epoch 126, Loss: 533.090934753418\n",
      "Epoch 127, Loss: 537.8690701998197\n",
      "Epoch 128, Loss: 611.7490938626803\n",
      "Epoch 129, Loss: 543.2573770376353\n",
      "Epoch 130, Loss: 534.5022048950195\n",
      "Epoch 131, Loss: 521.9396591186523\n",
      "Epoch 132, Loss: 525.807137122521\n",
      "Epoch 133, Loss: 518.34008202186\n",
      "Epoch 134, Loss: 534.9849548339844\n",
      "Epoch 135, Loss: 592.2840423583984\n",
      "Epoch 136, Loss: 518.4782286423904\n",
      "Epoch 137, Loss: 536.0585837730995\n",
      "Epoch 138, Loss: 511.99654271052435\n",
      "Epoch 139, Loss: 510.09960115872894\n",
      "Epoch 140, Loss: 524.7643479567307\n",
      "Epoch 141, Loss: 516.5468591543345\n",
      "Epoch 142, Loss: 509.94649094801684\n",
      "Epoch 143, Loss: 512.2593847421499\n",
      "Epoch 144, Loss: 524.1614960890549\n",
      "Epoch 145, Loss: 502.54627110407904\n",
      "Epoch 146, Loss: 497.0139946570763\n",
      "Epoch 147, Loss: 494.1651810866136\n",
      "Epoch 148, Loss: 512.7246692364032\n",
      "Epoch 149, Loss: 556.5884634164663\n",
      "Epoch 150, Loss: 565.0409275935247\n",
      "Epoch 151, Loss: 495.5500787588266\n",
      "Epoch 152, Loss: 508.1362574650691\n",
      "Epoch 153, Loss: 528.2255090566782\n",
      "Epoch 154, Loss: 503.81935853224536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:51:51,660] Trial 44 finished with value: 5325.765366943347 and parameters: {'latent_dim_z1': 15, 'latent_dim_z2': 10, 'hidden_dim': 10, 'epochs': 158, 'causal_reg': 0.13433670053588267, 'learning_rate': 0.02877400402809485}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155, Loss: 575.5293837327224\n",
      "Epoch 156, Loss: 1011.3381605881912\n",
      "Epoch 157, Loss: 618.5414064847506\n",
      "Epoch 158, Loss: 487.57838850754956\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 15:51:54,721] Trial 45 failed with parameters: {'latent_dim_z1': 38, 'latent_dim_z2': 66, 'hidden_dim': 26, 'epochs': 142, 'causal_reg': 0.2234712783775506, 'learning_rate': 0.07511497586040024} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 15:51:54,722] Trial 45 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 15:51:58,606] Trial 46 failed with parameters: {'latent_dim_z1': 41, 'latent_dim_z2': 65, 'hidden_dim': 31, 'epochs': 178, 'causal_reg': 0.22041259150892795, 'learning_rate': 0.09829570278478256} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 15:51:58,606] Trial 46 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 1, Loss: 3.695633752612839e+34\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 15:52:01,432] Trial 47 failed with parameters: {'latent_dim_z1': 39, 'latent_dim_z2': 33, 'hidden_dim': 29, 'epochs': 144, 'causal_reg': 0.3875048698132994, 'learning_rate': 0.07324041784622622} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 15:52:01,432] Trial 47 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 1, Loss: 884.039686349722\n",
      "Epoch 2, Loss: 403.52782440185547\n",
      "Epoch 3, Loss: 355.5263167161208\n",
      "Epoch 4, Loss: 286.9382426922138\n",
      "Epoch 5, Loss: 250.98287024864783\n",
      "Epoch 6, Loss: 222.30862573476938\n",
      "Epoch 7, Loss: 178.74367259098932\n",
      "Epoch 8, Loss: 161.50810006948618\n",
      "Epoch 9, Loss: 148.38675454946664\n",
      "Epoch 10, Loss: 128.07306759174054\n",
      "Epoch 11, Loss: 126.32780221792368\n",
      "Epoch 12, Loss: 102.65937585097093\n",
      "Epoch 13, Loss: 96.95983497913068\n",
      "Epoch 14, Loss: 97.02813368577223\n",
      "Epoch 15, Loss: 87.64921965965858\n",
      "Epoch 16, Loss: 69.08081648899959\n",
      "Epoch 17, Loss: 74.80071940788856\n",
      "Epoch 18, Loss: 62.35987736628606\n",
      "Epoch 19, Loss: 53.526660625751205\n",
      "Epoch 20, Loss: 51.02324999295748\n",
      "Epoch 21, Loss: 47.23752036461463\n",
      "Epoch 22, Loss: 39.90353444906381\n",
      "Epoch 23, Loss: 39.249029893141525\n",
      "Epoch 24, Loss: 34.78500964091374\n",
      "Epoch 25, Loss: 30.02791221325214\n",
      "Epoch 26, Loss: 27.68783063154954\n",
      "Epoch 27, Loss: 22.576873009021465\n",
      "Epoch 28, Loss: 23.80553828752958\n",
      "Epoch 29, Loss: 20.376385010205784\n",
      "Epoch 30, Loss: 18.749616586245022\n",
      "Epoch 31, Loss: 18.039254922133225\n",
      "Epoch 32, Loss: 26.55241713157067\n",
      "Epoch 33, Loss: 34.043577927809494\n",
      "Epoch 34, Loss: 24.952399290524998\n",
      "Epoch 35, Loss: 22.346998361440804\n",
      "Epoch 36, Loss: 19.449481450594387\n",
      "Epoch 37, Loss: 21.515478941110466\n",
      "Epoch 38, Loss: 21.349664578071007\n",
      "Epoch 39, Loss: 19.779733914595383\n",
      "Epoch 40, Loss: 21.7947988143334\n",
      "Epoch 41, Loss: 17.006473027742825\n",
      "Epoch 42, Loss: 17.923620994274433\n",
      "Epoch 43, Loss: 15.513341408509474\n",
      "Epoch 44, Loss: 14.740483944232647\n",
      "Epoch 45, Loss: 13.261486126826359\n",
      "Epoch 46, Loss: 13.39674109679002\n",
      "Epoch 47, Loss: 18.152216856296246\n",
      "Epoch 48, Loss: 20.709793457618126\n",
      "Epoch 49, Loss: 16.07744136223426\n",
      "Epoch 50, Loss: 13.442330048634457\n",
      "Epoch 51, Loss: 13.518534366901104\n",
      "Epoch 52, Loss: 11.074112800451426\n",
      "Epoch 53, Loss: 10.025996574988731\n",
      "Epoch 54, Loss: 9.687220335006714\n",
      "Epoch 55, Loss: 9.26720692561223\n",
      "Epoch 56, Loss: 9.342681939785297\n",
      "Epoch 57, Loss: 10.155992838052603\n",
      "Epoch 58, Loss: 8.844623822432299\n",
      "Epoch 59, Loss: 10.19568947645334\n",
      "Epoch 60, Loss: 10.593327210499691\n",
      "Epoch 61, Loss: 12.492063118861271\n",
      "Epoch 62, Loss: 13.068034502176138\n",
      "Epoch 63, Loss: 17.336309873140774\n",
      "Epoch 64, Loss: 18.298135427328255\n",
      "Epoch 65, Loss: 15.79822984108558\n",
      "Epoch 66, Loss: 18.590829445765568\n",
      "Epoch 67, Loss: 17.649962902069092\n",
      "Epoch 68, Loss: 15.247732437573946\n",
      "Epoch 69, Loss: 14.69101018172044\n",
      "Epoch 70, Loss: 14.480540605691763\n",
      "Epoch 71, Loss: 14.28276746089642\n",
      "Epoch 72, Loss: 12.147579376514141\n",
      "Epoch 73, Loss: 11.081921265675472\n",
      "Epoch 74, Loss: 9.408843939120953\n",
      "Epoch 75, Loss: 10.18691842372601\n",
      "Epoch 76, Loss: 11.416483145493727\n",
      "Epoch 77, Loss: 12.015293139677782\n",
      "Epoch 78, Loss: 9.969351420035728\n",
      "Epoch 79, Loss: 9.118239622849684\n",
      "Epoch 80, Loss: 9.004917181455172\n",
      "Epoch 81, Loss: 8.55493739935068\n",
      "Epoch 82, Loss: 7.831942741687481\n",
      "Epoch 83, Loss: 8.044441259824312\n",
      "Epoch 84, Loss: 7.976836919784546\n",
      "Epoch 85, Loss: 7.6908916510068455\n",
      "Epoch 86, Loss: 8.459797162276049\n",
      "Epoch 87, Loss: 8.608374669001652\n",
      "Epoch 88, Loss: 8.959584254484911\n",
      "Epoch 89, Loss: 11.680329781312208\n",
      "Epoch 90, Loss: 17.6918913767888\n",
      "Epoch 91, Loss: 19.09216462648832\n",
      "Epoch 92, Loss: 21.54007728283222\n",
      "Epoch 93, Loss: 17.64500746360192\n",
      "Epoch 94, Loss: 28.747262001037598\n",
      "Epoch 95, Loss: 32.95816238109882\n",
      "Epoch 96, Loss: 43.11057971074031\n",
      "Epoch 97, Loss: 28.253676928006687\n",
      "Epoch 98, Loss: 14.432159057030312\n",
      "Epoch 99, Loss: 11.381655216217041\n",
      "Epoch 100, Loss: 9.85426167341379\n",
      "Epoch 101, Loss: 8.80630164880019\n",
      "Epoch 102, Loss: 8.755175517155575\n",
      "Epoch 103, Loss: 8.722566329515898\n",
      "Epoch 104, Loss: 7.823760931308453\n",
      "Epoch 105, Loss: 7.729162234526414\n",
      "Epoch 106, Loss: 7.1075268892141485\n",
      "Epoch 107, Loss: 7.549045122586763\n",
      "Epoch 108, Loss: 7.4893972140092115\n",
      "Epoch 109, Loss: 7.594256180983323\n",
      "Epoch 110, Loss: 8.212959289550781\n",
      "Epoch 111, Loss: 8.26056720660283\n",
      "Epoch 112, Loss: 9.85514365709745\n",
      "Epoch 113, Loss: 8.63060107597938\n",
      "Epoch 114, Loss: 7.513107171425452\n",
      "Epoch 115, Loss: 6.785815789149358\n",
      "Epoch 116, Loss: 6.888098680056059\n",
      "Epoch 117, Loss: 7.762539625167847\n",
      "Epoch 118, Loss: 7.709888715010423\n",
      "Epoch 119, Loss: 7.447486804081843\n",
      "Epoch 120, Loss: 7.508937102097732\n",
      "Epoch 121, Loss: 7.487199801665086\n",
      "Epoch 122, Loss: 7.189296594032874\n",
      "Epoch 123, Loss: 6.924337790562556\n",
      "Epoch 124, Loss: 7.355936802350557\n",
      "Epoch 125, Loss: 7.260907448255098\n",
      "Epoch 126, Loss: 7.599908847075242\n",
      "Epoch 127, Loss: 7.654206844476553\n",
      "Epoch 128, Loss: 7.51643707202031\n",
      "Epoch 129, Loss: 7.9467330895937405\n",
      "Epoch 130, Loss: 9.278097849625807\n",
      "Epoch 131, Loss: 10.817431449890137\n",
      "Epoch 132, Loss: 11.771331420311562\n",
      "Epoch 133, Loss: 10.951254991384653\n",
      "Epoch 134, Loss: 12.597668666106005\n",
      "Epoch 135, Loss: 18.24873795876136\n",
      "Epoch 136, Loss: 19.810148239135742\n",
      "Epoch 137, Loss: 13.277908361875093\n",
      "Epoch 138, Loss: 13.372558997227596\n",
      "Epoch 139, Loss: 20.772592361156757\n",
      "Epoch 140, Loss: 21.068004828232986\n",
      "Epoch 141, Loss: 12.751187654641958\n",
      "Epoch 142, Loss: 11.34919863480788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:04,317] Trial 48 finished with value: 38.86940302059951 and parameters: {'latent_dim_z1': 38, 'latent_dim_z2': 33, 'hidden_dim': 30, 'epochs': 146, 'causal_reg': 0.05861460524959361, 'learning_rate': 0.0019090507806954343}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143, Loss: 11.207829292003925\n",
      "Epoch 144, Loss: 8.879098342015194\n",
      "Epoch 145, Loss: 8.428526474879337\n",
      "Epoch 146, Loss: 7.861545892862173\n",
      "Epoch 1, Loss: 620.1407869779147\n",
      "Epoch 2, Loss: 235.94280037513147\n",
      "Epoch 3, Loss: 177.21273216834436\n",
      "Epoch 4, Loss: 146.1297148191012\n",
      "Epoch 5, Loss: 120.00580963721642\n",
      "Epoch 6, Loss: 110.2106182391827\n",
      "Epoch 7, Loss: 110.84192628126878\n",
      "Epoch 8, Loss: 92.6589854313777\n",
      "Epoch 9, Loss: 87.87806246830867\n",
      "Epoch 10, Loss: 77.83119186988243\n",
      "Epoch 11, Loss: 74.15992230635423\n",
      "Epoch 12, Loss: 70.29613700279823\n",
      "Epoch 13, Loss: 65.53958408649152\n",
      "Epoch 14, Loss: 69.90480621044452\n",
      "Epoch 15, Loss: 57.31713500389686\n",
      "Epoch 16, Loss: 55.33884950784537\n",
      "Epoch 17, Loss: 53.47785230783316\n",
      "Epoch 18, Loss: 60.13338866600623\n",
      "Epoch 19, Loss: 50.81562900543213\n",
      "Epoch 20, Loss: 42.70363074082594\n",
      "Epoch 21, Loss: 43.48929031078632\n",
      "Epoch 22, Loss: 39.17415688588069\n",
      "Epoch 23, Loss: 36.6880882703341\n",
      "Epoch 24, Loss: 34.97164429151095\n",
      "Epoch 25, Loss: 35.46107482910156\n",
      "Epoch 26, Loss: 37.574529941265396\n",
      "Epoch 27, Loss: 45.19007990910457\n",
      "Epoch 28, Loss: 39.008095301114594\n",
      "Epoch 29, Loss: 41.39532250624437\n",
      "Epoch 30, Loss: 35.487248750833366\n",
      "Epoch 31, Loss: 43.203764108511116\n",
      "Epoch 32, Loss: 31.530225166907677\n",
      "Epoch 33, Loss: 32.56940012711745\n",
      "Epoch 34, Loss: 25.992319143735447\n",
      "Epoch 35, Loss: 25.07016765154325\n",
      "Epoch 36, Loss: 26.592274995950554\n",
      "Epoch 37, Loss: 23.311789145836464\n",
      "Epoch 38, Loss: 26.22252236879789\n",
      "Epoch 39, Loss: 27.082850456237793\n",
      "Epoch 40, Loss: 22.237365832695595\n",
      "Epoch 41, Loss: 20.52802166571984\n",
      "Epoch 42, Loss: 22.64762515288133\n",
      "Epoch 43, Loss: 18.43021689928495\n",
      "Epoch 44, Loss: 17.534381462977482\n",
      "Epoch 45, Loss: 24.338371423574593\n",
      "Epoch 46, Loss: 26.19638079863328\n",
      "Epoch 47, Loss: 22.66649756064782\n",
      "Epoch 48, Loss: 21.609996685614952\n",
      "Epoch 49, Loss: 22.057052520605232\n",
      "Epoch 50, Loss: 20.946716455312874\n",
      "Epoch 51, Loss: 27.29375956608699\n",
      "Epoch 52, Loss: 24.320063921121452\n",
      "Epoch 53, Loss: 31.509357452392578\n",
      "Epoch 54, Loss: 21.493679156670204\n",
      "Epoch 55, Loss: 19.23132452597985\n",
      "Epoch 56, Loss: 28.080475477071907\n",
      "Epoch 57, Loss: 20.920464002169094\n",
      "Epoch 58, Loss: 21.235510459313026\n",
      "Epoch 59, Loss: 21.189851247347317\n",
      "Epoch 60, Loss: 16.377101824833797\n",
      "Epoch 61, Loss: 14.269568003140963\n",
      "Epoch 62, Loss: 19.49853440431448\n",
      "Epoch 63, Loss: 17.358801218179558\n",
      "Epoch 64, Loss: 14.78834135715778\n",
      "Epoch 65, Loss: 14.062676906585693\n",
      "Epoch 66, Loss: 13.13348440023569\n",
      "Epoch 67, Loss: 11.467247999631441\n",
      "Epoch 68, Loss: 12.422853231430054\n",
      "Epoch 69, Loss: 13.00091422521151\n",
      "Epoch 70, Loss: 12.832011406238262\n",
      "Epoch 71, Loss: 13.781450491685133\n",
      "Epoch 72, Loss: 12.298028469085693\n",
      "Epoch 73, Loss: 12.144242983597975\n",
      "Epoch 74, Loss: 13.154525371698233\n",
      "Epoch 75, Loss: 13.102396158071665\n",
      "Epoch 76, Loss: 11.899426606985239\n",
      "Epoch 77, Loss: 20.674468700702374\n",
      "Epoch 78, Loss: 17.408133176656868\n",
      "Epoch 79, Loss: 17.790482154259315\n",
      "Epoch 80, Loss: 13.254215790675236\n",
      "Epoch 81, Loss: 15.501334447127123\n",
      "Epoch 82, Loss: 11.982013262235201\n",
      "Epoch 83, Loss: 12.341458962513851\n",
      "Epoch 84, Loss: 10.58489284148583\n",
      "Epoch 85, Loss: 15.134922779523409\n",
      "Epoch 86, Loss: 18.698509839864876\n",
      "Epoch 87, Loss: 28.084337564615105\n",
      "Epoch 88, Loss: 18.81308889389038\n",
      "Epoch 89, Loss: 29.020252741300144\n",
      "Epoch 90, Loss: 41.47668545062725\n",
      "Epoch 91, Loss: 45.06460453913762\n",
      "Epoch 92, Loss: 25.74230386660649\n",
      "Epoch 93, Loss: 15.966605039743277\n",
      "Epoch 94, Loss: 11.496264677781324\n",
      "Epoch 95, Loss: 10.189436454039354\n",
      "Epoch 96, Loss: 9.30052979175861\n",
      "Epoch 97, Loss: 10.740150616719173\n",
      "Epoch 98, Loss: 11.481764976794903\n",
      "Epoch 99, Loss: 10.499587774276733\n",
      "Epoch 100, Loss: 10.434182020334097\n",
      "Epoch 101, Loss: 10.684567763255192\n",
      "Epoch 102, Loss: 9.82255799953754\n",
      "Epoch 103, Loss: 9.345498855297382\n",
      "Epoch 104, Loss: 9.633877497452955\n",
      "Epoch 105, Loss: 8.781064436985897\n",
      "Epoch 106, Loss: 10.0217149074261\n",
      "Epoch 107, Loss: 9.120613006445078\n",
      "Epoch 108, Loss: 9.64497788135822\n",
      "Epoch 109, Loss: 12.577011291797344\n",
      "Epoch 110, Loss: 13.791095696962797\n",
      "Epoch 111, Loss: 12.60827662394597\n",
      "Epoch 112, Loss: 11.369340291390053\n",
      "Epoch 113, Loss: 10.203216992891752\n",
      "Epoch 114, Loss: 9.434847428248478\n",
      "Epoch 115, Loss: 9.095607170691856\n",
      "Epoch 116, Loss: 9.863658024714542\n",
      "Epoch 117, Loss: 8.983961838942308\n",
      "Epoch 118, Loss: 11.285460692185621\n",
      "Epoch 119, Loss: 11.047595024108887\n",
      "Epoch 120, Loss: 9.07963070502648\n",
      "Epoch 121, Loss: 9.165131073731642\n",
      "Epoch 122, Loss: 10.586274843949537\n",
      "Epoch 123, Loss: 10.994544267654419\n",
      "Epoch 124, Loss: 16.650207189413216\n",
      "Epoch 125, Loss: 14.966015082139235\n",
      "Epoch 126, Loss: 14.5964172069843\n",
      "Epoch 127, Loss: 10.438626436086802\n",
      "Epoch 128, Loss: 10.432976282559908\n",
      "Epoch 129, Loss: 8.960239208661593\n",
      "Epoch 130, Loss: 8.518039960127611\n",
      "Epoch 131, Loss: 8.58631678727957\n",
      "Epoch 132, Loss: 9.311655411353478\n",
      "Epoch 133, Loss: 9.919694093557505\n",
      "Epoch 134, Loss: 13.309419870376587\n",
      "Epoch 135, Loss: 11.532668168728168\n",
      "Epoch 136, Loss: 13.396590617986826\n",
      "Epoch 137, Loss: 13.921694058638353\n",
      "Epoch 138, Loss: 18.741511748387264\n",
      "Epoch 139, Loss: 18.39996103140024\n",
      "Epoch 140, Loss: 22.250206598868736\n",
      "Epoch 141, Loss: 15.11164610202496\n",
      "Epoch 142, Loss: 21.375272494096023\n",
      "Epoch 143, Loss: 24.87219788477971\n",
      "Epoch 144, Loss: 22.151810462658222\n",
      "Epoch 145, Loss: 16.44758646304791\n",
      "Epoch 146, Loss: 11.374596265646128\n",
      "Epoch 147, Loss: 9.076771020889282\n",
      "Epoch 148, Loss: 9.095306928341206\n",
      "Epoch 149, Loss: 9.768714391268217\n",
      "Epoch 150, Loss: 10.273709718997662\n",
      "Epoch 151, Loss: 8.952524221860445\n",
      "Epoch 152, Loss: 9.794053334456224\n",
      "Epoch 153, Loss: 9.11052602988023\n",
      "Epoch 154, Loss: 8.049919733634361\n",
      "Epoch 155, Loss: 9.054774871239296\n",
      "Epoch 156, Loss: 8.261874437332153\n",
      "Epoch 157, Loss: 8.092903320605938\n",
      "Epoch 158, Loss: 8.037046322455772\n",
      "Epoch 159, Loss: 7.892825493445764\n",
      "Epoch 160, Loss: 9.392541243479801\n",
      "Epoch 161, Loss: 9.035334770496075\n",
      "Epoch 162, Loss: 9.703791526647715\n",
      "Epoch 163, Loss: 10.550944346647997\n",
      "Epoch 164, Loss: 11.650341565792377\n",
      "Epoch 165, Loss: 13.785103302735548\n",
      "Epoch 166, Loss: 13.15656539110037\n",
      "Epoch 167, Loss: 10.472444204183725\n",
      "Epoch 168, Loss: 11.411673820935762\n",
      "Epoch 169, Loss: 10.70698462999784\n",
      "Epoch 170, Loss: 9.512202904774593\n",
      "Epoch 171, Loss: 10.566729692312387\n",
      "Epoch 172, Loss: 8.399714653308575\n",
      "Epoch 173, Loss: 8.090683955412645\n",
      "Epoch 174, Loss: 7.940356529675997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:07,928] Trial 49 finished with value: 28.317654407838322 and parameters: {'latent_dim_z1': 21, 'latent_dim_z2': 51, 'hidden_dim': 64, 'epochs': 178, 'causal_reg': 0.3777662662118752, 'learning_rate': 0.006968556130370316}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175, Loss: 10.450055324114286\n",
      "Epoch 176, Loss: 10.074724252407368\n",
      "Epoch 177, Loss: 15.19815518305852\n",
      "Epoch 178, Loss: 12.28396278161269\n",
      "Epoch 1, Loss: 1065.0688054011418\n",
      "Epoch 2, Loss: 446.8218762324407\n",
      "Epoch 3, Loss: 359.1070665212778\n",
      "Epoch 4, Loss: 285.76469113276556\n",
      "Epoch 5, Loss: 252.2324993426983\n",
      "Epoch 6, Loss: 219.67657412015475\n",
      "Epoch 7, Loss: 212.6819449204665\n",
      "Epoch 8, Loss: 178.34754650409405\n",
      "Epoch 9, Loss: 175.01676647479718\n",
      "Epoch 10, Loss: 162.4471514775203\n",
      "Epoch 11, Loss: 171.06027559133676\n",
      "Epoch 12, Loss: 127.16678047180176\n",
      "Epoch 13, Loss: 120.50295844444862\n",
      "Epoch 14, Loss: 110.62610919658954\n",
      "Epoch 15, Loss: 139.22333673330454\n",
      "Epoch 16, Loss: 97.63763178311862\n",
      "Epoch 17, Loss: 87.01678011967586\n",
      "Epoch 18, Loss: 83.99463125375601\n",
      "Epoch 19, Loss: 66.58649811377892\n",
      "Epoch 20, Loss: 61.974126448998085\n",
      "Epoch 21, Loss: 61.40578629420354\n",
      "Epoch 22, Loss: 69.45380364931546\n",
      "Epoch 23, Loss: 71.16580566993126\n",
      "Epoch 24, Loss: 69.17758809603177\n",
      "Epoch 25, Loss: 52.40177169212928\n",
      "Epoch 26, Loss: 48.04168803875263\n",
      "Epoch 27, Loss: 37.918459818913384\n",
      "Epoch 28, Loss: 36.776340631338265\n",
      "Epoch 29, Loss: 48.82733631134033\n",
      "Epoch 30, Loss: 49.92256755095262\n",
      "Epoch 31, Loss: 46.11955950810359\n",
      "Epoch 32, Loss: 41.583876316364\n",
      "Epoch 33, Loss: 47.69927809788631\n",
      "Epoch 34, Loss: 34.545560983511116\n",
      "Epoch 35, Loss: 32.741972996638374\n",
      "Epoch 36, Loss: 43.30155812777006\n",
      "Epoch 37, Loss: 47.844646380497856\n",
      "Epoch 38, Loss: 45.473993668189415\n",
      "Epoch 39, Loss: 44.12160088465764\n",
      "Epoch 40, Loss: 29.592179188361534\n",
      "Epoch 41, Loss: 26.979702656085674\n",
      "Epoch 42, Loss: 23.666431940518894\n",
      "Epoch 43, Loss: 25.519496954404392\n",
      "Epoch 44, Loss: 24.974022278418907\n",
      "Epoch 45, Loss: 20.18330214573787\n",
      "Epoch 46, Loss: 18.699003256284275\n",
      "Epoch 47, Loss: 19.236741689535286\n",
      "Epoch 48, Loss: 30.69927167892456\n",
      "Epoch 49, Loss: 27.40045870267428\n",
      "Epoch 50, Loss: 22.407235145568848\n",
      "Epoch 51, Loss: 18.218467785761906\n",
      "Epoch 52, Loss: 19.296803437746487\n",
      "Epoch 53, Loss: 17.853635787963867\n",
      "Epoch 54, Loss: 21.850463592089138\n",
      "Epoch 55, Loss: 28.103763763721172\n",
      "Epoch 56, Loss: 22.452737184671257\n",
      "Epoch 57, Loss: 17.9071595118596\n",
      "Epoch 58, Loss: 13.965575988476093\n",
      "Epoch 59, Loss: 13.21840346776522\n",
      "Epoch 60, Loss: 13.458810384456928\n",
      "Epoch 61, Loss: 11.55093449812669\n",
      "Epoch 62, Loss: 13.107798356276293\n",
      "Epoch 63, Loss: 19.775279705341045\n",
      "Epoch 64, Loss: 57.69776072868934\n",
      "Epoch 65, Loss: 59.67452511420617\n",
      "Epoch 66, Loss: 36.19050671504094\n",
      "Epoch 67, Loss: 34.834347651554985\n",
      "Epoch 68, Loss: 21.70778171832745\n",
      "Epoch 69, Loss: 14.80924155161931\n",
      "Epoch 70, Loss: 13.365187791677622\n",
      "Epoch 71, Loss: 11.04426094201895\n",
      "Epoch 72, Loss: 11.691353614513691\n",
      "Epoch 73, Loss: 11.131285392321074\n",
      "Epoch 74, Loss: 15.491984404050386\n",
      "Epoch 75, Loss: 17.05610286272489\n",
      "Epoch 76, Loss: 17.79159820996798\n",
      "Epoch 77, Loss: 30.013472703786995\n",
      "Epoch 78, Loss: 32.091159967275765\n",
      "Epoch 79, Loss: 23.988872418036827\n",
      "Epoch 80, Loss: 18.632490671597996\n",
      "Epoch 81, Loss: 17.25019282561082\n",
      "Epoch 82, Loss: 15.599935935093807\n",
      "Epoch 83, Loss: 12.671190628638634\n",
      "Epoch 84, Loss: 11.933094061337984\n",
      "Epoch 85, Loss: 11.189140154765202\n",
      "Epoch 86, Loss: 9.168589702019325\n",
      "Epoch 87, Loss: 10.486746934744028\n",
      "Epoch 88, Loss: 11.83781442275414\n",
      "Epoch 89, Loss: 10.263818374046913\n",
      "Epoch 90, Loss: 14.346886836565458\n",
      "Epoch 91, Loss: 17.953792828779953\n",
      "Epoch 92, Loss: 15.531511490161602\n",
      "Epoch 93, Loss: 16.57121643653283\n",
      "Epoch 94, Loss: 12.057916494516226\n",
      "Epoch 95, Loss: 13.288096556296715\n",
      "Epoch 96, Loss: 11.42078368480389\n",
      "Epoch 97, Loss: 13.223212865682749\n",
      "Epoch 98, Loss: 16.131984288875874\n",
      "Epoch 99, Loss: 11.594720180218037\n",
      "Epoch 100, Loss: 10.063270623867329\n",
      "Epoch 101, Loss: 8.453829196783213\n",
      "Epoch 102, Loss: 8.23632341164809\n",
      "Epoch 103, Loss: 9.900618699880747\n",
      "Epoch 104, Loss: 11.929244940097515\n",
      "Epoch 105, Loss: 14.03490374638484\n",
      "Epoch 106, Loss: 11.85318374633789\n",
      "Epoch 107, Loss: 12.292753824820885\n",
      "Epoch 108, Loss: 10.512621237681461\n",
      "Epoch 109, Loss: 8.797449295337383\n",
      "Epoch 110, Loss: 8.57842570084792\n",
      "Epoch 111, Loss: 8.036469056056095\n",
      "Epoch 112, Loss: 8.216273729617779\n",
      "Epoch 113, Loss: 11.40212475336515\n",
      "Epoch 114, Loss: 17.421536078819862\n",
      "Epoch 115, Loss: 21.16396526189951\n",
      "Epoch 116, Loss: 29.150291222792404\n",
      "Epoch 117, Loss: 34.59352669349084\n",
      "Epoch 118, Loss: 24.164495688218338\n",
      "Epoch 119, Loss: 17.6351781808413\n",
      "Epoch 120, Loss: 15.203632868253267\n",
      "Epoch 121, Loss: 15.18095539166377\n",
      "Epoch 122, Loss: 12.156824900553776\n",
      "Epoch 123, Loss: 12.388260676310612\n",
      "Epoch 124, Loss: 10.95537519454956\n",
      "Epoch 125, Loss: 11.213730078477125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:10,854] Trial 50 finished with value: 54.13655190184138 and parameters: {'latent_dim_z1': 51, 'latent_dim_z2': 38, 'hidden_dim': 114, 'epochs': 128, 'causal_reg': 0.24103078755312013, 'learning_rate': 0.003318110463038292}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126, Loss: 10.71759058878972\n",
      "Epoch 127, Loss: 10.450653021152203\n",
      "Epoch 128, Loss: 13.447634366842417\n",
      "Epoch 1, Loss: 4.630262351956493e+18\n",
      "Epoch 2, Loss: 436.61764467679535\n",
      "Epoch 3, Loss: 281.543389247014\n",
      "Epoch 4, Loss: 229.5619624211238\n",
      "Epoch 5, Loss: 217.72983815119818\n",
      "Epoch 6, Loss: 215.23460975060095\n",
      "Epoch 7, Loss: 214.76810337946966\n",
      "Epoch 8, Loss: 214.5159425001878\n",
      "Epoch 9, Loss: 214.45289024939905\n",
      "Epoch 10, Loss: 214.33379569420447\n",
      "Epoch 11, Loss: 214.0530014038086\n",
      "Epoch 12, Loss: 214.1983413696289\n",
      "Epoch 13, Loss: 214.00012823251578\n",
      "Epoch 14, Loss: 214.0684218773475\n",
      "Epoch 15, Loss: 214.2775614811824\n",
      "Epoch 16, Loss: 213.98491668701172\n",
      "Epoch 17, Loss: 214.14973009549655\n",
      "Epoch 18, Loss: 213.72073569664587\n",
      "Epoch 19, Loss: 213.78225561288687\n",
      "Epoch 20, Loss: 213.8057465186486\n",
      "Epoch 21, Loss: 213.79419825627252\n",
      "Epoch 22, Loss: 213.66359505286584\n",
      "Epoch 23, Loss: 213.58961868286133\n",
      "Epoch 24, Loss: 213.61531888521634\n",
      "Epoch 25, Loss: 214.0377558194674\n",
      "Epoch 26, Loss: 214.22804847130408\n",
      "Epoch 27, Loss: 214.05524239173303\n",
      "Epoch 28, Loss: 213.6807729280912\n",
      "Epoch 29, Loss: 213.76214981079102\n",
      "Epoch 30, Loss: 213.77954512376053\n",
      "Epoch 31, Loss: 213.57902644230768\n",
      "Epoch 32, Loss: 214.63298474825345\n",
      "Epoch 33, Loss: 213.60241171029898\n",
      "Epoch 34, Loss: 213.4850989121657\n",
      "Epoch 35, Loss: 213.31610899705154\n",
      "Epoch 36, Loss: 213.59236790583685\n",
      "Epoch 37, Loss: 213.6500473022461\n",
      "Epoch 38, Loss: 213.12023426936224\n",
      "Epoch 39, Loss: 213.16172497089093\n",
      "Epoch 40, Loss: 213.1441477262057\n",
      "Epoch 41, Loss: 213.25898038423978\n",
      "Epoch 42, Loss: 213.19410412128155\n",
      "Epoch 43, Loss: 213.3379795367901\n",
      "Epoch 44, Loss: 213.1724985562838\n",
      "Epoch 45, Loss: 213.23929361196664\n",
      "Epoch 46, Loss: 213.2311553955078\n",
      "Epoch 47, Loss: 213.2337423471304\n",
      "Epoch 48, Loss: 213.46805455134466\n",
      "Epoch 49, Loss: 213.38029685387244\n",
      "Epoch 50, Loss: 214.6083588233361\n",
      "Epoch 51, Loss: 213.73662185668945\n",
      "Epoch 52, Loss: 213.6773303105281\n",
      "Epoch 53, Loss: 213.41191218449518\n",
      "Epoch 54, Loss: 213.47502899169922\n",
      "Epoch 55, Loss: 213.1147587115948\n",
      "Epoch 56, Loss: 213.14877231304462\n",
      "Epoch 57, Loss: 213.5208998460036\n",
      "Epoch 58, Loss: 213.97055904681866\n",
      "Epoch 59, Loss: 214.2533692580003\n",
      "Epoch 60, Loss: 213.69687828650842\n",
      "Epoch 61, Loss: 214.0238433251014\n",
      "Epoch 62, Loss: 213.80210260244516\n",
      "Epoch 63, Loss: 213.69198608398438\n",
      "Epoch 64, Loss: 214.20174099848822\n",
      "Epoch 65, Loss: 214.1397332411546\n",
      "Epoch 66, Loss: 213.86365861159103\n",
      "Epoch 67, Loss: 213.0684333214393\n",
      "Epoch 68, Loss: 213.77954130906326\n",
      "Epoch 69, Loss: 213.39402947059045\n",
      "Epoch 70, Loss: 213.44728939349835\n",
      "Epoch 71, Loss: 213.40738971416766\n",
      "Epoch 72, Loss: 213.66225565396823\n",
      "Epoch 73, Loss: 214.4535129253681\n",
      "Epoch 74, Loss: 213.42178696852463\n",
      "Epoch 75, Loss: 212.88288615300104\n",
      "Epoch 76, Loss: 214.7083379305326\n",
      "Epoch 77, Loss: 213.1901523883526\n",
      "Epoch 78, Loss: 213.211181640625\n",
      "Epoch 79, Loss: 213.35007388775165\n",
      "Epoch 80, Loss: 213.39072608947754\n",
      "Epoch 81, Loss: 213.6655153127817\n",
      "Epoch 82, Loss: 214.20850783128006\n",
      "Epoch 83, Loss: 214.1290030846229\n",
      "Epoch 84, Loss: 213.01826080909143\n",
      "Epoch 85, Loss: 213.66774104191705\n",
      "Epoch 86, Loss: 213.15493334256686\n",
      "Epoch 87, Loss: 213.43866993830756\n",
      "Epoch 88, Loss: 213.67203932542068\n",
      "Epoch 89, Loss: 213.68704634446365\n",
      "Epoch 90, Loss: 213.37113747229944\n",
      "Epoch 91, Loss: 214.02883617694562\n",
      "Epoch 92, Loss: 213.6302771935096\n",
      "Epoch 93, Loss: 213.34665239774264\n",
      "Epoch 94, Loss: 212.87724920419546\n",
      "Epoch 95, Loss: 214.1099636371319\n",
      "Epoch 96, Loss: 214.00078010559082\n",
      "Epoch 97, Loss: 213.60212326049805\n",
      "Epoch 98, Loss: 213.20809643085187\n",
      "Epoch 99, Loss: 213.60981545081506\n",
      "Epoch 100, Loss: 213.22460233248196\n",
      "Epoch 101, Loss: 213.7119645338792\n",
      "Epoch 102, Loss: 213.45126430804913\n",
      "Epoch 103, Loss: 213.39409021230844\n",
      "Epoch 104, Loss: 213.224362886869\n",
      "Epoch 105, Loss: 213.31492849496695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:13,042] Trial 51 finished with value: 4.209329410869539e+16 and parameters: {'latent_dim_z1': 13, 'latent_dim_z2': 23, 'hidden_dim': 89, 'epochs': 110, 'causal_reg': 0.054436790443036634, 'learning_rate': 0.09147397250138546}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106, Loss: 216.96650196955756\n",
      "Epoch 107, Loss: 213.52704649705154\n",
      "Epoch 108, Loss: 214.29672622680664\n",
      "Epoch 109, Loss: 213.326115828294\n",
      "Epoch 110, Loss: 213.71236419677734\n",
      "Epoch 1, Loss: 407.58699064988355\n",
      "Epoch 2, Loss: 203.1050315270057\n",
      "Epoch 3, Loss: 146.3599681854248\n",
      "Epoch 4, Loss: 121.49170714158278\n",
      "Epoch 5, Loss: 108.93490116412823\n",
      "Epoch 6, Loss: 96.46883465693547\n",
      "Epoch 7, Loss: 91.6770987877479\n",
      "Epoch 8, Loss: 82.10131791921762\n",
      "Epoch 9, Loss: 74.36796584496132\n",
      "Epoch 10, Loss: 68.19580224844125\n",
      "Epoch 11, Loss: 63.11882664607121\n",
      "Epoch 12, Loss: 57.6283002266517\n",
      "Epoch 13, Loss: 52.392099160414475\n",
      "Epoch 14, Loss: 48.52613243689904\n",
      "Epoch 15, Loss: 47.29990100860596\n",
      "Epoch 16, Loss: 44.183044506953316\n",
      "Epoch 17, Loss: 40.94911435934213\n",
      "Epoch 18, Loss: 36.96546330818763\n",
      "Epoch 19, Loss: 34.2535818540133\n",
      "Epoch 20, Loss: 31.564429393181435\n",
      "Epoch 21, Loss: 30.28470230102539\n",
      "Epoch 22, Loss: 30.17990097632775\n",
      "Epoch 23, Loss: 24.70018746302678\n",
      "Epoch 24, Loss: 23.51047446177556\n",
      "Epoch 25, Loss: 21.731115047748272\n",
      "Epoch 26, Loss: 20.858491347386288\n",
      "Epoch 27, Loss: 18.97761216530433\n",
      "Epoch 28, Loss: 18.724054629986103\n",
      "Epoch 29, Loss: 16.977416845468376\n",
      "Epoch 30, Loss: 15.693492229168232\n",
      "Epoch 31, Loss: 15.750618787912222\n",
      "Epoch 32, Loss: 14.882445592146654\n",
      "Epoch 33, Loss: 13.842693952413706\n",
      "Epoch 34, Loss: 12.596423204128559\n",
      "Epoch 35, Loss: 11.673357963562012\n",
      "Epoch 36, Loss: 11.816406415059017\n",
      "Epoch 37, Loss: 11.702407305057232\n",
      "Epoch 38, Loss: 10.855711826911339\n",
      "Epoch 39, Loss: 10.413354103381817\n",
      "Epoch 40, Loss: 10.990213467524601\n",
      "Epoch 41, Loss: 10.336457931078398\n",
      "Epoch 42, Loss: 9.869302914692806\n",
      "Epoch 43, Loss: 9.221890981380756\n",
      "Epoch 44, Loss: 9.029585985036997\n",
      "Epoch 45, Loss: 9.111091907207783\n",
      "Epoch 46, Loss: 8.61636866056002\n",
      "Epoch 47, Loss: 9.694978145452646\n",
      "Epoch 48, Loss: 9.162184953689575\n",
      "Epoch 49, Loss: 8.188559257067167\n",
      "Epoch 50, Loss: 7.919384039365328\n",
      "Epoch 51, Loss: 7.911290370501005\n",
      "Epoch 52, Loss: 7.836428899031419\n",
      "Epoch 53, Loss: 7.742054719191331\n",
      "Epoch 54, Loss: 8.228644187633808\n",
      "Epoch 55, Loss: 7.866086959838867\n",
      "Epoch 56, Loss: 8.034106327937199\n",
      "Epoch 57, Loss: 7.605261600934542\n",
      "Epoch 58, Loss: 7.395348145411565\n",
      "Epoch 59, Loss: 7.368764565541194\n",
      "Epoch 60, Loss: 7.887853402357835\n",
      "Epoch 61, Loss: 7.883111568597647\n",
      "Epoch 62, Loss: 8.335073782847477\n",
      "Epoch 63, Loss: 7.684547130878155\n",
      "Epoch 64, Loss: 7.54384734080388\n",
      "Epoch 65, Loss: 8.383574210680449\n",
      "Epoch 66, Loss: 8.367080119939951\n",
      "Epoch 67, Loss: 8.16734998042767\n",
      "Epoch 68, Loss: 8.996309023637037\n",
      "Epoch 69, Loss: 8.370128044715294\n",
      "Epoch 70, Loss: 7.979987034430871\n",
      "Epoch 71, Loss: 7.324045107914851\n",
      "Epoch 72, Loss: 7.46065326837393\n",
      "Epoch 73, Loss: 7.6429619422325725\n",
      "Epoch 74, Loss: 7.629827719468337\n",
      "Epoch 75, Loss: 6.969981321921716\n",
      "Epoch 76, Loss: 6.920952961995051\n",
      "Epoch 77, Loss: 7.092841478494497\n",
      "Epoch 78, Loss: 7.995469991977398\n",
      "Epoch 79, Loss: 8.050947556128868\n",
      "Epoch 80, Loss: 7.514641413321862\n",
      "Epoch 81, Loss: 6.994137397179236\n",
      "Epoch 82, Loss: 6.858857521644006\n",
      "Epoch 83, Loss: 7.2061427006354695\n",
      "Epoch 84, Loss: 6.938230991363525\n",
      "Epoch 85, Loss: 6.673298689035269\n",
      "Epoch 86, Loss: 6.572080227044912\n",
      "Epoch 87, Loss: 7.056041424091045\n",
      "Epoch 88, Loss: 6.948255685659555\n",
      "Epoch 89, Loss: 6.856099403821505\n",
      "Epoch 90, Loss: 7.0901258908785305\n",
      "Epoch 91, Loss: 7.101547809747549\n",
      "Epoch 92, Loss: 7.65758202626155\n",
      "Epoch 93, Loss: 9.579589660351093\n",
      "Epoch 94, Loss: 10.594658136367798\n",
      "Epoch 95, Loss: 9.457590635006245\n",
      "Epoch 96, Loss: 9.338897209901075\n",
      "Epoch 97, Loss: 9.089906729184664\n",
      "Epoch 98, Loss: 8.644239865816557\n",
      "Epoch 99, Loss: 8.663544031289907\n",
      "Epoch 100, Loss: 8.617861087505634\n",
      "Epoch 101, Loss: 8.361747393241295\n",
      "Epoch 102, Loss: 7.864984512329102\n",
      "Epoch 103, Loss: 10.000495855624859\n",
      "Epoch 104, Loss: 10.7755876321059\n",
      "Epoch 105, Loss: 9.505634509600126\n",
      "Epoch 106, Loss: 7.8723802749927225\n",
      "Epoch 107, Loss: 7.4642630723806525\n",
      "Epoch 108, Loss: 7.061884256509634\n",
      "Epoch 109, Loss: 6.818026010806744\n",
      "Epoch 110, Loss: 6.650340538758498\n",
      "Epoch 111, Loss: 6.469331447894756\n",
      "Epoch 112, Loss: 6.345415463814368\n",
      "Epoch 113, Loss: 6.298647972253653\n",
      "Epoch 114, Loss: 6.2218744571392355\n",
      "Epoch 115, Loss: 6.106263472483708\n",
      "Epoch 116, Loss: 6.026315689086914\n",
      "Epoch 117, Loss: 5.968630203833947\n",
      "Epoch 118, Loss: 5.972792863845825\n",
      "Epoch 119, Loss: 5.980170213259184\n",
      "Epoch 120, Loss: 6.007549616006704\n",
      "Epoch 121, Loss: 6.0059997485234184\n",
      "Epoch 122, Loss: 5.985834946999183\n",
      "Epoch 123, Loss: 6.019978908392099\n",
      "Epoch 124, Loss: 6.03836046732389\n",
      "Epoch 125, Loss: 6.336923525883601\n",
      "Epoch 126, Loss: 6.2644988756913405\n",
      "Epoch 127, Loss: 6.5011303424835205\n",
      "Epoch 128, Loss: 6.544631756269014\n",
      "Epoch 129, Loss: 6.8473075903379\n",
      "Epoch 130, Loss: 7.109181990990272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:15,870] Trial 52 finished with value: 21.362770480481576 and parameters: {'latent_dim_z1': 13, 'latent_dim_z2': 30, 'hidden_dim': 78, 'epochs': 139, 'causal_reg': 0.13383082529264442, 'learning_rate': 0.0009761486080243242}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131, Loss: 7.14608278641334\n",
      "Epoch 132, Loss: 6.966416285588191\n",
      "Epoch 133, Loss: 7.426812318655161\n",
      "Epoch 134, Loss: 8.075160888525156\n",
      "Epoch 135, Loss: 11.862108303950382\n",
      "Epoch 136, Loss: 10.4957028902494\n",
      "Epoch 137, Loss: 8.707379249426035\n",
      "Epoch 138, Loss: 7.449480753678542\n",
      "Epoch 139, Loss: 7.260516515144935\n",
      "Epoch 1, Loss: 555.9657070453351\n",
      "Epoch 2, Loss: 238.31656118539664\n",
      "Epoch 3, Loss: 168.95120180570163\n",
      "Epoch 4, Loss: 141.61821247981146\n",
      "Epoch 5, Loss: 125.55170323298528\n",
      "Epoch 6, Loss: 106.20494446387657\n",
      "Epoch 7, Loss: 100.85508155822754\n",
      "Epoch 8, Loss: 86.501129590548\n",
      "Epoch 9, Loss: 71.23060549222507\n",
      "Epoch 10, Loss: 62.653938880333534\n",
      "Epoch 11, Loss: 56.479269174429085\n",
      "Epoch 12, Loss: 55.306328333341156\n",
      "Epoch 13, Loss: 45.532842269310585\n",
      "Epoch 14, Loss: 40.86250796684852\n",
      "Epoch 15, Loss: 35.973020993746246\n",
      "Epoch 16, Loss: 31.435324302086464\n",
      "Epoch 17, Loss: 31.14286121955285\n",
      "Epoch 18, Loss: 26.936711751497707\n",
      "Epoch 19, Loss: 26.26641977750338\n",
      "Epoch 20, Loss: 23.539436487051155\n",
      "Epoch 21, Loss: 22.225147577432487\n",
      "Epoch 22, Loss: 23.415102628561165\n",
      "Epoch 23, Loss: 25.99797876064594\n",
      "Epoch 24, Loss: 17.953532695770264\n",
      "Epoch 25, Loss: 17.31095530436589\n",
      "Epoch 26, Loss: 16.260168442359337\n",
      "Epoch 27, Loss: 14.310006581819975\n",
      "Epoch 28, Loss: 12.504651491458599\n",
      "Epoch 29, Loss: 11.64226823586684\n",
      "Epoch 30, Loss: 11.896719712477465\n",
      "Epoch 31, Loss: 12.169276861044077\n",
      "Epoch 32, Loss: 10.811074055158175\n",
      "Epoch 33, Loss: 10.223826261667105\n",
      "Epoch 34, Loss: 9.463422610209538\n",
      "Epoch 35, Loss: 8.853628892164965\n",
      "Epoch 36, Loss: 8.727084398269653\n",
      "Epoch 37, Loss: 9.159216037163368\n",
      "Epoch 38, Loss: 8.782445944272554\n",
      "Epoch 39, Loss: 8.698531554295467\n",
      "Epoch 40, Loss: 8.887416656200703\n",
      "Epoch 41, Loss: 8.14255477831914\n",
      "Epoch 42, Loss: 8.087311194493221\n",
      "Epoch 43, Loss: 9.199722033280592\n",
      "Epoch 44, Loss: 10.98486383144672\n",
      "Epoch 45, Loss: 10.923391763980572\n",
      "Epoch 46, Loss: 10.208390987836397\n",
      "Epoch 47, Loss: 11.270297087155855\n",
      "Epoch 48, Loss: 14.114273878244253\n",
      "Epoch 49, Loss: 13.30195665359497\n",
      "Epoch 50, Loss: 12.463722375723032\n",
      "Epoch 51, Loss: 10.279198499826284\n",
      "Epoch 52, Loss: 9.240871851260845\n",
      "Epoch 53, Loss: 8.653272206966694\n",
      "Epoch 54, Loss: 7.81355696458083\n",
      "Epoch 55, Loss: 7.610724705916184\n",
      "Epoch 56, Loss: 7.244144898194533\n",
      "Epoch 57, Loss: 6.899925323633047\n",
      "Epoch 58, Loss: 6.900212141183706\n",
      "Epoch 59, Loss: 6.705927628737229\n",
      "Epoch 60, Loss: 6.7400845014131985\n",
      "Epoch 61, Loss: 6.468816372064444\n",
      "Epoch 62, Loss: 6.57137131690979\n",
      "Epoch 63, Loss: 6.476043279354389\n",
      "Epoch 64, Loss: 6.523502276493953\n",
      "Epoch 65, Loss: 6.583020191926223\n",
      "Epoch 66, Loss: 6.3870467039255\n",
      "Epoch 67, Loss: 6.2140771975884075\n",
      "Epoch 68, Loss: 6.220491024164053\n",
      "Epoch 69, Loss: 6.176321378121009\n",
      "Epoch 70, Loss: 6.340415661151592\n",
      "Epoch 71, Loss: 6.328618324719942\n",
      "Epoch 72, Loss: 6.361503491034875\n",
      "Epoch 73, Loss: 6.54822145975553\n",
      "Epoch 74, Loss: 6.592300176620483\n",
      "Epoch 75, Loss: 6.874246854048509\n",
      "Epoch 76, Loss: 8.406399286710299\n",
      "Epoch 77, Loss: 10.874473700156578\n",
      "Epoch 78, Loss: 11.87184581389794\n",
      "Epoch 79, Loss: 12.255664953818687\n",
      "Epoch 80, Loss: 11.041198638769297\n",
      "Epoch 81, Loss: 11.553619659863985\n",
      "Epoch 82, Loss: 10.26254965708806\n",
      "Epoch 83, Loss: 10.897017699021559\n",
      "Epoch 84, Loss: 10.884441742530235\n",
      "Epoch 85, Loss: 10.34253925543565\n",
      "Epoch 86, Loss: 9.165775335752047\n",
      "Epoch 87, Loss: 7.999587719257061\n",
      "Epoch 88, Loss: 7.51331430215102\n",
      "Epoch 89, Loss: 7.17497966839717\n",
      "Epoch 90, Loss: 6.903506242311918\n",
      "Epoch 91, Loss: 6.737192520728478\n",
      "Epoch 92, Loss: 6.503109876926128\n",
      "Epoch 93, Loss: 6.287058426783635\n",
      "Epoch 94, Loss: 6.30513440645658\n",
      "Epoch 95, Loss: 6.471214807950533\n",
      "Epoch 96, Loss: 6.601246411983784\n",
      "Epoch 97, Loss: 6.652346849441528\n",
      "Epoch 98, Loss: 7.12377641751216\n",
      "Epoch 99, Loss: 7.177514516390287\n",
      "Epoch 100, Loss: 6.898725307904757\n",
      "Epoch 101, Loss: 6.865997406152578\n",
      "Epoch 102, Loss: 7.0288218168111944\n",
      "Epoch 103, Loss: 7.757088752893301\n",
      "Epoch 104, Loss: 7.770497707220224\n",
      "Epoch 105, Loss: 7.09664641893827\n",
      "Epoch 106, Loss: 6.923597904352041\n",
      "Epoch 107, Loss: 7.3668922827794\n",
      "Epoch 108, Loss: 8.171180486679077\n",
      "Epoch 109, Loss: 7.878548805530254\n",
      "Epoch 110, Loss: 8.570369500380297\n",
      "Epoch 111, Loss: 10.749819572155292\n",
      "Epoch 112, Loss: 13.786710335658146\n",
      "Epoch 113, Loss: 16.168545722961426\n",
      "Epoch 114, Loss: 14.61307982298044\n",
      "Epoch 115, Loss: 14.172763897822453\n",
      "Epoch 116, Loss: 11.667601585388184\n",
      "Epoch 117, Loss: 9.00280576485854\n",
      "Epoch 118, Loss: 7.7740113368401165\n",
      "Epoch 119, Loss: 6.936498183470506\n",
      "Epoch 120, Loss: 6.658935620234563\n",
      "Epoch 121, Loss: 6.614311016522921\n",
      "Epoch 122, Loss: 6.631307895366962\n",
      "Epoch 123, Loss: 6.427538468287541\n",
      "Epoch 124, Loss: 6.2531451261960544\n",
      "Epoch 125, Loss: 6.299253041927631\n",
      "Epoch 126, Loss: 6.272115615698008\n",
      "Epoch 127, Loss: 6.331881596491887\n",
      "Epoch 128, Loss: 6.44141855606666\n",
      "Epoch 129, Loss: 6.278852169330303\n",
      "Epoch 130, Loss: 6.224138039809007\n",
      "Epoch 131, Loss: 6.0814149746528035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:18,870] Trial 53 finished with value: 22.203483387402134 and parameters: {'latent_dim_z1': 17, 'latent_dim_z2': 65, 'hidden_dim': 54, 'epochs': 140, 'causal_reg': 0.13184034011148862, 'learning_rate': 0.0011267267727153943}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132, Loss: 6.036091914543738\n",
      "Epoch 133, Loss: 5.935568461051354\n",
      "Epoch 134, Loss: 5.9789260900937595\n",
      "Epoch 135, Loss: 5.944486838120681\n",
      "Epoch 136, Loss: 5.893737609569843\n",
      "Epoch 137, Loss: 5.906054460085356\n",
      "Epoch 138, Loss: 5.94265734232389\n",
      "Epoch 139, Loss: 5.961738696465125\n",
      "Epoch 140, Loss: 6.101728457670945\n",
      "Epoch 1, Loss: 802.9741287231445\n",
      "Epoch 2, Loss: 435.3593418414776\n",
      "Epoch 3, Loss: 287.45214520967926\n",
      "Epoch 4, Loss: 232.665100977971\n",
      "Epoch 5, Loss: 202.94978919396033\n",
      "Epoch 6, Loss: 180.51370694087103\n",
      "Epoch 7, Loss: 165.639231975262\n",
      "Epoch 8, Loss: 153.8117807828463\n",
      "Epoch 9, Loss: 140.92261732541598\n",
      "Epoch 10, Loss: 132.14765519362228\n",
      "Epoch 11, Loss: 123.57659215193529\n",
      "Epoch 12, Loss: 115.83815288543701\n",
      "Epoch 13, Loss: 108.5419004880465\n",
      "Epoch 14, Loss: 101.18891657315768\n",
      "Epoch 15, Loss: 94.44148195706882\n",
      "Epoch 16, Loss: 88.19423822256235\n",
      "Epoch 17, Loss: 80.62710967430702\n",
      "Epoch 18, Loss: 76.02184897202712\n",
      "Epoch 19, Loss: 71.05018513019269\n",
      "Epoch 20, Loss: 67.97677641648512\n",
      "Epoch 21, Loss: 62.77782337482159\n",
      "Epoch 22, Loss: 56.78852998293363\n",
      "Epoch 23, Loss: 53.53466342045711\n",
      "Epoch 24, Loss: 50.13737575824444\n",
      "Epoch 25, Loss: 45.35548283503606\n",
      "Epoch 26, Loss: 43.9181432723999\n",
      "Epoch 27, Loss: 40.46765554868258\n",
      "Epoch 28, Loss: 39.275259311382584\n",
      "Epoch 29, Loss: 35.02051192063551\n",
      "Epoch 30, Loss: 32.40645305926983\n",
      "Epoch 31, Loss: 30.890015308673565\n",
      "Epoch 32, Loss: 28.76189136505127\n",
      "Epoch 33, Loss: 27.386517928196835\n",
      "Epoch 34, Loss: 26.530971233661358\n",
      "Epoch 35, Loss: 25.508282257960392\n",
      "Epoch 36, Loss: 23.40749901991624\n",
      "Epoch 37, Loss: 21.42247225688054\n",
      "Epoch 38, Loss: 21.48973450293908\n",
      "Epoch 39, Loss: 21.040211714231052\n",
      "Epoch 40, Loss: 18.42274647492629\n",
      "Epoch 41, Loss: 17.771548381218544\n",
      "Epoch 42, Loss: 17.776405554551346\n",
      "Epoch 43, Loss: 16.296332689432\n",
      "Epoch 44, Loss: 15.028602820176344\n",
      "Epoch 45, Loss: 14.56958084840041\n",
      "Epoch 46, Loss: 13.71200887973492\n",
      "Epoch 47, Loss: 13.225709511683537\n",
      "Epoch 48, Loss: 12.612907116229717\n",
      "Epoch 49, Loss: 12.12486875974215\n",
      "Epoch 50, Loss: 11.62398569400494\n",
      "Epoch 51, Loss: 11.213710986650907\n",
      "Epoch 52, Loss: 10.798108632747944\n",
      "Epoch 53, Loss: 10.572758968059834\n",
      "Epoch 54, Loss: 10.24213668016287\n",
      "Epoch 55, Loss: 10.048454889884361\n",
      "Epoch 56, Loss: 9.999960367496197\n",
      "Epoch 57, Loss: 9.311285935915434\n",
      "Epoch 58, Loss: 9.114529921458317\n",
      "Epoch 59, Loss: 8.815358785482553\n",
      "Epoch 60, Loss: 8.579839669741117\n",
      "Epoch 61, Loss: 8.399988302817711\n",
      "Epoch 62, Loss: 8.231944505984966\n",
      "Epoch 63, Loss: 8.072438331750723\n",
      "Epoch 64, Loss: 8.166368869634775\n",
      "Epoch 65, Loss: 8.049825686674852\n",
      "Epoch 66, Loss: 7.826411540691669\n",
      "Epoch 67, Loss: 7.669543504714966\n",
      "Epoch 68, Loss: 7.666603363477266\n",
      "Epoch 69, Loss: 7.6136109828948975\n",
      "Epoch 70, Loss: 7.44425261937655\n",
      "Epoch 71, Loss: 7.244175617511456\n",
      "Epoch 72, Loss: 7.15575359417842\n",
      "Epoch 73, Loss: 6.978694934111375\n",
      "Epoch 74, Loss: 6.842316297384409\n",
      "Epoch 75, Loss: 6.825381499070388\n",
      "Epoch 76, Loss: 6.804052719703088\n",
      "Epoch 77, Loss: 6.726034512886634\n",
      "Epoch 78, Loss: 6.657798620370718\n",
      "Epoch 79, Loss: 6.542570902751042\n",
      "Epoch 80, Loss: 6.5973088557903585\n",
      "Epoch 81, Loss: 6.500971005513118\n",
      "Epoch 82, Loss: 6.515860685935388\n",
      "Epoch 83, Loss: 6.507826456656823\n",
      "Epoch 84, Loss: 6.418091755646926\n",
      "Epoch 85, Loss: 6.380407168315007\n",
      "Epoch 86, Loss: 6.314852402760432\n",
      "Epoch 87, Loss: 6.334648260703454\n",
      "Epoch 88, Loss: 6.341236591339111\n",
      "Epoch 89, Loss: 6.313716356570904\n",
      "Epoch 90, Loss: 6.301661546413715\n",
      "Epoch 91, Loss: 6.243962452961848\n",
      "Epoch 92, Loss: 6.238697162041297\n",
      "Epoch 93, Loss: 6.255304428247305\n",
      "Epoch 94, Loss: 6.28015842804542\n",
      "Epoch 95, Loss: 6.27390038050138\n",
      "Epoch 96, Loss: 6.203702944975633\n",
      "Epoch 97, Loss: 6.1908267461336575\n",
      "Epoch 98, Loss: 6.151289866520808\n",
      "Epoch 99, Loss: 6.115549949499277\n",
      "Epoch 100, Loss: 6.085373695080097\n",
      "Epoch 101, Loss: 6.119616783582247\n",
      "Epoch 102, Loss: 6.101044178009033\n",
      "Epoch 103, Loss: 6.117246242669912\n",
      "Epoch 104, Loss: 6.115312081116897\n",
      "Epoch 105, Loss: 6.1819042425889235\n",
      "Epoch 106, Loss: 6.248211035361657\n",
      "Epoch 107, Loss: 6.247872352600098\n",
      "Epoch 108, Loss: 6.414643471057598\n",
      "Epoch 109, Loss: 6.645924128018892\n",
      "Epoch 110, Loss: 6.840016218332144\n",
      "Epoch 111, Loss: 6.789668560028076\n",
      "Epoch 112, Loss: 6.6101564810826225\n",
      "Epoch 113, Loss: 7.0267890233259935\n",
      "Epoch 114, Loss: 7.032445063957801\n",
      "Epoch 115, Loss: 7.46212572317857\n",
      "Epoch 116, Loss: 8.647573911226713\n",
      "Epoch 117, Loss: 10.429963533694927\n",
      "Epoch 118, Loss: 9.305208921432495\n",
      "Epoch 119, Loss: 9.321584188021147\n",
      "Epoch 120, Loss: 7.969814612315251\n",
      "Epoch 121, Loss: 7.4591734042534465\n",
      "Epoch 122, Loss: 6.903963455787072\n",
      "Epoch 123, Loss: 6.473706795619084\n",
      "Epoch 124, Loss: 6.315370046175444\n",
      "Epoch 125, Loss: 6.1898836355942946\n",
      "Epoch 126, Loss: 6.123690825242263\n",
      "Epoch 127, Loss: 6.078474393257728\n",
      "Epoch 128, Loss: 6.0566017627716064\n",
      "Epoch 129, Loss: 6.163440025769747\n",
      "Epoch 130, Loss: 6.1904973433567925\n",
      "Epoch 131, Loss: 6.153573384651771\n",
      "Epoch 132, Loss: 6.10485315322876\n",
      "Epoch 133, Loss: 6.115879957492535\n",
      "Epoch 134, Loss: 6.015183687210083\n",
      "Epoch 135, Loss: 5.995941877365112\n",
      "Epoch 136, Loss: 6.013814944487351\n",
      "Epoch 137, Loss: 6.007799295278696\n",
      "Epoch 138, Loss: 5.9547589008624735\n",
      "Epoch 139, Loss: 5.960793403478769\n",
      "Epoch 140, Loss: 5.953015034015362\n",
      "Epoch 141, Loss: 5.968899800227239\n",
      "Epoch 142, Loss: 5.927172312369714\n",
      "Epoch 143, Loss: 5.912659773459802\n",
      "Epoch 144, Loss: 5.8792084363790655\n",
      "Epoch 145, Loss: 5.89604821571937\n",
      "Epoch 146, Loss: 5.904706716537476\n",
      "Epoch 147, Loss: 5.921211297695454\n",
      "Epoch 148, Loss: 5.972439417472253\n",
      "Epoch 149, Loss: 6.015919061807486\n",
      "Epoch 150, Loss: 6.229954554484441\n",
      "Epoch 151, Loss: 6.780286092024583\n",
      "Epoch 152, Loss: 7.331988297975981\n",
      "Epoch 153, Loss: 7.486142158508301\n",
      "Epoch 154, Loss: 8.581329932579628\n",
      "Epoch 155, Loss: 8.646654257407555\n",
      "Epoch 156, Loss: 8.243687299581675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:22,368] Trial 54 finished with value: 33.04808694628568 and parameters: {'latent_dim_z1': 18, 'latent_dim_z2': 65, 'hidden_dim': 72, 'epochs': 160, 'causal_reg': 0.21187515408937319, 'learning_rate': 0.000391342425396432}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157, Loss: 8.16598233809838\n",
      "Epoch 158, Loss: 7.383534944974459\n",
      "Epoch 159, Loss: 7.324300876030555\n",
      "Epoch 160, Loss: 6.908099724696233\n",
      "Epoch 1, Loss: 451.05277839073767\n",
      "Epoch 2, Loss: 191.52603032038763\n",
      "Epoch 3, Loss: 135.3164520263672\n",
      "Epoch 4, Loss: 116.21911034217247\n",
      "Epoch 5, Loss: 100.64284530052772\n",
      "Epoch 6, Loss: 89.61008291978102\n",
      "Epoch 7, Loss: 84.00241411649264\n",
      "Epoch 8, Loss: 69.74642342787523\n",
      "Epoch 9, Loss: 63.72851078326885\n",
      "Epoch 10, Loss: 55.511270156273476\n",
      "Epoch 11, Loss: 51.959073286790115\n",
      "Epoch 12, Loss: 46.607233634361855\n",
      "Epoch 13, Loss: 42.03335820711576\n",
      "Epoch 14, Loss: 37.971642420842095\n",
      "Epoch 15, Loss: 32.8851432800293\n",
      "Epoch 16, Loss: 33.90367031097412\n",
      "Epoch 17, Loss: 28.523533454308144\n",
      "Epoch 18, Loss: 25.621443051558273\n",
      "Epoch 19, Loss: 24.939438416407658\n",
      "Epoch 20, Loss: 23.19779557448167\n",
      "Epoch 21, Loss: 21.498704103323128\n",
      "Epoch 22, Loss: 20.223710793715256\n",
      "Epoch 23, Loss: 16.937468712146465\n",
      "Epoch 24, Loss: 15.821211704841026\n",
      "Epoch 25, Loss: 13.643996202028715\n",
      "Epoch 26, Loss: 14.0638516499446\n",
      "Epoch 27, Loss: 15.222443323868971\n",
      "Epoch 28, Loss: 12.622256223972027\n",
      "Epoch 29, Loss: 12.021233650354239\n",
      "Epoch 30, Loss: 12.440077304840088\n",
      "Epoch 31, Loss: 11.849743146162767\n",
      "Epoch 32, Loss: 10.447049471048208\n",
      "Epoch 33, Loss: 10.471350339742807\n",
      "Epoch 34, Loss: 10.001194146963266\n",
      "Epoch 35, Loss: 8.966314994371855\n",
      "Epoch 36, Loss: 8.488768650935246\n",
      "Epoch 37, Loss: 8.916753347103413\n",
      "Epoch 38, Loss: 9.062574955133291\n",
      "Epoch 39, Loss: 8.635942330727211\n",
      "Epoch 40, Loss: 8.409479232934805\n",
      "Epoch 41, Loss: 8.24411443563608\n",
      "Epoch 42, Loss: 7.864245964930608\n",
      "Epoch 43, Loss: 8.135488271713257\n",
      "Epoch 44, Loss: 8.075377152516293\n",
      "Epoch 45, Loss: 8.39993797815763\n",
      "Epoch 46, Loss: 7.429089491183941\n",
      "Epoch 47, Loss: 7.2588058068202095\n",
      "Epoch 48, Loss: 7.142776507597703\n",
      "Epoch 49, Loss: 7.037079370938814\n",
      "Epoch 50, Loss: 7.057621203936064\n",
      "Epoch 51, Loss: 7.291435681856596\n",
      "Epoch 52, Loss: 8.933750299307016\n",
      "Epoch 53, Loss: 15.444643900944637\n",
      "Epoch 54, Loss: 15.11234943683331\n",
      "Epoch 55, Loss: 13.12565513757559\n",
      "Epoch 56, Loss: 10.70092958670396\n",
      "Epoch 57, Loss: 9.305333430950459\n",
      "Epoch 58, Loss: 7.700897418535673\n",
      "Epoch 59, Loss: 7.267946628423838\n",
      "Epoch 60, Loss: 7.053422671097976\n",
      "Epoch 61, Loss: 7.281433527286236\n",
      "Epoch 62, Loss: 7.054086776880117\n",
      "Epoch 63, Loss: 6.788952772433941\n",
      "Epoch 64, Loss: 6.665496771152203\n",
      "Epoch 65, Loss: 7.531186525638287\n",
      "Epoch 66, Loss: 7.964530651385967\n",
      "Epoch 67, Loss: 7.834927357160128\n",
      "Epoch 68, Loss: 7.182576784720788\n",
      "Epoch 69, Loss: 6.942719587912927\n",
      "Epoch 70, Loss: 6.376935390325693\n",
      "Epoch 71, Loss: 6.256435981163611\n",
      "Epoch 72, Loss: 6.527278661727905\n",
      "Epoch 73, Loss: 6.740184252078716\n",
      "Epoch 74, Loss: 7.247585406670203\n",
      "Epoch 75, Loss: 8.31265112069937\n",
      "Epoch 76, Loss: 9.043368963094858\n",
      "Epoch 77, Loss: 8.227200379738441\n",
      "Epoch 78, Loss: 8.55257929288424\n",
      "Epoch 79, Loss: 8.30864139703604\n",
      "Epoch 80, Loss: 8.383355690882755\n",
      "Epoch 81, Loss: 8.401847802675688\n",
      "Epoch 82, Loss: 8.9810175712292\n",
      "Epoch 83, Loss: 9.265883133961605\n",
      "Epoch 84, Loss: 8.728718500870924\n",
      "Epoch 85, Loss: 8.050795280016386\n",
      "Epoch 86, Loss: 7.099979327275203\n",
      "Epoch 87, Loss: 7.415979275336633\n",
      "Epoch 88, Loss: 6.980064410429734\n",
      "Epoch 89, Loss: 6.7141911800091085\n",
      "Epoch 90, Loss: 6.860440272551316\n",
      "Epoch 91, Loss: 7.014952659606934\n",
      "Epoch 92, Loss: 6.6123031469491815\n",
      "Epoch 93, Loss: 6.651371570733877\n",
      "Epoch 94, Loss: 6.776772737503052\n",
      "Epoch 95, Loss: 6.980143216940073\n",
      "Epoch 96, Loss: 6.581940999397864\n",
      "Epoch 97, Loss: 6.329656949410071\n",
      "Epoch 98, Loss: 6.315776678232046\n",
      "Epoch 99, Loss: 6.3084420424241285\n",
      "Epoch 100, Loss: 6.213579397935134\n",
      "Epoch 101, Loss: 6.282841517375066\n",
      "Epoch 102, Loss: 6.231267360540537\n",
      "Epoch 103, Loss: 6.1519356690920315\n",
      "Epoch 104, Loss: 6.204086872247549\n",
      "Epoch 105, Loss: 6.082684847024771\n",
      "Epoch 106, Loss: 6.619482388863196\n",
      "Epoch 107, Loss: 7.206652237818791\n",
      "Epoch 108, Loss: 8.153648871641893\n",
      "Epoch 109, Loss: 11.360489405118502\n",
      "Epoch 110, Loss: 10.92216473359328\n",
      "Epoch 111, Loss: 11.308683633804321\n",
      "Epoch 112, Loss: 11.988186286045956\n",
      "Epoch 113, Loss: 15.48453870186439\n",
      "Epoch 114, Loss: 14.715727274234478\n",
      "Epoch 115, Loss: 10.574219960432787\n",
      "Epoch 116, Loss: 11.260704755783081\n",
      "Epoch 117, Loss: 8.605858380977924\n",
      "Epoch 118, Loss: 8.437271136503954\n",
      "Epoch 119, Loss: 7.309136115587675\n",
      "Epoch 120, Loss: 7.016262127802922\n",
      "Epoch 121, Loss: 6.706054284022405\n",
      "Epoch 122, Loss: 6.624103747881376\n",
      "Epoch 123, Loss: 6.506044699595525\n",
      "Epoch 124, Loss: 6.264458472912128\n",
      "Epoch 125, Loss: 6.166978175823505\n",
      "Epoch 126, Loss: 6.013738375443679\n",
      "Epoch 127, Loss: 5.916389667070829\n",
      "Epoch 128, Loss: 5.858768169696514\n",
      "Epoch 129, Loss: 5.8297059169182415\n",
      "Epoch 130, Loss: 5.850459025456355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:25,410] Trial 55 finished with value: 19.790516893225114 and parameters: {'latent_dim_z1': 13, 'latent_dim_z2': 61, 'hidden_dim': 130, 'epochs': 137, 'causal_reg': 0.1415936509272904, 'learning_rate': 0.0010493253566061004}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131, Loss: 5.865561797068669\n",
      "Epoch 132, Loss: 5.840099664834829\n",
      "Epoch 133, Loss: 5.797907700905433\n",
      "Epoch 134, Loss: 5.8182011017432576\n",
      "Epoch 135, Loss: 5.835990722362812\n",
      "Epoch 136, Loss: 5.844585748819204\n",
      "Epoch 137, Loss: 5.816667300004226\n",
      "Epoch 1, Loss: 1167.0473937988281\n",
      "Epoch 2, Loss: 855.5762481689453\n",
      "Epoch 3, Loss: 732.9498455341046\n",
      "Epoch 4, Loss: 612.1430493868314\n",
      "Epoch 5, Loss: 507.4372717050406\n",
      "Epoch 6, Loss: 440.79166529728815\n",
      "Epoch 7, Loss: 399.9146112295297\n",
      "Epoch 8, Loss: 371.88019532423755\n",
      "Epoch 9, Loss: 350.6655977689303\n",
      "Epoch 10, Loss: 331.20135321983923\n",
      "Epoch 11, Loss: 315.00049767127405\n",
      "Epoch 12, Loss: 300.4711626493014\n",
      "Epoch 13, Loss: 287.0379594656137\n",
      "Epoch 14, Loss: 276.0318650465745\n",
      "Epoch 15, Loss: 265.17654448289136\n",
      "Epoch 16, Loss: 254.4939986008864\n",
      "Epoch 17, Loss: 244.8587819612943\n",
      "Epoch 18, Loss: 236.46276650061975\n",
      "Epoch 19, Loss: 228.30481455876276\n",
      "Epoch 20, Loss: 220.92015501169058\n",
      "Epoch 21, Loss: 213.55147889944223\n",
      "Epoch 22, Loss: 207.25979232788086\n",
      "Epoch 23, Loss: 200.9520407456618\n",
      "Epoch 24, Loss: 194.88746026846079\n",
      "Epoch 25, Loss: 188.5554973895733\n",
      "Epoch 26, Loss: 182.78289677546576\n",
      "Epoch 27, Loss: 176.911311516395\n",
      "Epoch 28, Loss: 171.7155629671537\n",
      "Epoch 29, Loss: 167.66600858248196\n",
      "Epoch 30, Loss: 162.38301453223596\n",
      "Epoch 31, Loss: 157.94263590299167\n",
      "Epoch 32, Loss: 153.17215391305777\n",
      "Epoch 33, Loss: 149.5406115605281\n",
      "Epoch 34, Loss: 144.83121035649225\n",
      "Epoch 35, Loss: 140.78474793067346\n",
      "Epoch 36, Loss: 137.70343897892877\n",
      "Epoch 37, Loss: 134.25801834693323\n",
      "Epoch 38, Loss: 130.06733879676233\n",
      "Epoch 39, Loss: 127.28437174283542\n",
      "Epoch 40, Loss: 122.61512462909405\n",
      "Epoch 41, Loss: 119.21880428607648\n",
      "Epoch 42, Loss: 116.19681226290189\n",
      "Epoch 43, Loss: 113.71928508465106\n",
      "Epoch 44, Loss: 109.83776297936073\n",
      "Epoch 45, Loss: 107.2204406444843\n",
      "Epoch 46, Loss: 103.85178976792555\n",
      "Epoch 47, Loss: 100.81382766136757\n",
      "Epoch 48, Loss: 98.06570683992825\n",
      "Epoch 49, Loss: 95.38395676246056\n",
      "Epoch 50, Loss: 93.89105224609375\n",
      "Epoch 51, Loss: 90.85139259925255\n",
      "Epoch 52, Loss: 88.61805974520169\n",
      "Epoch 53, Loss: 86.43081532991849\n",
      "Epoch 54, Loss: 83.7281190432035\n",
      "Epoch 55, Loss: 80.42378997802734\n",
      "Epoch 56, Loss: 79.2078742980957\n",
      "Epoch 57, Loss: 76.71948403578538\n",
      "Epoch 58, Loss: 74.79553882892316\n",
      "Epoch 59, Loss: 72.32734489440918\n",
      "Epoch 60, Loss: 70.12364468207726\n",
      "Epoch 61, Loss: 69.2993092903724\n",
      "Epoch 62, Loss: 66.1562428841224\n",
      "Epoch 63, Loss: 64.22182163825401\n",
      "Epoch 64, Loss: 62.4258875480065\n",
      "Epoch 65, Loss: 60.854798610393814\n",
      "Epoch 66, Loss: 59.080627368046684\n",
      "Epoch 67, Loss: 58.0895382807805\n",
      "Epoch 68, Loss: 57.10964232224684\n",
      "Epoch 69, Loss: 54.86989256051871\n",
      "Epoch 70, Loss: 53.25845036139855\n",
      "Epoch 71, Loss: 51.82824729039119\n",
      "Epoch 72, Loss: 50.13282911594097\n",
      "Epoch 73, Loss: 50.09757063939021\n",
      "Epoch 74, Loss: 47.83877328725961\n",
      "Epoch 75, Loss: 46.48515811333289\n",
      "Epoch 76, Loss: 45.595953061030464\n",
      "Epoch 77, Loss: 43.89928040137658\n",
      "Epoch 78, Loss: 43.08774493290828\n",
      "Epoch 79, Loss: 41.609520838810845\n",
      "Epoch 80, Loss: 40.59280997056227\n",
      "Epoch 81, Loss: 39.330670650188736\n",
      "Epoch 82, Loss: 38.34506647403423\n",
      "Epoch 83, Loss: 37.591575402479904\n",
      "Epoch 84, Loss: 37.14817472604605\n",
      "Epoch 85, Loss: 35.56754625760592\n",
      "Epoch 86, Loss: 34.492899014399605\n",
      "Epoch 87, Loss: 33.786363675044136\n",
      "Epoch 88, Loss: 33.21940715496357\n",
      "Epoch 89, Loss: 32.246795654296875\n",
      "Epoch 90, Loss: 31.41111520620493\n",
      "Epoch 91, Loss: 30.576039974506084\n",
      "Epoch 92, Loss: 29.896458845872147\n",
      "Epoch 93, Loss: 29.2725390287546\n",
      "Epoch 94, Loss: 28.213772993821365\n",
      "Epoch 95, Loss: 28.13834516818707\n",
      "Epoch 96, Loss: 27.530058200542744\n",
      "Epoch 97, Loss: 26.44043100797213\n",
      "Epoch 98, Loss: 25.847193277799168\n",
      "Epoch 99, Loss: 25.175038814544678\n",
      "Epoch 100, Loss: 24.544401462261494\n",
      "Epoch 101, Loss: 24.036089493678165\n",
      "Epoch 102, Loss: 23.595683574676514\n",
      "Epoch 103, Loss: 22.898416152367226\n",
      "Epoch 104, Loss: 22.62367743712205\n",
      "Epoch 105, Loss: 21.774936345907356\n",
      "Epoch 106, Loss: 22.07976898780236\n",
      "Epoch 107, Loss: 20.86854865000798\n",
      "Epoch 108, Loss: 20.37538590798011\n",
      "Epoch 109, Loss: 19.988372545975906\n",
      "Epoch 110, Loss: 19.607967193310078\n",
      "Epoch 111, Loss: 19.152597940885105\n",
      "Epoch 112, Loss: 18.758460448338436\n",
      "Epoch 113, Loss: 18.240831045004036\n",
      "Epoch 114, Loss: 17.909583201775185\n",
      "Epoch 115, Loss: 17.554693405444805\n",
      "Epoch 116, Loss: 17.14408324314998\n",
      "Epoch 117, Loss: 16.75785020681528\n",
      "Epoch 118, Loss: 16.42117159183209\n",
      "Epoch 119, Loss: 16.190317740807167\n",
      "Epoch 120, Loss: 16.09730973610511\n",
      "Epoch 121, Loss: 15.509302396040697\n",
      "Epoch 122, Loss: 15.181597819695106\n",
      "Epoch 123, Loss: 15.058980244856615\n",
      "Epoch 124, Loss: 14.794646483201246\n",
      "Epoch 125, Loss: 14.428231789515568\n",
      "Epoch 126, Loss: 14.179965899540829\n",
      "Epoch 127, Loss: 13.876770496368408\n",
      "Epoch 128, Loss: 13.652913680443397\n",
      "Epoch 129, Loss: 13.36718170459454\n",
      "Epoch 130, Loss: 13.250365275603075\n",
      "Epoch 131, Loss: 13.042455489818867\n",
      "Epoch 132, Loss: 12.720567849966196\n",
      "Epoch 133, Loss: 12.507171630859375\n",
      "Epoch 134, Loss: 12.317913348858173\n",
      "Epoch 135, Loss: 12.012666115394005\n",
      "Epoch 136, Loss: 11.843172696920542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:28,749] Trial 56 finished with value: 113.39471959307954 and parameters: {'latent_dim_z1': 22, 'latent_dim_z2': 66, 'hidden_dim': 178, 'epochs': 140, 'causal_reg': 0.14697885946035397, 'learning_rate': 9.303040318122615e-05}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137, Loss: 11.624183141268217\n",
      "Epoch 138, Loss: 11.4470660503094\n",
      "Epoch 139, Loss: 11.292095771202675\n",
      "Epoch 140, Loss: 11.113763277347271\n",
      "Epoch 1, Loss: 744.1582688551682\n",
      "Epoch 2, Loss: 325.5491758493277\n",
      "Epoch 3, Loss: 252.50595444899338\n",
      "Epoch 4, Loss: 213.26014709472656\n",
      "Epoch 5, Loss: 182.17055834256686\n",
      "Epoch 6, Loss: 154.42965199397162\n",
      "Epoch 7, Loss: 137.54001822838416\n",
      "Epoch 8, Loss: 144.2844112102802\n",
      "Epoch 9, Loss: 111.57497611412636\n",
      "Epoch 10, Loss: 101.64962357741136\n",
      "Epoch 11, Loss: 84.75498118767372\n",
      "Epoch 12, Loss: 73.07262758108286\n",
      "Epoch 13, Loss: 64.42188185911913\n",
      "Epoch 14, Loss: 56.411393238947944\n",
      "Epoch 15, Loss: 49.69795901958759\n",
      "Epoch 16, Loss: 51.693258615640495\n",
      "Epoch 17, Loss: 48.82645786725558\n",
      "Epoch 18, Loss: 34.70437570718619\n",
      "Epoch 19, Loss: 28.571447739234337\n",
      "Epoch 20, Loss: 25.008550240443302\n",
      "Epoch 21, Loss: 24.879561277536247\n",
      "Epoch 22, Loss: 25.862553596496582\n",
      "Epoch 23, Loss: 22.515486350426308\n",
      "Epoch 24, Loss: 19.120896100997925\n",
      "Epoch 25, Loss: 15.78719399525569\n",
      "Epoch 26, Loss: 13.449742867396427\n",
      "Epoch 27, Loss: 12.389522882608267\n",
      "Epoch 28, Loss: 11.138680036251362\n",
      "Epoch 29, Loss: 10.715158407504742\n",
      "Epoch 30, Loss: 10.05440222300016\n",
      "Epoch 31, Loss: 9.449736008277306\n",
      "Epoch 32, Loss: 8.762405047049889\n",
      "Epoch 33, Loss: 8.168239245047936\n",
      "Epoch 34, Loss: 7.92506898366488\n",
      "Epoch 35, Loss: 7.610270793621357\n",
      "Epoch 36, Loss: 7.6181631821852465\n",
      "Epoch 37, Loss: 8.202359878099882\n",
      "Epoch 38, Loss: 9.290891243861271\n",
      "Epoch 39, Loss: 8.741169892824614\n",
      "Epoch 40, Loss: 9.2794619340163\n",
      "Epoch 41, Loss: 9.784829103029692\n",
      "Epoch 42, Loss: 9.71727130963252\n",
      "Epoch 43, Loss: 9.964360237121582\n",
      "Epoch 44, Loss: 10.031483356769268\n",
      "Epoch 45, Loss: 9.491691717734703\n",
      "Epoch 46, Loss: 9.421467230870174\n",
      "Epoch 47, Loss: 10.726552633138803\n",
      "Epoch 48, Loss: 10.175461842463566\n",
      "Epoch 49, Loss: 12.968667122033926\n",
      "Epoch 50, Loss: 17.4507464995751\n",
      "Epoch 51, Loss: 21.813242105337288\n",
      "Epoch 52, Loss: 21.84852416698749\n",
      "Epoch 53, Loss: 17.41290969115037\n",
      "Epoch 54, Loss: 12.619135563190166\n",
      "Epoch 55, Loss: 10.421381858678965\n",
      "Epoch 56, Loss: 9.121553182601929\n",
      "Epoch 57, Loss: 8.78461636029757\n",
      "Epoch 58, Loss: 7.884865027207595\n",
      "Epoch 59, Loss: 7.355949475215032\n",
      "Epoch 60, Loss: 7.18733103458698\n",
      "Epoch 61, Loss: 6.965984124403733\n",
      "Epoch 62, Loss: 6.698574231221126\n",
      "Epoch 63, Loss: 6.356478085884681\n",
      "Epoch 64, Loss: 6.32943083689763\n",
      "Epoch 65, Loss: 6.42300218802232\n",
      "Epoch 66, Loss: 6.273586750030518\n",
      "Epoch 67, Loss: 6.25356910778926\n",
      "Epoch 68, Loss: 7.321352665240948\n",
      "Epoch 69, Loss: 7.95654008938716\n",
      "Epoch 70, Loss: 9.19174441924462\n",
      "Epoch 71, Loss: 9.417520889869103\n",
      "Epoch 72, Loss: 9.64769566976107\n",
      "Epoch 73, Loss: 11.898867808855497\n",
      "Epoch 74, Loss: 10.801394132467417\n",
      "Epoch 75, Loss: 9.135918140411377\n",
      "Epoch 76, Loss: 8.739401927361122\n",
      "Epoch 77, Loss: 9.042384459422184\n",
      "Epoch 78, Loss: 8.555418491363525\n",
      "Epoch 79, Loss: 9.06154438165518\n",
      "Epoch 80, Loss: 8.470215302247267\n",
      "Epoch 81, Loss: 7.930382893635676\n",
      "Epoch 82, Loss: 7.484784181301411\n",
      "Epoch 83, Loss: 8.137326167179989\n",
      "Epoch 84, Loss: 7.2703094482421875\n",
      "Epoch 85, Loss: 6.82622060408959\n",
      "Epoch 86, Loss: 6.852582711439866\n",
      "Epoch 87, Loss: 7.424469525997456\n",
      "Epoch 88, Loss: 7.932149190169114\n",
      "Epoch 89, Loss: 8.817328526423527\n",
      "Epoch 90, Loss: 12.247309959851778\n",
      "Epoch 91, Loss: 11.031208625206581\n",
      "Epoch 92, Loss: 9.349947874362652\n",
      "Epoch 93, Loss: 8.213721458728497\n",
      "Epoch 94, Loss: 7.302480129095224\n",
      "Epoch 95, Loss: 8.0250037083259\n",
      "Epoch 96, Loss: 8.232879950450016\n",
      "Epoch 97, Loss: 8.401335221070509\n",
      "Epoch 98, Loss: 9.047425196721004\n",
      "Epoch 99, Loss: 8.809350728988647\n",
      "Epoch 100, Loss: 9.247543279941265\n",
      "Epoch 101, Loss: 9.42812334574186\n",
      "Epoch 102, Loss: 10.141196819452139\n",
      "Epoch 103, Loss: 12.037131804686327\n",
      "Epoch 104, Loss: 10.055787049807035\n",
      "Epoch 105, Loss: 8.940762336437519\n",
      "Epoch 106, Loss: 9.361934863604032\n",
      "Epoch 107, Loss: 8.655953755745522\n",
      "Epoch 108, Loss: 7.564490630076482\n",
      "Epoch 109, Loss: 7.246303411630484\n",
      "Epoch 110, Loss: 6.905752218686617\n",
      "Epoch 111, Loss: 6.650934604498056\n",
      "Epoch 112, Loss: 6.635096293229323\n",
      "Epoch 113, Loss: 6.219161565487202\n",
      "Epoch 114, Loss: 6.324805149665246\n",
      "Epoch 115, Loss: 6.257617253523606\n",
      "Epoch 116, Loss: 6.421619250224187\n",
      "Epoch 117, Loss: 6.220545713718121\n",
      "Epoch 118, Loss: 6.1880911863767185\n",
      "Epoch 119, Loss: 6.201440297640287\n",
      "Epoch 120, Loss: 6.971239034946148\n",
      "Epoch 121, Loss: 7.863148835989145\n",
      "Epoch 122, Loss: 7.712983278127817\n",
      "Epoch 123, Loss: 8.563125867110033\n",
      "Epoch 124, Loss: 9.006963693178617\n",
      "Epoch 125, Loss: 8.379882152263935\n",
      "Epoch 126, Loss: 7.504540773538443\n",
      "Epoch 127, Loss: 7.2796609951899605\n",
      "Epoch 128, Loss: 6.558324538744413\n",
      "Epoch 129, Loss: 6.412905142857478\n",
      "Epoch 130, Loss: 6.957393462841328\n",
      "Epoch 131, Loss: 7.40514219724215\n",
      "Epoch 132, Loss: 9.76531665141766\n",
      "Epoch 133, Loss: 10.978608479866615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:32,434] Trial 57 finished with value: 28.87783389033435 and parameters: {'latent_dim_z1': 26, 'latent_dim_z2': 71, 'hidden_dim': 211, 'epochs': 139, 'causal_reg': 0.8625849725137769, 'learning_rate': 0.0010672339656070955}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134, Loss: 9.803385514479418\n",
      "Epoch 135, Loss: 9.26221223977896\n",
      "Epoch 136, Loss: 8.865681629914503\n",
      "Epoch 137, Loss: 8.913830115244938\n",
      "Epoch 138, Loss: 8.074142364355234\n",
      "Epoch 139, Loss: 7.730570921531091\n",
      "Epoch 1, Loss: 617.2092003455529\n",
      "Epoch 2, Loss: 442.79097630427435\n",
      "Epoch 3, Loss: 345.0588607788086\n",
      "Epoch 4, Loss: 260.68299836378833\n",
      "Epoch 5, Loss: 213.01749742948093\n",
      "Epoch 6, Loss: 183.6397837125338\n",
      "Epoch 7, Loss: 166.34199670644907\n",
      "Epoch 8, Loss: 153.8347476078914\n",
      "Epoch 9, Loss: 143.9288846529447\n",
      "Epoch 10, Loss: 134.9538868390597\n",
      "Epoch 11, Loss: 128.21915949307956\n",
      "Epoch 12, Loss: 121.19513511657715\n",
      "Epoch 13, Loss: 115.41322311988243\n",
      "Epoch 14, Loss: 110.41625697796161\n",
      "Epoch 15, Loss: 106.66491097670335\n",
      "Epoch 16, Loss: 101.47840485206017\n",
      "Epoch 17, Loss: 96.19640086247371\n",
      "Epoch 18, Loss: 91.78018951416016\n",
      "Epoch 19, Loss: 87.92357870248648\n",
      "Epoch 20, Loss: 84.5013283949632\n",
      "Epoch 21, Loss: 81.78715456449069\n",
      "Epoch 22, Loss: 78.51387750185452\n",
      "Epoch 23, Loss: 76.62177995535043\n",
      "Epoch 24, Loss: 71.76965111952562\n",
      "Epoch 25, Loss: 68.87036198836107\n",
      "Epoch 26, Loss: 65.95900733654315\n",
      "Epoch 27, Loss: 63.43728490976187\n",
      "Epoch 28, Loss: 61.00588101607103\n",
      "Epoch 29, Loss: 58.59204996549166\n",
      "Epoch 30, Loss: 56.61980078770564\n",
      "Epoch 31, Loss: 55.59669208526611\n",
      "Epoch 32, Loss: 52.698524988614594\n",
      "Epoch 33, Loss: 51.161956346951996\n",
      "Epoch 34, Loss: 49.585666583134575\n",
      "Epoch 35, Loss: 47.38729429244995\n",
      "Epoch 36, Loss: 45.287691776569076\n",
      "Epoch 37, Loss: 43.70629934164194\n",
      "Epoch 38, Loss: 41.55858905498798\n",
      "Epoch 39, Loss: 40.5810349537776\n",
      "Epoch 40, Loss: 38.80731226847722\n",
      "Epoch 41, Loss: 37.18936865146343\n",
      "Epoch 42, Loss: 35.856909898611214\n",
      "Epoch 43, Loss: 34.562830448150635\n",
      "Epoch 44, Loss: 33.956823935875526\n",
      "Epoch 45, Loss: 32.70037379631629\n",
      "Epoch 46, Loss: 30.956993726583626\n",
      "Epoch 47, Loss: 30.130879915677586\n",
      "Epoch 48, Loss: 29.39933681488037\n",
      "Epoch 49, Loss: 27.926510957571175\n",
      "Epoch 50, Loss: 26.72185597052941\n",
      "Epoch 51, Loss: 26.28383966592642\n",
      "Epoch 52, Loss: 25.35624012580285\n",
      "Epoch 53, Loss: 24.552197823157677\n",
      "Epoch 54, Loss: 23.578560499044563\n",
      "Epoch 55, Loss: 21.98971931750958\n",
      "Epoch 56, Loss: 21.64101967444787\n",
      "Epoch 57, Loss: 20.591278809767502\n",
      "Epoch 58, Loss: 20.010921735029953\n",
      "Epoch 59, Loss: 19.49145008967473\n",
      "Epoch 60, Loss: 18.58211396290706\n",
      "Epoch 61, Loss: 18.067710142869217\n",
      "Epoch 62, Loss: 18.064989163325382\n",
      "Epoch 63, Loss: 16.844991885698757\n",
      "Epoch 64, Loss: 16.206070166367752\n",
      "Epoch 65, Loss: 15.705189851614145\n",
      "Epoch 66, Loss: 15.254586953383226\n",
      "Epoch 67, Loss: 14.718597228710468\n",
      "Epoch 68, Loss: 14.269993231846737\n",
      "Epoch 69, Loss: 13.89623005573566\n",
      "Epoch 70, Loss: 13.594641025249775\n",
      "Epoch 71, Loss: 13.326102825311514\n",
      "Epoch 72, Loss: 13.317905462705172\n",
      "Epoch 73, Loss: 12.885278445023756\n",
      "Epoch 74, Loss: 12.308770436507006\n",
      "Epoch 75, Loss: 12.748259214254526\n",
      "Epoch 76, Loss: 11.621431002250084\n",
      "Epoch 77, Loss: 11.260787835487953\n",
      "Epoch 78, Loss: 11.090126752853394\n",
      "Epoch 79, Loss: 10.846362168972309\n",
      "Epoch 80, Loss: 10.922528523665209\n",
      "Epoch 81, Loss: 10.61662875688993\n",
      "Epoch 82, Loss: 10.207685287182148\n",
      "Epoch 83, Loss: 9.934298276901245\n",
      "Epoch 84, Loss: 9.728772805287289\n",
      "Epoch 85, Loss: 9.629537343978882\n",
      "Epoch 86, Loss: 9.405520274088932\n",
      "Epoch 87, Loss: 9.267983931761522\n",
      "Epoch 88, Loss: 9.047603900615986\n",
      "Epoch 89, Loss: 9.019443658682016\n",
      "Epoch 90, Loss: 8.73344874382019\n",
      "Epoch 91, Loss: 8.609494796166054\n",
      "Epoch 92, Loss: 8.448573185847355\n",
      "Epoch 93, Loss: 8.323314703427828\n",
      "Epoch 94, Loss: 8.470023155212402\n",
      "Epoch 95, Loss: 8.272884130477905\n",
      "Epoch 96, Loss: 8.031678676605225\n",
      "Epoch 97, Loss: 7.901811012854943\n",
      "Epoch 98, Loss: 8.007482803784884\n",
      "Epoch 99, Loss: 7.771572113037109\n",
      "Epoch 100, Loss: 7.6841035806215725\n",
      "Epoch 101, Loss: 7.564107913237351\n",
      "Epoch 102, Loss: 7.482652774223914\n",
      "Epoch 103, Loss: 7.369129419326782\n",
      "Epoch 104, Loss: 7.292270550361047\n",
      "Epoch 105, Loss: 7.219096752313467\n",
      "Epoch 106, Loss: 7.188299600894634\n",
      "Epoch 107, Loss: 7.16334187067472\n",
      "Epoch 108, Loss: 7.030077439088088\n",
      "Epoch 109, Loss: 6.973935475716224\n",
      "Epoch 110, Loss: 6.947980477259709\n",
      "Epoch 111, Loss: 6.915621005571806\n",
      "Epoch 112, Loss: 6.869164962034959\n",
      "Epoch 113, Loss: 6.838858164273775\n",
      "Epoch 114, Loss: 6.807180312963633\n",
      "Epoch 115, Loss: 6.784918363277729\n",
      "Epoch 116, Loss: 6.727655355746929\n",
      "Epoch 117, Loss: 6.6526486323429985\n",
      "Epoch 118, Loss: 6.5905285981985235\n",
      "Epoch 119, Loss: 6.5983662605285645\n",
      "Epoch 120, Loss: 6.509782516039335\n",
      "Epoch 121, Loss: 6.484616958178007\n",
      "Epoch 122, Loss: 6.497884365228506\n",
      "Epoch 123, Loss: 6.534803904019869\n",
      "Epoch 124, Loss: 6.388737201690674\n",
      "Epoch 125, Loss: 6.344155091505784\n",
      "Epoch 126, Loss: 6.32056003350478\n",
      "Epoch 127, Loss: 6.312480046198918\n",
      "Epoch 128, Loss: 6.30024818273691\n",
      "Epoch 129, Loss: 6.249400212214543\n",
      "Epoch 130, Loss: 6.237975982519297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:35,327] Trial 58 finished with value: 46.25757185977415 and parameters: {'latent_dim_z1': 13, 'latent_dim_z2': 59, 'hidden_dim': 129, 'epochs': 131, 'causal_reg': 0.302463079693862, 'learning_rate': 0.00022388471055283127}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131, Loss: 6.208464292379526\n",
      "Epoch 1, Loss: 1044.2905842707708\n",
      "Epoch 2, Loss: 509.73275052584137\n",
      "Epoch 3, Loss: 375.312377342811\n",
      "Epoch 4, Loss: 321.5241672809307\n",
      "Epoch 5, Loss: 283.8788860027607\n",
      "Epoch 6, Loss: 254.04150346609262\n",
      "Epoch 7, Loss: 229.51458974984976\n",
      "Epoch 8, Loss: 212.1257426922138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:35,637] Trial 59 finished with value: 311.36331419408674 and parameters: {'latent_dim_z1': 29, 'latent_dim_z2': 49, 'hidden_dim': 148, 'epochs': 13, 'causal_reg': 0.7206695226953339, 'learning_rate': 0.0004811423047777029}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 193.3867962176983\n",
      "Epoch 10, Loss: 176.20496280376727\n",
      "Epoch 11, Loss: 162.21462660569412\n",
      "Epoch 12, Loss: 148.1281393491305\n",
      "Epoch 13, Loss: 137.36795821556677\n",
      "Epoch 1, Loss: 1178.7766887958232\n",
      "Epoch 2, Loss: 571.3971088115985\n",
      "Epoch 3, Loss: 443.26112659160907\n",
      "Epoch 4, Loss: 358.3921288710374\n",
      "Epoch 5, Loss: 301.5426976130559\n",
      "Epoch 6, Loss: 249.25576136662409\n",
      "Epoch 7, Loss: 240.16909364553598\n",
      "Epoch 8, Loss: 195.28163205660306\n",
      "Epoch 9, Loss: 156.94555077185998\n",
      "Epoch 10, Loss: 131.94218210073618\n",
      "Epoch 11, Loss: 109.78845170828012\n",
      "Epoch 12, Loss: 91.29841789832481\n",
      "Epoch 13, Loss: 80.82138751103328\n",
      "Epoch 14, Loss: 68.45870267427884\n",
      "Epoch 15, Loss: 58.09099373450646\n",
      "Epoch 16, Loss: 48.31117630004883\n",
      "Epoch 17, Loss: 41.91345610985389\n",
      "Epoch 18, Loss: 36.539585700401894\n",
      "Epoch 19, Loss: 35.86225678370549\n",
      "Epoch 20, Loss: 30.530630588531494\n",
      "Epoch 21, Loss: 30.981489768395058\n",
      "Epoch 22, Loss: 25.85761121603159\n",
      "Epoch 23, Loss: 22.065033215742844\n",
      "Epoch 24, Loss: 22.216073513031006\n",
      "Epoch 25, Loss: 20.91270509132972\n",
      "Epoch 26, Loss: 18.284182878640983\n",
      "Epoch 27, Loss: 19.295795110555794\n",
      "Epoch 28, Loss: 22.61063762811514\n",
      "Epoch 29, Loss: 18.33009099960327\n",
      "Epoch 30, Loss: 16.61655541566702\n",
      "Epoch 31, Loss: 15.754669813009409\n",
      "Epoch 32, Loss: 16.014687868265007\n",
      "Epoch 33, Loss: 20.384557980757492\n",
      "Epoch 34, Loss: 21.888864260453445\n",
      "Epoch 35, Loss: 19.0739755997291\n",
      "Epoch 36, Loss: 14.694038464472843\n",
      "Epoch 37, Loss: 14.475160305316631\n",
      "Epoch 38, Loss: 13.82470728800847\n",
      "Epoch 39, Loss: 17.37351025067843\n",
      "Epoch 40, Loss: 17.123835655359123\n",
      "Epoch 41, Loss: 14.637559267190786\n",
      "Epoch 42, Loss: 14.573566986964298\n",
      "Epoch 43, Loss: 13.429618927148672\n",
      "Epoch 44, Loss: 12.954635986915001\n",
      "Epoch 45, Loss: 14.94848108291626\n",
      "Epoch 46, Loss: 14.583620328169603\n",
      "Epoch 47, Loss: 15.24125128525954\n",
      "Epoch 48, Loss: 14.064258538759672\n",
      "Epoch 49, Loss: 11.518540602463942\n",
      "Epoch 50, Loss: 10.230218758949867\n",
      "Epoch 51, Loss: 14.769748797783485\n",
      "Epoch 52, Loss: 22.18095772082989\n",
      "Epoch 53, Loss: 21.80308716113751\n",
      "Epoch 54, Loss: 14.7423127247737\n",
      "Epoch 55, Loss: 13.092583619631254\n",
      "Epoch 56, Loss: 11.399196808154766\n",
      "Epoch 57, Loss: 11.157781160794771\n",
      "Epoch 58, Loss: 11.675402567936825\n",
      "Epoch 59, Loss: 11.216072339278002\n",
      "Epoch 60, Loss: 11.014759595577534\n",
      "Epoch 61, Loss: 11.053403836030226\n",
      "Epoch 62, Loss: 10.03908634185791\n",
      "Epoch 63, Loss: 10.160335100614107\n",
      "Epoch 64, Loss: 9.535952678093544\n",
      "Epoch 65, Loss: 10.471951392980722\n",
      "Epoch 66, Loss: 10.163571486106285\n",
      "Epoch 67, Loss: 11.43333413050725\n",
      "Epoch 68, Loss: 11.405292932803814\n",
      "Epoch 69, Loss: 10.750438836904673\n",
      "Epoch 70, Loss: 12.548962556398832\n",
      "Epoch 71, Loss: 12.466540758426373\n",
      "Epoch 72, Loss: 11.889325013527504\n",
      "Epoch 73, Loss: 13.510044262959408\n",
      "Epoch 74, Loss: 12.523722740320059\n",
      "Epoch 75, Loss: 11.931993777935322\n",
      "Epoch 76, Loss: 12.476702451705933\n",
      "Epoch 77, Loss: 11.94383300267733\n",
      "Epoch 78, Loss: 11.053130241540762\n",
      "Epoch 79, Loss: 10.683443858073307\n",
      "Epoch 80, Loss: 11.241855951455923\n",
      "Epoch 81, Loss: 10.615160281841572\n",
      "Epoch 82, Loss: 10.418417930603027\n",
      "Epoch 83, Loss: 9.444038060995249\n",
      "Epoch 84, Loss: 8.226786943582388\n",
      "Epoch 85, Loss: 10.225340898220356\n",
      "Epoch 86, Loss: 11.49747316653912\n",
      "Epoch 87, Loss: 11.954570110027607\n",
      "Epoch 88, Loss: 13.861467104691725\n",
      "Epoch 89, Loss: 12.105210671058067\n",
      "Epoch 90, Loss: 10.789921412101158\n",
      "Epoch 91, Loss: 9.336732992759117\n",
      "Epoch 92, Loss: 12.855065033986019\n",
      "Epoch 93, Loss: 16.204880200899563\n",
      "Epoch 94, Loss: 12.063025712966919\n",
      "Epoch 95, Loss: 12.462733305417574\n",
      "Epoch 96, Loss: 14.037875468914326\n",
      "Epoch 97, Loss: 15.782861709594727\n",
      "Epoch 98, Loss: 14.225727007939266\n",
      "Epoch 99, Loss: 12.361750309283916\n",
      "Epoch 100, Loss: 10.397367349037758\n",
      "Epoch 101, Loss: 10.20949989098769\n",
      "Epoch 102, Loss: 9.955673566231361\n",
      "Epoch 103, Loss: 8.954910131601187\n",
      "Epoch 104, Loss: 8.055403342613808\n",
      "Epoch 105, Loss: 7.4740173633282\n",
      "Epoch 106, Loss: 7.141580783403837\n",
      "Epoch 107, Loss: 6.91309299835792\n",
      "Epoch 108, Loss: 6.965935560373159\n",
      "Epoch 109, Loss: 6.919026448176457\n",
      "Epoch 110, Loss: 6.725123130358183\n",
      "Epoch 111, Loss: 6.579175875737117\n",
      "Epoch 112, Loss: 6.3722004523644085\n",
      "Epoch 113, Loss: 6.420124494112455\n",
      "Epoch 114, Loss: 7.2851688128251295\n",
      "Epoch 115, Loss: 7.5929224491119385\n",
      "Epoch 116, Loss: 7.620910699550922\n",
      "Epoch 117, Loss: 7.879713498629057\n",
      "Epoch 118, Loss: 9.683733738385714\n",
      "Epoch 119, Loss: 10.2498339506296\n",
      "Epoch 120, Loss: 12.394394801213192\n",
      "Epoch 121, Loss: 12.58230235026433\n",
      "Epoch 122, Loss: 13.303120704797598\n",
      "Epoch 123, Loss: 13.914204872571505\n",
      "Epoch 124, Loss: 23.2454656454233\n",
      "Epoch 125, Loss: 16.553462175222542\n",
      "Epoch 126, Loss: 12.492355731817392\n",
      "Epoch 127, Loss: 10.735595483046312\n",
      "Epoch 128, Loss: 8.776028082920956\n",
      "Epoch 129, Loss: 8.181536216002245\n",
      "Epoch 130, Loss: 8.13351845741272\n",
      "Epoch 131, Loss: 7.223878273597131\n",
      "Epoch 132, Loss: 6.911356357427744\n",
      "Epoch 133, Loss: 6.793025108484121\n",
      "Epoch 134, Loss: 7.431855733578022\n",
      "Epoch 135, Loss: 7.93060458623446\n",
      "Epoch 136, Loss: 9.749625334372887\n",
      "Epoch 137, Loss: 12.26063763178312\n",
      "Epoch 138, Loss: 13.21992378968459\n",
      "Epoch 139, Loss: 12.386881241431603\n",
      "Epoch 140, Loss: 10.407596771533672\n",
      "Epoch 141, Loss: 10.299787851480337\n",
      "Epoch 142, Loss: 10.071435304788443\n",
      "Epoch 143, Loss: 10.398310936414278\n",
      "Epoch 144, Loss: 13.47441108410175\n",
      "Epoch 145, Loss: 16.82951523707463\n",
      "Epoch 146, Loss: 15.554193515043993\n",
      "Epoch 147, Loss: 12.012184583223783\n",
      "Epoch 148, Loss: 11.427027500592745\n",
      "Epoch 149, Loss: 10.467960504385141\n",
      "Epoch 150, Loss: 9.831138959297768\n",
      "Epoch 151, Loss: 9.733372339835533\n",
      "Epoch 152, Loss: 8.208332025087797\n",
      "Epoch 153, Loss: 7.830727155391987\n",
      "Epoch 154, Loss: 7.8025165521181545\n",
      "Epoch 155, Loss: 8.495864519706139\n",
      "Epoch 156, Loss: 8.224907875061035\n",
      "Epoch 157, Loss: 8.231533105556782\n",
      "Epoch 158, Loss: 7.7575308909783\n",
      "Epoch 159, Loss: 7.461695634401762\n",
      "Epoch 160, Loss: 7.201937106939463\n",
      "Epoch 161, Loss: 7.020224167750432\n",
      "Epoch 162, Loss: 7.537416549829336\n",
      "Epoch 163, Loss: 8.079858009631817\n",
      "Epoch 164, Loss: 9.227921357521645\n",
      "Epoch 165, Loss: 10.449109902748695\n",
      "Epoch 166, Loss: 13.405074559725248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:39,773] Trial 60 finished with value: 36.29653677425157 and parameters: {'latent_dim_z1': 46, 'latent_dim_z2': 59, 'hidden_dim': 123, 'epochs': 173, 'causal_reg': 0.13237258803421023, 'learning_rate': 0.0009026808492272368}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167, Loss: 12.423763843683096\n",
      "Epoch 168, Loss: 11.592411353037907\n",
      "Epoch 169, Loss: 11.671242292110737\n",
      "Epoch 170, Loss: 10.593882780808668\n",
      "Epoch 171, Loss: 10.350504710124088\n",
      "Epoch 172, Loss: 10.272329898980948\n",
      "Epoch 173, Loss: 9.997130063863901\n",
      "Epoch 1, Loss: 405.08956322303186\n",
      "Epoch 2, Loss: 159.50276917677658\n",
      "Epoch 3, Loss: 119.53359867976262\n",
      "Epoch 4, Loss: 103.50487488966722\n",
      "Epoch 5, Loss: 84.7407266176664\n",
      "Epoch 6, Loss: 79.98327284592848\n",
      "Epoch 7, Loss: 67.94237459622897\n",
      "Epoch 8, Loss: 63.654781341552734\n",
      "Epoch 9, Loss: 59.32493782043457\n",
      "Epoch 10, Loss: 50.5354364835299\n",
      "Epoch 11, Loss: 43.633028690631576\n",
      "Epoch 12, Loss: 40.90295344132643\n",
      "Epoch 13, Loss: 37.47229400047889\n",
      "Epoch 14, Loss: 32.78755176984347\n",
      "Epoch 15, Loss: 29.533413813664364\n",
      "Epoch 16, Loss: 27.17595966045673\n",
      "Epoch 17, Loss: 26.25518065232497\n",
      "Epoch 18, Loss: 23.81947084573599\n",
      "Epoch 19, Loss: 22.183862062600944\n",
      "Epoch 20, Loss: 21.07129665521475\n",
      "Epoch 21, Loss: 23.177945100344143\n",
      "Epoch 22, Loss: 18.940209535452034\n",
      "Epoch 23, Loss: 15.126417563511776\n",
      "Epoch 24, Loss: 14.051906805772047\n",
      "Epoch 25, Loss: 12.897579706632174\n",
      "Epoch 26, Loss: 12.647737264633179\n",
      "Epoch 27, Loss: 12.079244870405931\n",
      "Epoch 28, Loss: 11.104057018573467\n",
      "Epoch 29, Loss: 11.495072548206036\n",
      "Epoch 30, Loss: 10.499489380763126\n",
      "Epoch 31, Loss: 9.819487113219042\n",
      "Epoch 32, Loss: 9.092802341167744\n",
      "Epoch 33, Loss: 8.511641080562885\n",
      "Epoch 34, Loss: 8.803291412500235\n",
      "Epoch 35, Loss: 8.620076436262865\n",
      "Epoch 36, Loss: 7.902486397669866\n",
      "Epoch 37, Loss: 7.416891153042133\n",
      "Epoch 38, Loss: 7.446465345529409\n",
      "Epoch 39, Loss: 7.351245916806734\n",
      "Epoch 40, Loss: 7.1472116983853855\n",
      "Epoch 41, Loss: 6.961121632502629\n",
      "Epoch 42, Loss: 6.941466019703792\n",
      "Epoch 43, Loss: 7.0111784018003025\n",
      "Epoch 44, Loss: 7.308252297914946\n",
      "Epoch 45, Loss: 9.399537544984083\n",
      "Epoch 46, Loss: 9.043454041847816\n",
      "Epoch 47, Loss: 11.091879496207603\n",
      "Epoch 48, Loss: 13.851103012378399\n",
      "Epoch 49, Loss: 13.235625688846294\n",
      "Epoch 50, Loss: 9.958113982127262\n",
      "Epoch 51, Loss: 8.542070223734928\n",
      "Epoch 52, Loss: 7.642906207304734\n",
      "Epoch 53, Loss: 7.641342639923096\n",
      "Epoch 54, Loss: 7.682208703114436\n",
      "Epoch 55, Loss: 7.575778044187105\n",
      "Epoch 56, Loss: 7.712505175517156\n",
      "Epoch 57, Loss: 7.310051312813392\n",
      "Epoch 58, Loss: 7.492285435016338\n",
      "Epoch 59, Loss: 7.643898028593797\n",
      "Epoch 60, Loss: 7.7556382692777195\n",
      "Epoch 61, Loss: 7.228401165742141\n",
      "Epoch 62, Loss: 6.80732772900508\n",
      "Epoch 63, Loss: 6.407517524865957\n",
      "Epoch 64, Loss: 6.589462298613328\n",
      "Epoch 65, Loss: 6.349239037587092\n",
      "Epoch 66, Loss: 6.272245150346023\n",
      "Epoch 67, Loss: 6.134803570233858\n",
      "Epoch 68, Loss: 6.241409026659452\n",
      "Epoch 69, Loss: 6.503090051504282\n",
      "Epoch 70, Loss: 6.589481041981624\n",
      "Epoch 71, Loss: 10.984842520493727\n",
      "Epoch 72, Loss: 12.308666339287392\n",
      "Epoch 73, Loss: 10.995497501813448\n",
      "Epoch 74, Loss: 9.630321374306313\n",
      "Epoch 75, Loss: 10.694537951396061\n",
      "Epoch 76, Loss: 9.210245260825523\n",
      "Epoch 77, Loss: 7.568936476340661\n",
      "Epoch 78, Loss: 7.08409964121305\n",
      "Epoch 79, Loss: 6.787440556746263\n",
      "Epoch 80, Loss: 6.376578055895292\n",
      "Epoch 81, Loss: 6.166791072258582\n",
      "Epoch 82, Loss: 6.241010555854211\n",
      "Epoch 83, Loss: 6.170485973358154\n",
      "Epoch 84, Loss: 6.043256136087271\n",
      "Epoch 85, Loss: 5.950681264583881\n",
      "Epoch 86, Loss: 5.917657760473398\n",
      "Epoch 87, Loss: 5.941662238194392\n",
      "Epoch 88, Loss: 5.908107152351966\n",
      "Epoch 89, Loss: 5.872973607136653\n",
      "Epoch 90, Loss: 5.889319566579966\n",
      "Epoch 91, Loss: 5.922852186056284\n",
      "Epoch 92, Loss: 5.956118803757888\n",
      "Epoch 93, Loss: 6.002706325971163\n",
      "Epoch 94, Loss: 6.066692700752845\n",
      "Epoch 95, Loss: 6.181234378081101\n",
      "Epoch 96, Loss: 6.207661188565767\n",
      "Epoch 97, Loss: 6.175953443233784\n",
      "Epoch 98, Loss: 6.250291842680711\n",
      "Epoch 99, Loss: 6.315266499152551\n",
      "Epoch 100, Loss: 6.604360488744883\n",
      "Epoch 101, Loss: 6.696821561226478\n",
      "Epoch 102, Loss: 6.689318235103901\n",
      "Epoch 103, Loss: 7.0598156818976765\n",
      "Epoch 104, Loss: 7.552247707660381\n",
      "Epoch 105, Loss: 9.400840245760405\n",
      "Epoch 106, Loss: 11.682824336565458\n",
      "Epoch 107, Loss: 10.498074439855722\n",
      "Epoch 108, Loss: 9.0336547264686\n",
      "Epoch 109, Loss: 8.5393218260545\n",
      "Epoch 110, Loss: 7.9739376581632175\n",
      "Epoch 111, Loss: 8.214569311875563\n",
      "Epoch 112, Loss: 8.491381773581871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:42,305] Trial 61 finished with value: 19.87254599440321 and parameters: {'latent_dim_z1': 12, 'latent_dim_z2': 64, 'hidden_dim': 91, 'epochs': 116, 'causal_reg': 0.178980029413362, 'learning_rate': 0.0014955992772788874}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113, Loss: 8.642645267339853\n",
      "Epoch 114, Loss: 7.606188040513259\n",
      "Epoch 115, Loss: 7.217400312423706\n",
      "Epoch 116, Loss: 6.788046653454121\n",
      "Epoch 1, Loss: 496.0312890272874\n",
      "Epoch 2, Loss: 216.30835107656625\n",
      "Epoch 3, Loss: 163.94455924400916\n",
      "Epoch 4, Loss: 137.07548024104193\n",
      "Epoch 5, Loss: 116.93852703387921\n",
      "Epoch 6, Loss: 101.06004106081448\n",
      "Epoch 7, Loss: 95.51705624507024\n",
      "Epoch 8, Loss: 81.54156215374286\n",
      "Epoch 9, Loss: 71.2173810005188\n",
      "Epoch 10, Loss: 58.763409908001236\n",
      "Epoch 11, Loss: 56.37143721947303\n",
      "Epoch 12, Loss: 49.409669142503006\n",
      "Epoch 13, Loss: 45.144210595351\n",
      "Epoch 14, Loss: 43.729154953589806\n",
      "Epoch 15, Loss: 44.09669215862568\n",
      "Epoch 16, Loss: 41.216646707974945\n",
      "Epoch 17, Loss: 32.229789587167595\n",
      "Epoch 18, Loss: 28.22117805480957\n",
      "Epoch 19, Loss: 26.080138573279747\n",
      "Epoch 20, Loss: 25.064277758965126\n",
      "Epoch 21, Loss: 21.863012900719276\n",
      "Epoch 22, Loss: 18.771241151369534\n",
      "Epoch 23, Loss: 17.156020402908325\n",
      "Epoch 24, Loss: 17.191907992729774\n",
      "Epoch 25, Loss: 15.271953142606295\n",
      "Epoch 26, Loss: 14.237247430361235\n",
      "Epoch 27, Loss: 13.060848859640268\n",
      "Epoch 28, Loss: 11.629552767826961\n",
      "Epoch 29, Loss: 12.271570279048039\n",
      "Epoch 30, Loss: 12.016087238605206\n",
      "Epoch 31, Loss: 10.069253206253052\n",
      "Epoch 32, Loss: 9.714286143963154\n",
      "Epoch 33, Loss: 9.61639925149771\n",
      "Epoch 34, Loss: 9.96545070868272\n",
      "Epoch 35, Loss: 9.687808018464308\n",
      "Epoch 36, Loss: 10.319242257338304\n",
      "Epoch 37, Loss: 9.53006674693181\n",
      "Epoch 38, Loss: 8.665132907720713\n",
      "Epoch 39, Loss: 8.108458500642042\n",
      "Epoch 40, Loss: 8.561582143490131\n",
      "Epoch 41, Loss: 8.559479493361254\n",
      "Epoch 42, Loss: 8.956895443109365\n",
      "Epoch 43, Loss: 8.876209955949049\n",
      "Epoch 44, Loss: 10.353516853772676\n",
      "Epoch 45, Loss: 9.944794508127066\n",
      "Epoch 46, Loss: 10.06374247257526\n",
      "Epoch 47, Loss: 8.837626182115995\n",
      "Epoch 48, Loss: 8.052908163804274\n",
      "Epoch 49, Loss: 7.978448721078726\n",
      "Epoch 50, Loss: 8.79944948049692\n",
      "Epoch 51, Loss: 8.559650787940392\n",
      "Epoch 52, Loss: 9.742525192407461\n",
      "Epoch 53, Loss: 12.77463463636545\n",
      "Epoch 54, Loss: 15.024417015222403\n",
      "Epoch 55, Loss: 11.592860331902138\n",
      "Epoch 56, Loss: 9.54540830392104\n",
      "Epoch 57, Loss: 9.665789695886465\n",
      "Epoch 58, Loss: 10.110441262905415\n",
      "Epoch 59, Loss: 9.194032467328585\n",
      "Epoch 60, Loss: 8.453507918577928\n",
      "Epoch 61, Loss: 8.65074216402494\n",
      "Epoch 62, Loss: 8.247223964104286\n",
      "Epoch 63, Loss: 7.971079349517822\n",
      "Epoch 64, Loss: 7.6094231972327595\n",
      "Epoch 65, Loss: 6.965887839977558\n",
      "Epoch 66, Loss: 6.709000935921302\n",
      "Epoch 67, Loss: 6.703252608959492\n",
      "Epoch 68, Loss: 6.663658087070171\n",
      "Epoch 69, Loss: 6.519739792897151\n",
      "Epoch 70, Loss: 6.404082298278809\n",
      "Epoch 71, Loss: 6.599614180051363\n",
      "Epoch 72, Loss: 6.691177698282095\n",
      "Epoch 73, Loss: 6.844057945104746\n",
      "Epoch 74, Loss: 7.404163250556359\n",
      "Epoch 75, Loss: 8.041843744424673\n",
      "Epoch 76, Loss: 9.395128818658682\n",
      "Epoch 77, Loss: 9.98956194290748\n",
      "Epoch 78, Loss: 8.81905750127939\n",
      "Epoch 79, Loss: 9.590021903698261\n",
      "Epoch 80, Loss: 8.894828704687265\n",
      "Epoch 81, Loss: 8.310399770736694\n",
      "Epoch 82, Loss: 9.269459632726816\n",
      "Epoch 83, Loss: 8.23466733785776\n",
      "Epoch 84, Loss: 7.365682620268601\n",
      "Epoch 85, Loss: 7.086440416482779\n",
      "Epoch 86, Loss: 6.7422572282644415\n",
      "Epoch 87, Loss: 6.871466031441321\n",
      "Epoch 88, Loss: 6.802399176817674\n",
      "Epoch 89, Loss: 6.868142953285804\n",
      "Epoch 90, Loss: 7.032834951694195\n",
      "Epoch 91, Loss: 6.941188408778264\n",
      "Epoch 92, Loss: 7.102594815767729\n",
      "Epoch 93, Loss: 7.097941416960496\n",
      "Epoch 94, Loss: 7.132764156048115\n",
      "Epoch 95, Loss: 7.676555780264048\n",
      "Epoch 96, Loss: 8.43602651816148\n",
      "Epoch 97, Loss: 9.253311469004704\n",
      "Epoch 98, Loss: 10.468183920933651\n",
      "Epoch 99, Loss: 11.800154997752262\n",
      "Epoch 100, Loss: 12.85436410170335\n",
      "Epoch 101, Loss: 10.277570761167086\n",
      "Epoch 102, Loss: 8.294952447597797\n",
      "Epoch 103, Loss: 7.356611050092257\n",
      "Epoch 104, Loss: 7.5635436131404\n",
      "Epoch 105, Loss: 8.450545145915104\n",
      "Epoch 106, Loss: 7.524237412672776\n",
      "Epoch 107, Loss: 7.029302982183603\n",
      "Epoch 108, Loss: 6.583066701889038\n",
      "Epoch 109, Loss: 7.059453450716459\n",
      "Epoch 110, Loss: 6.909528713959914\n",
      "Epoch 111, Loss: 7.758158427018386\n",
      "Epoch 112, Loss: 7.980208818729107\n",
      "Epoch 113, Loss: 8.602367309423594\n",
      "Epoch 114, Loss: 7.815934859789335\n",
      "Epoch 115, Loss: 7.15985133097722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:45,011] Trial 62 finished with value: 24.11797023383824 and parameters: {'latent_dim_z1': 17, 'latent_dim_z2': 66, 'hidden_dim': 106, 'epochs': 117, 'causal_reg': 0.18217695807650022, 'learning_rate': 0.001189583058758892}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116, Loss: 7.196576338547927\n",
      "Epoch 117, Loss: 7.425913682350745\n",
      "Epoch 1, Loss: 502.820678124061\n",
      "Epoch 2, Loss: 240.57260395930365\n",
      "Epoch 3, Loss: 162.49661944462702\n",
      "Epoch 4, Loss: 134.4779340303861\n",
      "Epoch 5, Loss: 114.50139764639047\n",
      "Epoch 6, Loss: 103.98948999551627\n",
      "Epoch 7, Loss: 95.45378450246957\n",
      "Epoch 8, Loss: 90.42119657076321\n",
      "Epoch 9, Loss: 77.25720200171837\n",
      "Epoch 10, Loss: 70.70755092914288\n",
      "Epoch 11, Loss: 65.16065230736366\n",
      "Epoch 12, Loss: 59.86602020263672\n",
      "Epoch 13, Loss: 57.44926456304697\n",
      "Epoch 14, Loss: 55.195792748377876\n",
      "Epoch 15, Loss: 50.76592848851131\n",
      "Epoch 16, Loss: 42.44273237081674\n",
      "Epoch 17, Loss: 36.94355080677913\n",
      "Epoch 18, Loss: 33.617433034456695\n",
      "Epoch 19, Loss: 30.026586642632118\n",
      "Epoch 20, Loss: 28.165802808908317\n",
      "Epoch 21, Loss: 26.697025372431828\n",
      "Epoch 22, Loss: 24.688750523787277\n",
      "Epoch 23, Loss: 23.17262810927171\n",
      "Epoch 24, Loss: 22.393343925476074\n",
      "Epoch 25, Loss: 19.540805669931267\n",
      "Epoch 26, Loss: 16.579327711692223\n",
      "Epoch 27, Loss: 15.166090415074276\n",
      "Epoch 28, Loss: 14.744994291892418\n",
      "Epoch 29, Loss: 13.376437113835262\n",
      "Epoch 30, Loss: 12.787185815664438\n",
      "Epoch 31, Loss: 12.394811116732084\n",
      "Epoch 32, Loss: 11.643837782052847\n",
      "Epoch 33, Loss: 10.840880100543682\n",
      "Epoch 34, Loss: 9.965564691103422\n",
      "Epoch 35, Loss: 9.638982424369225\n",
      "Epoch 36, Loss: 9.147040623884935\n",
      "Epoch 37, Loss: 8.849971129344059\n",
      "Epoch 38, Loss: 8.589454155701857\n",
      "Epoch 39, Loss: 8.102239168607271\n",
      "Epoch 40, Loss: 8.188806680532602\n",
      "Epoch 41, Loss: 7.8639101431919975\n",
      "Epoch 42, Loss: 7.559171071419349\n",
      "Epoch 43, Loss: 7.406199125143198\n",
      "Epoch 44, Loss: 7.187434031413152\n",
      "Epoch 45, Loss: 6.9444283338693475\n",
      "Epoch 46, Loss: 6.837807380236113\n",
      "Epoch 47, Loss: 6.801146378883948\n",
      "Epoch 48, Loss: 6.795558195847732\n",
      "Epoch 49, Loss: 6.6927008628845215\n",
      "Epoch 50, Loss: 6.590348060314472\n",
      "Epoch 51, Loss: 6.5359856348771315\n",
      "Epoch 52, Loss: 6.469179905377901\n",
      "Epoch 53, Loss: 6.617231955895057\n",
      "Epoch 54, Loss: 6.586923177425678\n",
      "Epoch 55, Loss: 6.55340341421274\n",
      "Epoch 56, Loss: 6.916168781427237\n",
      "Epoch 57, Loss: 7.1264911614931545\n",
      "Epoch 58, Loss: 6.906114816665649\n",
      "Epoch 59, Loss: 6.519814087794377\n",
      "Epoch 60, Loss: 7.252899628419143\n",
      "Epoch 61, Loss: 7.40080714225769\n",
      "Epoch 62, Loss: 7.865396187855647\n",
      "Epoch 63, Loss: 8.949801463347216\n",
      "Epoch 64, Loss: 9.723010595028217\n",
      "Epoch 65, Loss: 9.23610806465149\n",
      "Epoch 66, Loss: 8.225265081112202\n",
      "Epoch 67, Loss: 7.391391130594107\n",
      "Epoch 68, Loss: 7.466290363898644\n",
      "Epoch 69, Loss: 7.172593483558068\n",
      "Epoch 70, Loss: 7.311318030724158\n",
      "Epoch 71, Loss: 7.340260872474084\n",
      "Epoch 72, Loss: 7.200908587529109\n",
      "Epoch 73, Loss: 6.758376103181106\n",
      "Epoch 74, Loss: 6.663439879050622\n",
      "Epoch 75, Loss: 6.5909986312572775\n",
      "Epoch 76, Loss: 6.561205497154822\n",
      "Epoch 77, Loss: 6.350481051665086\n",
      "Epoch 78, Loss: 6.46512743142935\n",
      "Epoch 79, Loss: 6.862415918937097\n",
      "Epoch 80, Loss: 6.887222069960374\n",
      "Epoch 81, Loss: 6.673115436847393\n",
      "Epoch 82, Loss: 6.423777506901668\n",
      "Epoch 83, Loss: 6.280303588280311\n",
      "Epoch 84, Loss: 6.51279878616333\n",
      "Epoch 85, Loss: 6.825284059231098\n",
      "Epoch 86, Loss: 6.828862373645489\n",
      "Epoch 87, Loss: 7.108632509525005\n",
      "Epoch 88, Loss: 8.160154305971586\n",
      "Epoch 89, Loss: 8.885653495788574\n",
      "Epoch 90, Loss: 8.627539854783278\n",
      "Epoch 91, Loss: 8.267844071755043\n",
      "Epoch 92, Loss: 7.5920359538151665\n",
      "Epoch 93, Loss: 7.761227259269128\n",
      "Epoch 94, Loss: 8.175447629048275\n",
      "Epoch 95, Loss: 8.757485187970675\n",
      "Epoch 96, Loss: 8.330477842917809\n",
      "Epoch 97, Loss: 8.057596610142635\n",
      "Epoch 98, Loss: 7.817049961823684\n",
      "Epoch 99, Loss: 7.7080652346977825\n",
      "Epoch 100, Loss: 8.81962303014902\n",
      "Epoch 101, Loss: 7.931175213593703\n",
      "Epoch 102, Loss: 7.8993092867044306\n",
      "Epoch 103, Loss: 8.670132838762724\n",
      "Epoch 104, Loss: 8.50597568658682\n",
      "Epoch 105, Loss: 7.327896925119253\n",
      "Epoch 106, Loss: 7.8906411207639255\n",
      "Epoch 107, Loss: 7.7380198148580694\n",
      "Epoch 108, Loss: 7.768457394379836\n",
      "Epoch 109, Loss: 6.969945834233211\n",
      "Epoch 110, Loss: 6.65770017183744\n",
      "Epoch 111, Loss: 6.5196913205660305\n",
      "Epoch 112, Loss: 6.3673741817474365\n",
      "Epoch 113, Loss: 6.125695760433491\n",
      "Epoch 114, Loss: 6.056589658443745\n",
      "Epoch 115, Loss: 5.9969858939831076\n",
      "Epoch 116, Loss: 6.07585307267996\n",
      "Epoch 117, Loss: 6.066489769862248\n",
      "Epoch 118, Loss: 6.030606434895442\n",
      "Epoch 119, Loss: 5.929117019359882\n",
      "Epoch 120, Loss: 5.889030419863188\n",
      "Epoch 121, Loss: 5.893759140601525\n",
      "Epoch 122, Loss: 5.874454186512874\n",
      "Epoch 123, Loss: 5.973951559800368\n",
      "Epoch 124, Loss: 6.0327187684866095\n",
      "Epoch 125, Loss: 6.008300909629235\n",
      "Epoch 126, Loss: 6.04534493959867\n",
      "Epoch 127, Loss: 6.1319828400245076\n",
      "Epoch 128, Loss: 6.2125408832843485\n",
      "Epoch 129, Loss: 6.411917778161856\n",
      "Epoch 130, Loss: 6.265841594109168\n",
      "Epoch 131, Loss: 6.2619222310873175\n",
      "Epoch 132, Loss: 6.628241117183979\n",
      "Epoch 133, Loss: 6.495439217640803\n",
      "Epoch 134, Loss: 6.672428351182204\n",
      "Epoch 135, Loss: 6.86487550001878\n",
      "Epoch 136, Loss: 7.3426415003263035\n",
      "Epoch 137, Loss: 7.179442699138935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:48,061] Trial 63 finished with value: 21.69718733668913 and parameters: {'latent_dim_z1': 13, 'latent_dim_z2': 72, 'hidden_dim': 56, 'epochs': 141, 'causal_reg': 0.12211607358491022, 'learning_rate': 0.0007048487779331476}. Best is trial 16 with value: 18.69924799078573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138, Loss: 7.912034749984741\n",
      "Epoch 139, Loss: 9.314570958797749\n",
      "Epoch 140, Loss: 8.424009121381319\n",
      "Epoch 141, Loss: 7.901602176519541\n",
      "Epoch 1, Loss: 392.2218043987568\n",
      "Epoch 2, Loss: 143.10542488098145\n",
      "Epoch 3, Loss: 106.92106114901028\n",
      "Epoch 4, Loss: 87.17196259131798\n",
      "Epoch 5, Loss: 79.01627745995155\n",
      "Epoch 6, Loss: 69.91616307772122\n",
      "Epoch 7, Loss: 60.9752297768226\n",
      "Epoch 8, Loss: 54.31410620762752\n",
      "Epoch 9, Loss: 55.841371022737945\n",
      "Epoch 10, Loss: 47.60053341205303\n",
      "Epoch 11, Loss: 43.22371585552509\n",
      "Epoch 12, Loss: 38.664996073796196\n",
      "Epoch 13, Loss: 38.26625435168926\n",
      "Epoch 14, Loss: 41.904960302206185\n",
      "Epoch 15, Loss: 30.50758567223182\n",
      "Epoch 16, Loss: 37.451723685631386\n",
      "Epoch 17, Loss: 36.397308349609375\n",
      "Epoch 18, Loss: 29.625126838684082\n",
      "Epoch 19, Loss: 26.864477451031025\n",
      "Epoch 20, Loss: 25.22325633122371\n",
      "Epoch 21, Loss: 25.324116156651424\n",
      "Epoch 22, Loss: 24.205711988302376\n",
      "Epoch 23, Loss: 24.867199127490704\n",
      "Epoch 24, Loss: 21.282776575822098\n",
      "Epoch 25, Loss: 20.278277984032265\n",
      "Epoch 26, Loss: 18.997762276576115\n",
      "Epoch 27, Loss: 22.165607342353233\n",
      "Epoch 28, Loss: 17.11971433346088\n",
      "Epoch 29, Loss: 18.086243299337532\n",
      "Epoch 30, Loss: 16.500861131227932\n",
      "Epoch 31, Loss: 15.417706764661348\n",
      "Epoch 32, Loss: 15.383183772747334\n",
      "Epoch 33, Loss: 17.205986903263973\n",
      "Epoch 34, Loss: 16.15235236974863\n",
      "Epoch 35, Loss: 14.932868810800406\n",
      "Epoch 36, Loss: 13.35938783792349\n",
      "Epoch 37, Loss: 15.576742318960337\n",
      "Epoch 38, Loss: 16.703508432094868\n",
      "Epoch 39, Loss: 14.367928101466251\n",
      "Epoch 40, Loss: 12.447984622075008\n",
      "Epoch 41, Loss: 12.25786172426664\n",
      "Epoch 42, Loss: 13.53303912969736\n",
      "Epoch 43, Loss: 14.479264075939472\n",
      "Epoch 44, Loss: 12.859394660362831\n",
      "Epoch 45, Loss: 11.68679721538837\n",
      "Epoch 46, Loss: 11.7230839912708\n",
      "Epoch 47, Loss: 14.514067979959341\n",
      "Epoch 48, Loss: 12.263196248274584\n",
      "Epoch 49, Loss: 12.98838133078355\n",
      "Epoch 50, Loss: 13.1680277127486\n",
      "Epoch 51, Loss: 15.60332518357497\n",
      "Epoch 52, Loss: 12.895147653726431\n",
      "Epoch 53, Loss: 16.072596660027138\n",
      "Epoch 54, Loss: 13.759486675262451\n",
      "Epoch 55, Loss: 11.530801552992601\n",
      "Epoch 56, Loss: 10.835788598427406\n",
      "Epoch 57, Loss: 11.541219472885132\n",
      "Epoch 58, Loss: 9.613745909470778\n",
      "Epoch 59, Loss: 9.264073408566988\n",
      "Epoch 60, Loss: 9.28406286239624\n",
      "Epoch 61, Loss: 9.986026415458092\n",
      "Epoch 62, Loss: 10.434081554412842\n",
      "Epoch 63, Loss: 10.018153263972355\n",
      "Epoch 64, Loss: 11.796373165570772\n",
      "Epoch 65, Loss: 10.164373214428242\n",
      "Epoch 66, Loss: 9.267567286124596\n",
      "Epoch 67, Loss: 8.76005876981295\n",
      "Epoch 68, Loss: 10.74617180457482\n",
      "Epoch 69, Loss: 9.788109907737145\n",
      "Epoch 70, Loss: 9.890895898525532\n",
      "Epoch 71, Loss: 8.846589546937208\n",
      "Epoch 72, Loss: 9.21683669090271\n",
      "Epoch 73, Loss: 8.093701967826256\n",
      "Epoch 74, Loss: 7.795235065313486\n",
      "Epoch 75, Loss: 7.866773972144494\n",
      "Epoch 76, Loss: 8.565625557532677\n",
      "Epoch 77, Loss: 7.86610850921044\n",
      "Epoch 78, Loss: 7.998548012513381\n",
      "Epoch 79, Loss: 8.147393630101131\n",
      "Epoch 80, Loss: 9.405311841231127\n",
      "Epoch 81, Loss: 9.240831741919884\n",
      "Epoch 82, Loss: 9.858345306836641\n",
      "Epoch 83, Loss: 9.144832005867592\n",
      "Epoch 84, Loss: 8.46486650980436\n",
      "Epoch 85, Loss: 9.912143523876484\n",
      "Epoch 86, Loss: 8.741004008513231\n",
      "Epoch 87, Loss: 8.226545077103834\n",
      "Epoch 88, Loss: 8.991453207456148\n",
      "Epoch 89, Loss: 9.509615733073307\n",
      "Epoch 90, Loss: 9.142425188651451\n",
      "Epoch 91, Loss: 10.594550994726328\n",
      "Epoch 92, Loss: 10.93436648295476\n",
      "Epoch 93, Loss: 12.78141162945674\n",
      "Epoch 94, Loss: 11.263010318462666\n",
      "Epoch 95, Loss: 12.720897344442514\n",
      "Epoch 96, Loss: 11.558127439939058\n",
      "Epoch 97, Loss: 12.520039301652174\n",
      "Epoch 98, Loss: 14.273529456211971\n",
      "Epoch 99, Loss: 10.359700294641348\n",
      "Epoch 100, Loss: 8.974490825946514\n",
      "Epoch 101, Loss: 8.102162342805128\n",
      "Epoch 102, Loss: 7.364819930149959\n",
      "Epoch 103, Loss: 7.981594489170955\n",
      "Epoch 104, Loss: 7.63445181113023\n",
      "Epoch 105, Loss: 7.04050403374892\n",
      "Epoch 106, Loss: 6.920577617791983\n",
      "Epoch 107, Loss: 6.884087984378521\n",
      "Epoch 108, Loss: 7.11353353353647\n",
      "Epoch 109, Loss: 7.242567374156072\n",
      "Epoch 110, Loss: 7.5332093972426195\n",
      "Epoch 111, Loss: 8.167135935563307\n",
      "Epoch 112, Loss: 9.755200202648457\n",
      "Epoch 113, Loss: 8.765081093861507\n",
      "Epoch 114, Loss: 9.277593209193302\n",
      "Epoch 115, Loss: 8.227724093657274\n",
      "Epoch 116, Loss: 8.546145145709698\n",
      "Epoch 117, Loss: 7.823390685594999\n",
      "Epoch 118, Loss: 7.717132091522217\n",
      "Epoch 119, Loss: 7.827293249276968\n",
      "Epoch 120, Loss: 7.298901172784658\n",
      "Epoch 121, Loss: 7.350282118870662\n",
      "Epoch 122, Loss: 7.11184417284452\n",
      "Epoch 123, Loss: 7.334862232208252\n",
      "Epoch 124, Loss: 7.9088323299701395\n",
      "Epoch 125, Loss: 9.41232628088731\n",
      "Epoch 126, Loss: 8.571935653686523\n",
      "Epoch 127, Loss: 8.926582629864033\n",
      "Epoch 128, Loss: 8.45897441643935\n",
      "Epoch 129, Loss: 8.326959463266226\n",
      "Epoch 130, Loss: 7.285163109119122\n",
      "Epoch 131, Loss: 8.1971875337454\n",
      "Epoch 132, Loss: 7.9894804404332085\n",
      "Epoch 133, Loss: 7.94918863589947\n",
      "Epoch 134, Loss: 7.234899154076209\n",
      "Epoch 135, Loss: 7.27145240857051\n",
      "Epoch 136, Loss: 7.4498853866870585\n",
      "Epoch 137, Loss: 8.895667956425594\n",
      "Epoch 138, Loss: 9.958232256082388\n",
      "Epoch 139, Loss: 15.084720758291391\n",
      "Epoch 140, Loss: 16.640838072850155\n",
      "Epoch 141, Loss: 12.126248176281269\n",
      "Epoch 142, Loss: 11.066792616477379\n",
      "Epoch 143, Loss: 10.074101209640503\n",
      "Epoch 144, Loss: 8.74652336193965\n",
      "Epoch 145, Loss: 8.743588704329271\n",
      "Epoch 146, Loss: 7.978657649113582\n",
      "Epoch 147, Loss: 7.4061055183410645\n",
      "Epoch 148, Loss: 9.182434210410484\n",
      "Epoch 149, Loss: 7.751185508874746\n",
      "Epoch 150, Loss: 7.885970262380747\n",
      "Epoch 151, Loss: 7.381228098502526\n",
      "Epoch 152, Loss: 6.843333519422091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:51,488] Trial 64 finished with value: 18.698836011176525 and parameters: {'latent_dim_z1': 13, 'latent_dim_z2': 76, 'hidden_dim': 92, 'epochs': 156, 'causal_reg': 0.2044219631246504, 'learning_rate': 0.002558398410046827}. Best is trial 64 with value: 18.698836011176525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153, Loss: 6.596078175764817\n",
      "Epoch 154, Loss: 6.735904730283297\n",
      "Epoch 155, Loss: 7.80598416695228\n",
      "Epoch 156, Loss: 8.783117881188026\n",
      "Epoch 1, Loss: 1270.7008338341345\n",
      "Epoch 2, Loss: 543.2857554509089\n",
      "Epoch 3, Loss: 419.3979280911959\n",
      "Epoch 4, Loss: 353.9200415978065\n",
      "Epoch 5, Loss: 305.9187874427208\n",
      "Epoch 6, Loss: 223.93566513061523\n",
      "Epoch 7, Loss: 190.0927288348858\n",
      "Epoch 8, Loss: 165.09537858229416\n",
      "Epoch 9, Loss: 147.32735633850098\n",
      "Epoch 10, Loss: 167.54515823951135\n",
      "Epoch 11, Loss: 155.17192781888522\n",
      "Epoch 12, Loss: 111.49590015411377\n",
      "Epoch 13, Loss: 92.24875802260179\n",
      "Epoch 14, Loss: 69.13519111046425\n",
      "Epoch 15, Loss: 67.82279916910025\n",
      "Epoch 16, Loss: 62.955400246840256\n",
      "Epoch 17, Loss: 66.14113257481502\n",
      "Epoch 18, Loss: 48.16859333331768\n",
      "Epoch 19, Loss: 48.64401479867789\n",
      "Epoch 20, Loss: 54.23226892031156\n",
      "Epoch 21, Loss: 60.707895095531754\n",
      "Epoch 22, Loss: 37.275716048020584\n",
      "Epoch 23, Loss: 38.46701273551354\n",
      "Epoch 24, Loss: 37.582327329195465\n",
      "Epoch 25, Loss: 34.587981297419624\n",
      "Epoch 26, Loss: 23.557401253626896\n",
      "Epoch 27, Loss: 25.826206610752987\n",
      "Epoch 28, Loss: 20.367924030010517\n",
      "Epoch 29, Loss: 22.351213675278885\n",
      "Epoch 30, Loss: 20.163298496833214\n",
      "Epoch 31, Loss: 17.05316741649921\n",
      "Epoch 32, Loss: 18.71478612606342\n",
      "Epoch 33, Loss: 16.08887591728797\n",
      "Epoch 34, Loss: 14.256076922783485\n",
      "Epoch 35, Loss: 16.900439977645874\n",
      "Epoch 36, Loss: 16.738130276019756\n",
      "Epoch 37, Loss: 15.19349035849938\n",
      "Epoch 38, Loss: 15.563870063194862\n",
      "Epoch 39, Loss: 16.514290993030254\n",
      "Epoch 40, Loss: 18.68384229219877\n",
      "Epoch 41, Loss: 19.29150885802049\n",
      "Epoch 42, Loss: 18.264197789705715\n",
      "Epoch 43, Loss: 19.531169121082012\n",
      "Epoch 44, Loss: 29.534143227797287\n",
      "Epoch 45, Loss: 28.893112182617188\n",
      "Epoch 46, Loss: 35.656783507420464\n",
      "Epoch 47, Loss: 30.627963689657356\n",
      "Epoch 48, Loss: 20.871929498819206\n",
      "Epoch 49, Loss: 18.766997759158794\n",
      "Epoch 50, Loss: 16.383175189678486\n",
      "Epoch 51, Loss: 15.089022746452919\n",
      "Epoch 52, Loss: 12.726234619434063\n",
      "Epoch 53, Loss: 14.039523344773512\n",
      "Epoch 54, Loss: 22.25866783582247\n",
      "Epoch 55, Loss: 21.6761202445397\n",
      "Epoch 56, Loss: 22.644927354959343\n",
      "Epoch 57, Loss: 18.89816320859469\n",
      "Epoch 58, Loss: 17.211141659663273\n",
      "Epoch 59, Loss: 16.58418794778677\n",
      "Epoch 60, Loss: 15.952342143425575\n",
      "Epoch 61, Loss: 15.63913840513963\n",
      "Epoch 62, Loss: 16.31685231282161\n",
      "Epoch 63, Loss: 27.905259682581974\n",
      "Epoch 64, Loss: 34.240406623253456\n",
      "Epoch 65, Loss: 24.228469885312595\n",
      "Epoch 66, Loss: 20.18604990152212\n",
      "Epoch 67, Loss: 20.78174767127404\n",
      "Epoch 68, Loss: 15.936297324987558\n",
      "Epoch 69, Loss: 12.841836012326754\n",
      "Epoch 70, Loss: 11.491179979764498\n",
      "Epoch 71, Loss: 12.295026247317974\n",
      "Epoch 72, Loss: 11.297812626912044\n",
      "Epoch 73, Loss: 12.58583166049077\n",
      "Epoch 74, Loss: 10.678599485984215\n",
      "Epoch 75, Loss: 10.153162827858559\n",
      "Epoch 76, Loss: 12.364071479210487\n",
      "Epoch 77, Loss: 11.840653309455284\n",
      "Epoch 78, Loss: 9.930683594483595\n",
      "Epoch 79, Loss: 10.195568011357235\n",
      "Epoch 80, Loss: 9.27418275979849\n",
      "Epoch 81, Loss: 8.236718159455519\n",
      "Epoch 82, Loss: 8.226150384316078\n",
      "Epoch 83, Loss: 8.590373039245605\n",
      "Epoch 84, Loss: 7.941124017422016\n",
      "Epoch 85, Loss: 8.752622494330772\n",
      "Epoch 86, Loss: 10.008916323001568\n",
      "Epoch 87, Loss: 11.987763258127066\n",
      "Epoch 88, Loss: 16.97793197631836\n",
      "Epoch 89, Loss: 19.654568085303673\n",
      "Epoch 90, Loss: 31.586121779221756\n",
      "Epoch 91, Loss: 25.99896739079402\n",
      "Epoch 92, Loss: 30.71723853624784\n",
      "Epoch 93, Loss: 29.513415336608887\n",
      "Epoch 94, Loss: 25.874010929694542\n",
      "Epoch 95, Loss: 19.31878823500413\n",
      "Epoch 96, Loss: 15.419677110818716\n",
      "Epoch 97, Loss: 15.157231477590708\n",
      "Epoch 98, Loss: 12.982172819284292\n",
      "Epoch 99, Loss: 15.170244968854464\n",
      "Epoch 100, Loss: 16.24449924322275\n",
      "Epoch 101, Loss: 16.45763811698327\n",
      "Epoch 102, Loss: 21.307702871469353\n",
      "Epoch 103, Loss: 21.259244441986084\n",
      "Epoch 104, Loss: 19.301806303171013\n",
      "Epoch 105, Loss: 19.57715137188251\n",
      "Epoch 106, Loss: 31.0016450881958\n",
      "Epoch 107, Loss: 22.694954982170692\n",
      "Epoch 108, Loss: 19.32049193749061\n",
      "Epoch 109, Loss: 12.368868406002338\n",
      "Epoch 110, Loss: 10.983671866930449\n",
      "Epoch 111, Loss: 10.144621317203228\n",
      "Epoch 112, Loss: 8.612545031767626\n",
      "Epoch 113, Loss: 8.491832036238451\n",
      "Epoch 114, Loss: 9.982753992080688\n",
      "Epoch 115, Loss: 9.342620042654184\n",
      "Epoch 116, Loss: 8.671474365087656\n",
      "Epoch 117, Loss: 8.650627741446861\n",
      "Epoch 118, Loss: 8.008732630656315\n",
      "Epoch 119, Loss: 7.816972402425913\n",
      "Epoch 120, Loss: 7.728892069596511\n",
      "Epoch 121, Loss: 7.958446631064782\n",
      "Epoch 122, Loss: 8.715598363142748\n",
      "Epoch 123, Loss: 8.454354451252865\n",
      "Epoch 124, Loss: 8.970918527016273\n",
      "Epoch 125, Loss: 8.531871227117685\n",
      "Epoch 126, Loss: 8.660169656460102\n",
      "Epoch 127, Loss: 8.770710138174204\n",
      "Epoch 128, Loss: 9.785031410364004\n",
      "Epoch 129, Loss: 13.233164163736197\n",
      "Epoch 130, Loss: 12.24103335233835\n",
      "Epoch 131, Loss: 15.894059529671303\n",
      "Epoch 132, Loss: 28.479748469132645\n",
      "Epoch 133, Loss: 46.2792288340055\n",
      "Epoch 134, Loss: 46.1782149534959\n",
      "Epoch 135, Loss: 35.996270179748535\n",
      "Epoch 136, Loss: 26.21454088504498\n",
      "Epoch 137, Loss: 27.059447765350342\n",
      "Epoch 138, Loss: 24.2552089874561\n",
      "Epoch 139, Loss: 17.580858670748196\n",
      "Epoch 140, Loss: 19.017287951249344\n",
      "Epoch 141, Loss: 18.763055434593788\n",
      "Epoch 142, Loss: 14.402614721885094\n",
      "Epoch 143, Loss: 15.453307261833778\n",
      "Epoch 144, Loss: 13.319854296170748\n",
      "Epoch 145, Loss: 14.172234810315645\n",
      "Epoch 146, Loss: 13.516105706875141\n",
      "Epoch 147, Loss: 13.062117723318247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:55,476] Trial 65 finished with value: 44.902062971211045 and parameters: {'latent_dim_z1': 61, 'latent_dim_z2': 80, 'hidden_dim': 92, 'epochs': 156, 'causal_reg': 0.19928022784599608, 'learning_rate': 0.0028847139909003615}. Best is trial 64 with value: 18.698836011176525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148, Loss: 11.857272735008827\n",
      "Epoch 149, Loss: 11.190626584566557\n",
      "Epoch 150, Loss: 14.546261145518375\n",
      "Epoch 151, Loss: 15.450588116279015\n",
      "Epoch 152, Loss: 14.937640300163856\n",
      "Epoch 153, Loss: 14.32436675291795\n",
      "Epoch 154, Loss: 14.800180215101976\n",
      "Epoch 155, Loss: 14.63414925795335\n",
      "Epoch 156, Loss: 13.018793381177462\n",
      "Epoch 1, Loss: 535.8841450030988\n",
      "Epoch 2, Loss: 221.30619342510516\n",
      "Epoch 3, Loss: 172.4402768061711\n",
      "Epoch 4, Loss: 142.7103501833402\n",
      "Epoch 5, Loss: 118.77900358346793\n",
      "Epoch 6, Loss: 104.642210153433\n",
      "Epoch 7, Loss: 96.95710989145132\n",
      "Epoch 8, Loss: 124.30590028029222\n",
      "Epoch 9, Loss: 87.693512403048\n",
      "Epoch 10, Loss: 76.98479857811562\n",
      "Epoch 11, Loss: 64.91415962806114\n",
      "Epoch 12, Loss: 59.10428061852088\n",
      "Epoch 13, Loss: 54.67946822826679\n",
      "Epoch 14, Loss: 50.09162748776949\n",
      "Epoch 15, Loss: 45.25959638448862\n",
      "Epoch 16, Loss: 44.54419312110314\n",
      "Epoch 17, Loss: 38.153216948876015\n",
      "Epoch 18, Loss: 41.930077406076286\n",
      "Epoch 19, Loss: 35.763625401716965\n",
      "Epoch 20, Loss: 44.434786136333756\n",
      "Epoch 21, Loss: 56.37489322515634\n",
      "Epoch 22, Loss: 36.04435377854567\n",
      "Epoch 23, Loss: 34.0872457577632\n",
      "Epoch 24, Loss: 31.74776392716628\n",
      "Epoch 25, Loss: 27.713042956132156\n",
      "Epoch 26, Loss: 25.6010270852309\n",
      "Epoch 27, Loss: 26.43705074603741\n",
      "Epoch 28, Loss: 26.3665818801293\n",
      "Epoch 29, Loss: 20.150719605959377\n",
      "Epoch 30, Loss: 19.40820105259235\n",
      "Epoch 31, Loss: 19.480134963989258\n",
      "Epoch 32, Loss: 25.934715454395\n",
      "Epoch 33, Loss: 38.685786760770355\n",
      "Epoch 34, Loss: 29.656513617588924\n",
      "Epoch 35, Loss: 44.57334767855131\n",
      "Epoch 36, Loss: 25.83329043021569\n",
      "Epoch 37, Loss: 18.423233949221096\n",
      "Epoch 38, Loss: 15.660844509418194\n",
      "Epoch 39, Loss: 17.161289618565487\n",
      "Epoch 40, Loss: 15.74619540801415\n",
      "Epoch 41, Loss: 15.27139032804049\n",
      "Epoch 42, Loss: 23.89113294161283\n",
      "Epoch 43, Loss: 21.45213350882897\n",
      "Epoch 44, Loss: 15.311197134164663\n",
      "Epoch 45, Loss: 14.248297746364887\n",
      "Epoch 46, Loss: 16.648200255173904\n",
      "Epoch 47, Loss: 14.800403044773983\n",
      "Epoch 48, Loss: 13.321003547081581\n",
      "Epoch 49, Loss: 17.366065832284782\n",
      "Epoch 50, Loss: 13.70319073016827\n",
      "Epoch 51, Loss: 12.189327753507174\n",
      "Epoch 52, Loss: 22.2982800923861\n",
      "Epoch 53, Loss: 19.150194241450382\n",
      "Epoch 54, Loss: 21.139093765845665\n",
      "Epoch 55, Loss: 17.728183086101826\n",
      "Epoch 56, Loss: 16.169825333815353\n",
      "Epoch 57, Loss: 17.223419189453125\n",
      "Epoch 58, Loss: 15.401411019838774\n",
      "Epoch 59, Loss: 12.742564568152794\n",
      "Epoch 60, Loss: 14.223605486062857\n",
      "Epoch 61, Loss: 15.66306394797105\n",
      "Epoch 62, Loss: 12.096450108748217\n",
      "Epoch 63, Loss: 10.389636865028969\n",
      "Epoch 64, Loss: 10.389183081113375\n",
      "Epoch 65, Loss: 11.577701055086576\n",
      "Epoch 66, Loss: 10.038053292494554\n",
      "Epoch 67, Loss: 9.375071360514713\n",
      "Epoch 68, Loss: 11.343357031161968\n",
      "Epoch 69, Loss: 16.408793504421528\n",
      "Epoch 70, Loss: 17.175662957704983\n",
      "Epoch 71, Loss: 15.939556506963877\n",
      "Epoch 72, Loss: 16.35508185166579\n",
      "Epoch 73, Loss: 12.723456125992994\n",
      "Epoch 74, Loss: 10.73450598349938\n",
      "Epoch 75, Loss: 9.77738305238577\n",
      "Epoch 76, Loss: 8.631389727959267\n",
      "Epoch 77, Loss: 8.740216915424053\n",
      "Epoch 78, Loss: 8.716251648389376\n",
      "Epoch 79, Loss: 9.129295092362623\n",
      "Epoch 80, Loss: 9.22408281839811\n",
      "Epoch 81, Loss: 8.49852673824017\n",
      "Epoch 82, Loss: 8.72932375394381\n",
      "Epoch 83, Loss: 10.542992078340971\n",
      "Epoch 84, Loss: 12.312225011678843\n",
      "Epoch 85, Loss: 14.512309826337374\n",
      "Epoch 86, Loss: 29.520132835094746\n",
      "Epoch 87, Loss: 21.919793128967285\n",
      "Epoch 88, Loss: 17.714930626062248\n",
      "Epoch 89, Loss: 12.528680287874662\n",
      "Epoch 90, Loss: 10.60538282761207\n",
      "Epoch 91, Loss: 10.361844264543974\n",
      "Epoch 92, Loss: 10.35011150286748\n",
      "Epoch 93, Loss: 10.651679057341356\n",
      "Epoch 94, Loss: 12.748834609985352\n",
      "Epoch 95, Loss: 10.756798047285814\n",
      "Epoch 96, Loss: 10.343373151925894\n",
      "Epoch 97, Loss: 12.06260031920213\n",
      "Epoch 98, Loss: 19.426657053140495\n",
      "Epoch 99, Loss: 22.219944146963265\n",
      "Epoch 100, Loss: 15.773384350996752\n",
      "Epoch 101, Loss: 13.167361699617826\n",
      "Epoch 102, Loss: 9.652963638305664\n",
      "Epoch 103, Loss: 8.416719876802885\n",
      "Epoch 104, Loss: 9.212047705283531\n",
      "Epoch 105, Loss: 10.74397690479572\n",
      "Epoch 106, Loss: 12.531625912739681\n",
      "Epoch 107, Loss: 11.164942484635572\n",
      "Epoch 108, Loss: 9.776711610647348\n",
      "Epoch 109, Loss: 9.007823577293983\n",
      "Epoch 110, Loss: 7.88513680604788\n",
      "Epoch 111, Loss: 7.258337424351619\n",
      "Epoch 112, Loss: 6.863976588616004\n",
      "Epoch 113, Loss: 6.9958441991072435\n",
      "Epoch 114, Loss: 7.403494743200449\n",
      "Epoch 115, Loss: 7.27948887531574\n",
      "Epoch 116, Loss: 8.612174914433407\n",
      "Epoch 117, Loss: 8.66385478239793\n",
      "Epoch 118, Loss: 10.211888221594004\n",
      "Epoch 119, Loss: 13.188422991679264\n",
      "Epoch 120, Loss: 16.398586126474235\n",
      "Epoch 121, Loss: 16.26705426436204\n",
      "Epoch 122, Loss: 13.984199670644907\n",
      "Epoch 123, Loss: 10.172673812279335\n",
      "Epoch 124, Loss: 9.238662958145142\n",
      "Epoch 125, Loss: 9.455425629248985\n",
      "Epoch 126, Loss: 13.176743177267221\n",
      "Epoch 127, Loss: 11.355537744668814\n",
      "Epoch 128, Loss: 10.77860846886268\n",
      "Epoch 129, Loss: 18.497437935609085\n",
      "Epoch 130, Loss: 13.928767442703247\n",
      "Epoch 131, Loss: 12.755301823982826\n",
      "Epoch 132, Loss: 9.555910422251774\n",
      "Epoch 133, Loss: 8.459866597102238\n",
      "Epoch 134, Loss: 7.729071011910071\n",
      "Epoch 135, Loss: 7.774514253322895\n",
      "Epoch 136, Loss: 7.632738975378183\n",
      "Epoch 137, Loss: 8.606528905721811\n",
      "Epoch 138, Loss: 8.763190801327045\n",
      "Epoch 139, Loss: 11.3734138745528\n",
      "Epoch 140, Loss: 13.459857812294594\n",
      "Epoch 141, Loss: 12.521834978690514\n",
      "Epoch 142, Loss: 10.609727712777945\n",
      "Epoch 143, Loss: 10.181266692968515\n",
      "Epoch 144, Loss: 8.82708925467271\n",
      "Epoch 145, Loss: 7.898497141324556\n",
      "Epoch 146, Loss: 8.428997461612408\n",
      "Epoch 147, Loss: 8.748635255373442\n",
      "Epoch 148, Loss: 9.452603853665865\n",
      "Epoch 149, Loss: 12.041058246905987\n",
      "Epoch 150, Loss: 13.377762372677143\n",
      "Epoch 151, Loss: 12.08900473668025\n",
      "Epoch 152, Loss: 9.748684369600737\n",
      "Epoch 153, Loss: 8.48312706213731\n",
      "Epoch 154, Loss: 8.594562878975502\n",
      "Epoch 155, Loss: 8.251354144169735\n",
      "Epoch 156, Loss: 8.46316724557143\n",
      "Epoch 157, Loss: 8.690373879212599\n",
      "Epoch 158, Loss: 8.959406944421621\n",
      "Epoch 159, Loss: 8.256413789895864\n",
      "Epoch 160, Loss: 8.12753178523137\n",
      "Epoch 161, Loss: 9.223555656579824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:52:59,472] Trial 66 finished with value: 25.846167239787277 and parameters: {'latent_dim_z1': 24, 'latent_dim_z2': 76, 'hidden_dim': 137, 'epochs': 163, 'causal_reg': 0.3077502752908593, 'learning_rate': 0.00478712605855294}. Best is trial 64 with value: 18.698836011176525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162, Loss: 10.966911040819609\n",
      "Epoch 163, Loss: 17.63140869140625\n",
      "Epoch 1, Loss: 357.3743623586801\n",
      "Epoch 2, Loss: 133.75519913893478\n",
      "Epoch 3, Loss: 98.2146735558143\n",
      "Epoch 4, Loss: 85.40341670696552\n",
      "Epoch 5, Loss: 69.16055466578557\n",
      "Epoch 6, Loss: 62.59375997690054\n",
      "Epoch 7, Loss: 56.29195983593281\n",
      "Epoch 8, Loss: 57.653190245995155\n",
      "Epoch 9, Loss: 46.42491153570322\n",
      "Epoch 10, Loss: 39.70914605947641\n",
      "Epoch 11, Loss: 39.57990756401649\n",
      "Epoch 12, Loss: 38.20895943274865\n",
      "Epoch 13, Loss: 37.37786729519184\n",
      "Epoch 14, Loss: 35.5736949627216\n",
      "Epoch 15, Loss: 33.63788744119498\n",
      "Epoch 16, Loss: 27.836019809429462\n",
      "Epoch 17, Loss: 25.642149925231934\n",
      "Epoch 18, Loss: 21.89255534685575\n",
      "Epoch 19, Loss: 24.790980559128982\n",
      "Epoch 20, Loss: 25.34871193078848\n",
      "Epoch 21, Loss: 27.3369921537546\n",
      "Epoch 22, Loss: 22.348221118633564\n",
      "Epoch 23, Loss: 20.927549252143272\n",
      "Epoch 24, Loss: 18.685864576926598\n",
      "Epoch 25, Loss: 15.796676525702843\n",
      "Epoch 26, Loss: 13.781656742095947\n",
      "Epoch 27, Loss: 13.717886814704308\n",
      "Epoch 28, Loss: 14.19095985706036\n",
      "Epoch 29, Loss: 14.043309890306913\n",
      "Epoch 30, Loss: 12.353735226851244\n",
      "Epoch 31, Loss: 12.609208767230694\n",
      "Epoch 32, Loss: 11.68637123474708\n",
      "Epoch 33, Loss: 11.749137438260592\n",
      "Epoch 34, Loss: 16.72487365282499\n",
      "Epoch 35, Loss: 11.531567866985615\n",
      "Epoch 36, Loss: 10.47891301375169\n",
      "Epoch 37, Loss: 9.067346517856304\n",
      "Epoch 38, Loss: 9.54572578576895\n",
      "Epoch 39, Loss: 10.823302324001606\n",
      "Epoch 40, Loss: 9.285118048007671\n",
      "Epoch 41, Loss: 7.68413442831773\n",
      "Epoch 42, Loss: 7.929393181434045\n",
      "Epoch 43, Loss: 7.634471434813279\n",
      "Epoch 44, Loss: 7.600010468409612\n",
      "Epoch 45, Loss: 7.174680599799523\n",
      "Epoch 46, Loss: 7.124533873337966\n",
      "Epoch 47, Loss: 7.4001976526700535\n",
      "Epoch 48, Loss: 7.295972255560068\n",
      "Epoch 49, Loss: 8.072405998523418\n",
      "Epoch 50, Loss: 7.800291263140165\n",
      "Epoch 51, Loss: 8.331049644030058\n",
      "Epoch 52, Loss: 8.817602671109713\n",
      "Epoch 53, Loss: 8.905764946570763\n",
      "Epoch 54, Loss: 7.856477205569927\n",
      "Epoch 55, Loss: 8.36219178713285\n",
      "Epoch 56, Loss: 9.219190945992104\n",
      "Epoch 57, Loss: 9.869756313470694\n",
      "Epoch 58, Loss: 9.101914974359365\n",
      "Epoch 59, Loss: 9.700821326329159\n",
      "Epoch 60, Loss: 10.461153030395508\n",
      "Epoch 61, Loss: 8.86041155228248\n",
      "Epoch 62, Loss: 7.960661576344417\n",
      "Epoch 63, Loss: 7.141938337912927\n",
      "Epoch 64, Loss: 6.9147189763876105\n",
      "Epoch 65, Loss: 6.344760638016921\n",
      "Epoch 66, Loss: 6.228228899148794\n",
      "Epoch 67, Loss: 6.2391746044158936\n",
      "Epoch 68, Loss: 6.411565817319429\n",
      "Epoch 69, Loss: 6.189337528668917\n",
      "Epoch 70, Loss: 5.978126525878906\n",
      "Epoch 71, Loss: 5.821933342860295\n",
      "Epoch 72, Loss: 5.976525141642644\n",
      "Epoch 73, Loss: 6.168463872029231\n",
      "Epoch 74, Loss: 6.759489481265728\n",
      "Epoch 75, Loss: 7.518327107796302\n",
      "Epoch 76, Loss: 9.240582392765926\n",
      "Epoch 77, Loss: 8.652518272399902\n",
      "Epoch 78, Loss: 8.309169145730825\n",
      "Epoch 79, Loss: 7.833011517157922\n",
      "Epoch 80, Loss: 6.727726514522846\n",
      "Epoch 81, Loss: 7.1421736020308275\n",
      "Epoch 82, Loss: 7.194962079708393\n",
      "Epoch 83, Loss: 7.862955258442805\n",
      "Epoch 84, Loss: 7.681470815952007\n",
      "Epoch 85, Loss: 6.9872843485612135\n",
      "Epoch 86, Loss: 7.777875625170195\n",
      "Epoch 87, Loss: 9.356719805644108\n",
      "Epoch 88, Loss: 8.467306027045616\n",
      "Epoch 89, Loss: 7.742776485589834\n",
      "Epoch 90, Loss: 7.296800136566162\n",
      "Epoch 91, Loss: 7.359266464526836\n",
      "Epoch 92, Loss: 6.752965707045335\n",
      "Epoch 93, Loss: 6.49731942323538\n",
      "Epoch 94, Loss: 7.097110289793748\n",
      "Epoch 95, Loss: 7.820598877393282\n",
      "Epoch 96, Loss: 7.955620417228112\n",
      "Epoch 97, Loss: 7.29799668605511\n",
      "Epoch 98, Loss: 6.333415343211247\n",
      "Epoch 99, Loss: 6.2823845606583815\n",
      "Epoch 100, Loss: 6.31190930880033\n",
      "Epoch 101, Loss: 6.061019787421594\n",
      "Epoch 102, Loss: 6.054315053499662\n",
      "Epoch 103, Loss: 5.811168542275062\n",
      "Epoch 104, Loss: 5.950711598763099\n",
      "Epoch 105, Loss: 5.775236056401179\n",
      "Epoch 106, Loss: 5.606145015129676\n",
      "Epoch 107, Loss: 5.689011317033034\n",
      "Epoch 108, Loss: 5.6687989968519945\n",
      "Epoch 109, Loss: 6.280227367694561\n",
      "Epoch 110, Loss: 6.836472291212815\n",
      "Epoch 111, Loss: 7.371319972551786\n",
      "Epoch 112, Loss: 8.363711833953857\n",
      "Epoch 113, Loss: 8.409663145358746\n",
      "Epoch 114, Loss: 7.644126543631921\n",
      "Epoch 115, Loss: 6.557665054614727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:02,190] Trial 67 finished with value: 17.832464751316955 and parameters: {'latent_dim_z1': 12, 'latent_dim_z2': 56, 'hidden_dim': 96, 'epochs': 125, 'causal_reg': 0.995559698242531, 'learning_rate': 0.0020159491893124}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116, Loss: 6.418448503200825\n",
      "Epoch 117, Loss: 7.269366227663481\n",
      "Epoch 118, Loss: 7.566768701259907\n",
      "Epoch 119, Loss: 6.892832389244666\n",
      "Epoch 120, Loss: 7.000314767544086\n",
      "Epoch 121, Loss: 6.662064570647019\n",
      "Epoch 122, Loss: 7.64429574746352\n",
      "Epoch 123, Loss: 7.727578915082491\n",
      "Epoch 124, Loss: 7.672046294579139\n",
      "Epoch 125, Loss: 8.132863558255709\n",
      "Epoch 1, Loss: 486.7526870140663\n",
      "Epoch 2, Loss: 193.42028603186975\n",
      "Epoch 3, Loss: 143.1258459824782\n",
      "Epoch 4, Loss: 115.54569171025203\n",
      "Epoch 5, Loss: 103.23797812828651\n",
      "Epoch 6, Loss: 86.68441082881047\n",
      "Epoch 7, Loss: 72.51568295405461\n",
      "Epoch 8, Loss: 70.17703004983755\n",
      "Epoch 9, Loss: 62.58446964850793\n",
      "Epoch 10, Loss: 56.460702455960785\n",
      "Epoch 11, Loss: 50.7442684173584\n",
      "Epoch 12, Loss: 43.700704941382774\n",
      "Epoch 13, Loss: 39.83296071566068\n",
      "Epoch 14, Loss: 42.092646011939415\n",
      "Epoch 15, Loss: 37.87395829420824\n",
      "Epoch 16, Loss: 31.834853098942684\n",
      "Epoch 17, Loss: 29.417833181527946\n",
      "Epoch 18, Loss: 27.6632509965163\n",
      "Epoch 19, Loss: 27.634062326871433\n",
      "Epoch 20, Loss: 24.47370382455679\n",
      "Epoch 21, Loss: 28.544389174534725\n",
      "Epoch 22, Loss: 25.079236177297737\n",
      "Epoch 23, Loss: 21.352010616889366\n",
      "Epoch 24, Loss: 19.801283469566933\n",
      "Epoch 25, Loss: 22.20848615352924\n",
      "Epoch 26, Loss: 36.067458336169906\n",
      "Epoch 27, Loss: 24.792118292588455\n",
      "Epoch 28, Loss: 24.917500495910645\n",
      "Epoch 29, Loss: 17.744113812079796\n",
      "Epoch 30, Loss: 18.246704431680534\n",
      "Epoch 31, Loss: 17.299909848433273\n",
      "Epoch 32, Loss: 19.870193518125095\n",
      "Epoch 33, Loss: 16.31034341225257\n",
      "Epoch 34, Loss: 15.390387058258057\n",
      "Epoch 35, Loss: 18.449113368988037\n",
      "Epoch 36, Loss: 24.01792434545664\n",
      "Epoch 37, Loss: 29.781503383929913\n",
      "Epoch 38, Loss: 20.005231637221115\n",
      "Epoch 39, Loss: 13.894859644082876\n",
      "Epoch 40, Loss: 14.600320430902334\n",
      "Epoch 41, Loss: 12.04510314647968\n",
      "Epoch 42, Loss: 9.839389984424297\n",
      "Epoch 43, Loss: 10.533241730469923\n",
      "Epoch 44, Loss: 10.605307175562931\n",
      "Epoch 45, Loss: 9.320837607750526\n",
      "Epoch 46, Loss: 9.25895749605619\n",
      "Epoch 47, Loss: 8.881449864460873\n",
      "Epoch 48, Loss: 8.431803960066576\n",
      "Epoch 49, Loss: 7.472873064187857\n",
      "Epoch 50, Loss: 7.476937532424927\n",
      "Epoch 51, Loss: 7.70833002603971\n",
      "Epoch 52, Loss: 7.523028978934655\n",
      "Epoch 53, Loss: 7.988058457007775\n",
      "Epoch 54, Loss: 7.912642313883855\n",
      "Epoch 55, Loss: 8.65827701641963\n",
      "Epoch 56, Loss: 8.719742591564472\n",
      "Epoch 57, Loss: 9.224054868404682\n",
      "Epoch 58, Loss: 8.659310560960035\n",
      "Epoch 59, Loss: 10.46353866503789\n",
      "Epoch 60, Loss: 9.879571199417114\n",
      "Epoch 61, Loss: 13.079460327441875\n",
      "Epoch 62, Loss: 13.88194095171415\n",
      "Epoch 63, Loss: 13.556482755220854\n",
      "Epoch 64, Loss: 12.263955519749569\n",
      "Epoch 65, Loss: 11.27946637226985\n",
      "Epoch 66, Loss: 11.24464620076693\n",
      "Epoch 67, Loss: 9.703801998725304\n",
      "Epoch 68, Loss: 8.87262014242319\n",
      "Epoch 69, Loss: 9.569720525007982\n",
      "Epoch 70, Loss: 9.05860347014207\n",
      "Epoch 71, Loss: 8.735885986915001\n",
      "Epoch 72, Loss: 7.91607299217811\n",
      "Epoch 73, Loss: 7.909486128733708\n",
      "Epoch 74, Loss: 7.828373432159424\n",
      "Epoch 75, Loss: 7.299765256734995\n",
      "Epoch 76, Loss: 7.280361303916345\n",
      "Epoch 77, Loss: 7.98160167840811\n",
      "Epoch 78, Loss: 7.933542966842651\n",
      "Epoch 79, Loss: 8.45873891390287\n",
      "Epoch 80, Loss: 7.954051421238826\n",
      "Epoch 81, Loss: 7.817922188685491\n",
      "Epoch 82, Loss: 9.895761068050678\n",
      "Epoch 83, Loss: 12.076017434780415\n",
      "Epoch 84, Loss: 12.023437426640438\n",
      "Epoch 85, Loss: 9.750545978546143\n",
      "Epoch 86, Loss: 10.077349461041964\n",
      "Epoch 87, Loss: 8.54057530256418\n",
      "Epoch 88, Loss: 9.77533665070167\n",
      "Epoch 89, Loss: 8.11829686164856\n",
      "Epoch 90, Loss: 9.004017408077534\n",
      "Epoch 91, Loss: 8.056059965720543\n",
      "Epoch 92, Loss: 8.80318441757789\n",
      "Epoch 93, Loss: 9.10064357977647\n",
      "Epoch 94, Loss: 8.648081486041729\n",
      "Epoch 95, Loss: 8.30844416985145\n",
      "Epoch 96, Loss: 8.085116624832153\n",
      "Epoch 97, Loss: 8.063865973399235\n",
      "Epoch 98, Loss: 7.597186748798077\n",
      "Epoch 99, Loss: 8.712117910385132\n",
      "Epoch 100, Loss: 7.610021297748272\n",
      "Epoch 101, Loss: 7.156172385582557\n",
      "Epoch 102, Loss: 6.980192991403433\n",
      "Epoch 103, Loss: 6.5954006268427925\n",
      "Epoch 104, Loss: 6.452628667537983\n",
      "Epoch 105, Loss: 7.669942085559551\n",
      "Epoch 106, Loss: 7.933199919187105\n",
      "Epoch 107, Loss: 9.073862644342276\n",
      "Epoch 108, Loss: 8.53189936051002\n",
      "Epoch 109, Loss: 8.215763293779814\n",
      "Epoch 110, Loss: 7.798559812399057\n",
      "Epoch 111, Loss: 7.645047444563645\n",
      "Epoch 112, Loss: 9.37376961341271\n",
      "Epoch 113, Loss: 9.25485163468581\n",
      "Epoch 114, Loss: 9.176061666928804\n",
      "Epoch 115, Loss: 7.8555082724644585\n",
      "Epoch 116, Loss: 7.93592933508066\n",
      "Epoch 117, Loss: 9.456173438292284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:04,965] Trial 68 finished with value: 23.439919723841445 and parameters: {'latent_dim_z1': 19, 'latent_dim_z2': 54, 'hidden_dim': 96, 'epochs': 126, 'causal_reg': 0.7367737133613046, 'learning_rate': 0.0020958840547934984}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, Loss: 9.82620431826665\n",
      "Epoch 119, Loss: 9.333696695474478\n",
      "Epoch 120, Loss: 9.888027337881235\n",
      "Epoch 121, Loss: 10.73473701110253\n",
      "Epoch 122, Loss: 8.797557133894701\n",
      "Epoch 123, Loss: 10.07415925539457\n",
      "Epoch 124, Loss: 9.221571115347055\n",
      "Epoch 125, Loss: 10.687430363435011\n",
      "Epoch 126, Loss: 11.140408020753126\n",
      "Epoch 1, Loss: 353.1438487126277\n",
      "Epoch 2, Loss: 119.11241164574257\n",
      "Epoch 3, Loss: 84.54068477337177\n",
      "Epoch 4, Loss: 72.50431728363037\n",
      "Epoch 5, Loss: 70.54468932518593\n",
      "Epoch 6, Loss: 54.33261255117563\n",
      "Epoch 7, Loss: 52.31265596243051\n",
      "Epoch 8, Loss: 49.56963847233699\n",
      "Epoch 9, Loss: 42.12640989743746\n",
      "Epoch 10, Loss: 37.94289383521447\n",
      "Epoch 11, Loss: 34.92049466646635\n",
      "Epoch 12, Loss: 32.99434419778677\n",
      "Epoch 13, Loss: 30.909599744356594\n",
      "Epoch 14, Loss: 30.18768017108624\n",
      "Epoch 15, Loss: 31.21713374211238\n",
      "Epoch 16, Loss: 28.239624463594875\n",
      "Epoch 17, Loss: 26.498116163107063\n",
      "Epoch 18, Loss: 28.221770139840935\n",
      "Epoch 19, Loss: 24.677400222191444\n",
      "Epoch 20, Loss: 23.28039737848135\n",
      "Epoch 21, Loss: 26.290598906003513\n",
      "Epoch 22, Loss: 26.758141150841347\n",
      "Epoch 23, Loss: 25.183421318347637\n",
      "Epoch 24, Loss: 19.721451612619255\n",
      "Epoch 25, Loss: 21.871991854447586\n",
      "Epoch 26, Loss: 19.745453577775223\n",
      "Epoch 27, Loss: 17.854675146249626\n",
      "Epoch 28, Loss: 18.722513528970573\n",
      "Epoch 29, Loss: 19.799378138322098\n",
      "Epoch 30, Loss: 19.245187282562256\n",
      "Epoch 31, Loss: 16.078741807204025\n",
      "Epoch 32, Loss: 15.389895915985107\n",
      "Epoch 33, Loss: 14.989740224984976\n",
      "Epoch 34, Loss: 16.077090520125168\n",
      "Epoch 35, Loss: 16.239202957886917\n",
      "Epoch 36, Loss: 15.694149035673876\n",
      "Epoch 37, Loss: 14.306635709909292\n",
      "Epoch 38, Loss: 16.541225066551796\n",
      "Epoch 39, Loss: 17.70299585048969\n",
      "Epoch 40, Loss: 17.64485423381512\n",
      "Epoch 41, Loss: 14.225077959207388\n",
      "Epoch 42, Loss: 15.451894430013803\n",
      "Epoch 43, Loss: 12.118318979556744\n",
      "Epoch 44, Loss: 13.386947485116812\n",
      "Epoch 45, Loss: 15.073415095989521\n",
      "Epoch 46, Loss: 14.973910588484545\n",
      "Epoch 47, Loss: 14.08921689253587\n",
      "Epoch 48, Loss: 14.986026213719295\n",
      "Epoch 49, Loss: 12.474466947408823\n",
      "Epoch 50, Loss: 11.864530966832088\n",
      "Epoch 51, Loss: 11.983357979701115\n",
      "Epoch 52, Loss: 12.589097353128286\n",
      "Epoch 53, Loss: 11.794269451728233\n",
      "Epoch 54, Loss: 11.017939714285044\n",
      "Epoch 55, Loss: 13.162942446195162\n",
      "Epoch 56, Loss: 13.897772018726055\n",
      "Epoch 57, Loss: 12.673354368943434\n",
      "Epoch 58, Loss: 9.351757177939781\n",
      "Epoch 59, Loss: 9.357130784254808\n",
      "Epoch 60, Loss: 8.802993591015156\n",
      "Epoch 61, Loss: 8.772194660626925\n",
      "Epoch 62, Loss: 8.736628459050106\n",
      "Epoch 63, Loss: 8.690983093701876\n",
      "Epoch 64, Loss: 10.13199965770428\n",
      "Epoch 65, Loss: 13.426059576181265\n",
      "Epoch 66, Loss: 13.604398030501146\n",
      "Epoch 67, Loss: 16.687204746099617\n",
      "Epoch 68, Loss: 21.339413532843956\n",
      "Epoch 69, Loss: 16.449309275700497\n",
      "Epoch 70, Loss: 13.568508551670956\n",
      "Epoch 71, Loss: 11.535191462590145\n",
      "Epoch 72, Loss: 10.933547606834999\n",
      "Epoch 73, Loss: 9.989189899884737\n",
      "Epoch 74, Loss: 12.566487624095036\n",
      "Epoch 75, Loss: 10.669100706393902\n",
      "Epoch 76, Loss: 12.215579729813795\n",
      "Epoch 77, Loss: 11.071936148863573\n",
      "Epoch 78, Loss: 9.085048088660606\n",
      "Epoch 79, Loss: 9.047638342930721\n",
      "Epoch 80, Loss: 8.299969599797176\n",
      "Epoch 81, Loss: 8.18098574418288\n",
      "Epoch 82, Loss: 8.886865835923414\n",
      "Epoch 83, Loss: 8.800540759013249\n",
      "Epoch 84, Loss: 8.826303005218506\n",
      "Epoch 85, Loss: 8.703241641704853\n",
      "Epoch 86, Loss: 8.63950958618751\n",
      "Epoch 87, Loss: 10.935101160636314\n",
      "Epoch 88, Loss: 9.114952124082125\n",
      "Epoch 89, Loss: 8.381770922587467\n",
      "Epoch 90, Loss: 8.25740440075214\n",
      "Epoch 91, Loss: 8.257080793380737\n",
      "Epoch 92, Loss: 10.65744455044086\n",
      "Epoch 93, Loss: 9.4704540509444\n",
      "Epoch 94, Loss: 11.168031013928926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:07,276] Trial 69 finished with value: 22.632277364284914 and parameters: {'latent_dim_z1': 11, 'latent_dim_z2': 57, 'hidden_dim': 115, 'epochs': 102, 'causal_reg': 0.48789322113823197, 'learning_rate': 0.005109278250049192}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, Loss: 15.110085909183208\n",
      "Epoch 96, Loss: 15.019854637292715\n",
      "Epoch 97, Loss: 13.774211443387545\n",
      "Epoch 98, Loss: 10.757501125335693\n",
      "Epoch 99, Loss: 8.150504644100483\n",
      "Epoch 100, Loss: 8.424182011530949\n",
      "Epoch 101, Loss: 7.546643000382644\n",
      "Epoch 102, Loss: 8.200746371195866\n",
      "Epoch 1, Loss: 2804.503194955679\n",
      "Epoch 2, Loss: 627.6751022338867\n",
      "Epoch 3, Loss: 429.17037787804236\n",
      "Epoch 4, Loss: 369.6376351576585\n",
      "Epoch 5, Loss: 321.7872126652644\n",
      "Epoch 6, Loss: 293.38896501981293\n",
      "Epoch 7, Loss: 260.13800342266376\n",
      "Epoch 8, Loss: 272.3368624173678\n",
      "Epoch 9, Loss: 230.30162591200607\n",
      "Epoch 10, Loss: 212.54786036564752\n",
      "Epoch 11, Loss: 214.20045001690204\n",
      "Epoch 12, Loss: 253.01155002300555\n",
      "Epoch 13, Loss: 255.78929372934195\n",
      "Epoch 14, Loss: 194.4905351492075\n",
      "Epoch 15, Loss: 196.75016696636493\n",
      "Epoch 16, Loss: 147.41939904139593\n",
      "Epoch 17, Loss: 152.22157610379733\n",
      "Epoch 18, Loss: 160.462218944843\n",
      "Epoch 19, Loss: 139.8628179843609\n",
      "Epoch 20, Loss: 142.38349665128268\n",
      "Epoch 21, Loss: 128.15471795889047\n",
      "Epoch 22, Loss: 118.20639771681566\n",
      "Epoch 23, Loss: 178.31975394028885\n",
      "Epoch 24, Loss: 167.42818788381723\n",
      "Epoch 25, Loss: 112.73560157189003\n",
      "Epoch 26, Loss: 110.89708717052753\n",
      "Epoch 27, Loss: 86.92384265019344\n",
      "Epoch 28, Loss: 93.39161330003004\n",
      "Epoch 29, Loss: 103.21942006624661\n",
      "Epoch 30, Loss: 77.37118926415077\n",
      "Epoch 31, Loss: 71.18886126004733\n",
      "Epoch 32, Loss: 87.51381448599008\n",
      "Epoch 33, Loss: 109.56875536992\n",
      "Epoch 34, Loss: 127.75527396568886\n",
      "Epoch 35, Loss: 103.34066735781155\n",
      "Epoch 36, Loss: 66.0145908502432\n",
      "Epoch 37, Loss: 61.106881801898666\n",
      "Epoch 38, Loss: 95.01085530794583\n",
      "Epoch 39, Loss: 88.01521242581882\n",
      "Epoch 40, Loss: 80.6411341887254\n",
      "Epoch 41, Loss: 77.40416292043832\n",
      "Epoch 42, Loss: 61.35163087111253\n",
      "Epoch 43, Loss: 50.3018217086792\n",
      "Epoch 44, Loss: 48.42500660969661\n",
      "Epoch 45, Loss: 53.74194724743183\n",
      "Epoch 46, Loss: 64.15140173985408\n",
      "Epoch 47, Loss: 99.9212733048659\n",
      "Epoch 48, Loss: 67.32511241619403\n",
      "Epoch 49, Loss: 70.9146716778095\n",
      "Epoch 50, Loss: 77.20743784537682\n",
      "Epoch 51, Loss: 64.81459360856276\n",
      "Epoch 52, Loss: 49.57084391667293\n",
      "Epoch 53, Loss: 59.56556833707369\n",
      "Epoch 54, Loss: 63.6551990875831\n",
      "Epoch 55, Loss: 76.36028920687161\n",
      "Epoch 56, Loss: 76.66403198242188\n",
      "Epoch 57, Loss: 59.27335306314322\n",
      "Epoch 58, Loss: 75.5283540578989\n",
      "Epoch 59, Loss: 51.496785383958084\n",
      "Epoch 60, Loss: 53.84107780456543\n",
      "Epoch 61, Loss: 51.897158035865196\n",
      "Epoch 62, Loss: 70.89985781449538\n",
      "Epoch 63, Loss: 48.350739405705376\n",
      "Epoch 64, Loss: 40.29360499748817\n",
      "Epoch 65, Loss: 45.73536076912513\n",
      "Epoch 66, Loss: 43.13100499373216\n",
      "Epoch 67, Loss: 38.11677012076745\n",
      "Epoch 68, Loss: 28.207631514622616\n",
      "Epoch 69, Loss: 25.195272042201115\n",
      "Epoch 70, Loss: 34.96338022672213\n",
      "Epoch 71, Loss: 68.50451256678654\n",
      "Epoch 72, Loss: 65.3417189671443\n",
      "Epoch 73, Loss: 38.583143087533806\n",
      "Epoch 74, Loss: 30.906837976895847\n",
      "Epoch 75, Loss: 26.45633308704083\n",
      "Epoch 76, Loss: 43.2591008406419\n",
      "Epoch 77, Loss: 60.1219886633066\n",
      "Epoch 78, Loss: 71.88187012305626\n",
      "Epoch 79, Loss: 87.28984392606296\n",
      "Epoch 80, Loss: 46.536768399752106\n",
      "Epoch 81, Loss: 48.31923561829787\n",
      "Epoch 82, Loss: 37.65726412259615\n",
      "Epoch 83, Loss: 38.26413800166203\n",
      "Epoch 84, Loss: 45.25865177007822\n",
      "Epoch 85, Loss: 41.07603329878587\n",
      "Epoch 86, Loss: 30.94939716045673\n",
      "Epoch 87, Loss: 26.664840588202843\n",
      "Epoch 88, Loss: 33.38970110966609\n",
      "Epoch 89, Loss: 26.056605082291824\n",
      "Epoch 90, Loss: 25.30165907052847\n",
      "Epoch 91, Loss: 23.396956480466404\n",
      "Epoch 92, Loss: 25.538383153768685\n",
      "Epoch 93, Loss: 33.42258886190561\n",
      "Epoch 94, Loss: 30.820992433107815\n",
      "Epoch 95, Loss: 33.28913244834313\n",
      "Epoch 96, Loss: 32.886689993051384\n",
      "Epoch 97, Loss: 51.783511015085075\n",
      "Epoch 98, Loss: 54.03445922411405\n",
      "Epoch 99, Loss: 40.673898220062256\n",
      "Epoch 100, Loss: 34.614342542795036\n",
      "Epoch 101, Loss: 37.32280547802265\n",
      "Epoch 102, Loss: 24.75334328871507\n",
      "Epoch 103, Loss: 27.78264757303091\n",
      "Epoch 104, Loss: 36.81925494854267\n",
      "Epoch 105, Loss: 28.23997710301326\n",
      "Epoch 106, Loss: 36.63363376030555\n",
      "Epoch 107, Loss: 57.76995064662053\n",
      "Epoch 108, Loss: 77.26418011005109\n",
      "Epoch 109, Loss: 39.48916963430551\n",
      "Epoch 110, Loss: 26.553485503563515\n",
      "Epoch 111, Loss: 21.039105305304894\n",
      "Epoch 112, Loss: 17.069053649902344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:10,149] Trial 70 finished with value: 115.70645188094797 and parameters: {'latent_dim_z1': 71, 'latent_dim_z2': 63, 'hidden_dim': 102, 'epochs': 114, 'causal_reg': 0.9115226305286314, 'learning_rate': 0.01102180944635942}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113, Loss: 16.638477692237267\n",
      "Epoch 114, Loss: 17.692286014556885\n",
      "Epoch 1, Loss: 417.1449579092172\n",
      "Epoch 2, Loss: 160.67063038165753\n",
      "Epoch 3, Loss: 117.23547319265512\n",
      "Epoch 4, Loss: 98.72551683279184\n",
      "Epoch 5, Loss: 88.20173483628493\n",
      "Epoch 6, Loss: 77.25384125342735\n",
      "Epoch 7, Loss: 70.1719242976262\n",
      "Epoch 8, Loss: 63.779662499061\n",
      "Epoch 9, Loss: 55.05731333219088\n",
      "Epoch 10, Loss: 50.31576585769653\n",
      "Epoch 11, Loss: 44.36631254049448\n",
      "Epoch 12, Loss: 41.530566949110764\n",
      "Epoch 13, Loss: 39.97737539731539\n",
      "Epoch 14, Loss: 34.83279697711651\n",
      "Epoch 15, Loss: 41.709454389718864\n",
      "Epoch 16, Loss: 32.443415274986855\n",
      "Epoch 17, Loss: 26.89805192213792\n",
      "Epoch 18, Loss: 25.08648263491117\n",
      "Epoch 19, Loss: 23.4914231300354\n",
      "Epoch 20, Loss: 22.20423570046058\n",
      "Epoch 21, Loss: 20.693002297328068\n",
      "Epoch 22, Loss: 17.97964327151959\n",
      "Epoch 23, Loss: 15.928212532630333\n",
      "Epoch 24, Loss: 18.565379179441013\n",
      "Epoch 25, Loss: 14.021878059093769\n",
      "Epoch 26, Loss: 13.698629929469181\n",
      "Epoch 27, Loss: 17.978306880364052\n",
      "Epoch 28, Loss: 13.909136185279259\n",
      "Epoch 29, Loss: 12.291046656095064\n",
      "Epoch 30, Loss: 12.288211694130531\n",
      "Epoch 31, Loss: 12.073822204883282\n",
      "Epoch 32, Loss: 10.599993247252245\n",
      "Epoch 33, Loss: 9.405872069872343\n",
      "Epoch 34, Loss: 10.451144860341\n",
      "Epoch 35, Loss: 10.145315133608305\n",
      "Epoch 36, Loss: 10.012540157024677\n",
      "Epoch 37, Loss: 8.699671855339638\n",
      "Epoch 38, Loss: 8.953579425811768\n",
      "Epoch 39, Loss: 8.426268027378963\n",
      "Epoch 40, Loss: 7.6342049562014065\n",
      "Epoch 41, Loss: 7.239983356915987\n",
      "Epoch 42, Loss: 7.247071339533879\n",
      "Epoch 43, Loss: 7.08793489749615\n",
      "Epoch 44, Loss: 7.042911566220797\n",
      "Epoch 45, Loss: 7.9843480587005615\n",
      "Epoch 46, Loss: 8.672793736824623\n",
      "Epoch 47, Loss: 7.647468181756826\n",
      "Epoch 48, Loss: 7.853645984943096\n",
      "Epoch 49, Loss: 10.61142712373\n",
      "Epoch 50, Loss: 9.331586562670195\n",
      "Epoch 51, Loss: 8.775188464384813\n",
      "Epoch 52, Loss: 8.247582013790424\n",
      "Epoch 53, Loss: 7.182587165098924\n",
      "Epoch 54, Loss: 6.866774155543401\n",
      "Epoch 55, Loss: 7.077753910651574\n",
      "Epoch 56, Loss: 7.093287688035232\n",
      "Epoch 57, Loss: 7.560999283423791\n",
      "Epoch 58, Loss: 8.502305085842426\n",
      "Epoch 59, Loss: 7.776524598781879\n",
      "Epoch 60, Loss: 7.220794109197763\n",
      "Epoch 61, Loss: 6.954030201985286\n",
      "Epoch 62, Loss: 6.96645555129418\n",
      "Epoch 63, Loss: 6.852944594163161\n",
      "Epoch 64, Loss: 6.710520836023184\n",
      "Epoch 65, Loss: 7.733796156369722\n",
      "Epoch 66, Loss: 11.8927376637092\n",
      "Epoch 67, Loss: 13.526637810927172\n",
      "Epoch 68, Loss: 10.160583184315609\n",
      "Epoch 69, Loss: 8.798825117257925\n",
      "Epoch 70, Loss: 7.9076726620013895\n",
      "Epoch 71, Loss: 7.380531659493079\n",
      "Epoch 72, Loss: 6.705035026256855\n",
      "Epoch 73, Loss: 6.4032305754148044\n",
      "Epoch 74, Loss: 6.304655735309307\n",
      "Epoch 75, Loss: 6.409262180328369\n",
      "Epoch 76, Loss: 6.06036010155311\n",
      "Epoch 77, Loss: 6.022426715263953\n",
      "Epoch 78, Loss: 6.164633182378916\n",
      "Epoch 79, Loss: 6.033947981320894\n",
      "Epoch 80, Loss: 5.830078180019672\n",
      "Epoch 81, Loss: 6.163944812921377\n",
      "Epoch 82, Loss: 6.2089173426994915\n",
      "Epoch 83, Loss: 6.357677899874174\n",
      "Epoch 84, Loss: 6.291478432141817\n",
      "Epoch 85, Loss: 6.681353825789231\n",
      "Epoch 86, Loss: 6.8490349146036005\n",
      "Epoch 87, Loss: 7.111122718224158\n",
      "Epoch 88, Loss: 6.689812018321111\n",
      "Epoch 89, Loss: 6.614430134113018\n",
      "Epoch 90, Loss: 6.403114337187547\n",
      "Epoch 91, Loss: 7.692963673518254\n",
      "Epoch 92, Loss: 8.114175613109882\n",
      "Epoch 93, Loss: 8.669506146357609\n",
      "Epoch 94, Loss: 8.713780293097862\n",
      "Epoch 95, Loss: 8.5653450855842\n",
      "Epoch 96, Loss: 8.04420067713811\n",
      "Epoch 97, Loss: 8.57179542688223\n",
      "Epoch 98, Loss: 8.004475887005146\n",
      "Epoch 99, Loss: 7.380257643186129\n",
      "Epoch 100, Loss: 7.039224239496084\n",
      "Epoch 101, Loss: 7.182153591742883\n",
      "Epoch 102, Loss: 7.275401738973764\n",
      "Epoch 103, Loss: 7.314462680083055\n",
      "Epoch 104, Loss: 7.157737401815561\n",
      "Epoch 105, Loss: 6.611458925100473\n",
      "Epoch 106, Loss: 6.214636655954214\n",
      "Epoch 107, Loss: 6.337713168217586\n",
      "Epoch 108, Loss: 6.202635544996995\n",
      "Epoch 109, Loss: 6.454222220640916\n",
      "Epoch 110, Loss: 6.648626510913555\n",
      "Epoch 111, Loss: 6.577622156876784\n",
      "Epoch 112, Loss: 6.2551844120025635\n",
      "Epoch 113, Loss: 6.1877887065594015\n",
      "Epoch 114, Loss: 6.177237859139075\n",
      "Epoch 115, Loss: 6.043091333829439\n",
      "Epoch 116, Loss: 6.060509846760676\n",
      "Epoch 117, Loss: 6.179211488136878\n",
      "Epoch 118, Loss: 6.345942313854511\n",
      "Epoch 119, Loss: 6.866827377906213\n",
      "Epoch 120, Loss: 8.590266576180092\n",
      "Epoch 121, Loss: 8.433157168901884\n",
      "Epoch 122, Loss: 8.775629080258883\n",
      "Epoch 123, Loss: 7.80086902471689\n",
      "Epoch 124, Loss: 7.217956708027766\n",
      "Epoch 125, Loss: 7.61689673937284\n",
      "Epoch 126, Loss: 7.61452839924739\n",
      "Epoch 127, Loss: 7.744405122903677\n",
      "Epoch 128, Loss: 6.945870399475098\n",
      "Epoch 129, Loss: 7.479382845071646\n",
      "Epoch 130, Loss: 7.130996539042546\n",
      "Epoch 131, Loss: 6.52626633644104\n",
      "Epoch 132, Loss: 6.628451420710637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:13,119] Trial 71 finished with value: 18.55751000297352 and parameters: {'latent_dim_z1': 13, 'latent_dim_z2': 70, 'hidden_dim': 87, 'epochs': 133, 'causal_reg': 0.9919365258800441, 'learning_rate': 0.0015635825380110886}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133, Loss: 6.690952649483314\n",
      "Epoch 1, Loss: 465.48811839177057\n",
      "Epoch 2, Loss: 192.7579677288349\n",
      "Epoch 3, Loss: 139.97905085637018\n",
      "Epoch 4, Loss: 116.17608231764574\n",
      "Epoch 5, Loss: 104.34931329580454\n",
      "Epoch 6, Loss: 86.71153361980731\n",
      "Epoch 7, Loss: 74.74299555558424\n",
      "Epoch 8, Loss: 64.03289838937613\n",
      "Epoch 9, Loss: 58.76101061014029\n",
      "Epoch 10, Loss: 51.957367823674126\n",
      "Epoch 11, Loss: 47.5531362753648\n",
      "Epoch 12, Loss: 38.45798771197979\n",
      "Epoch 13, Loss: 35.18289903494028\n",
      "Epoch 14, Loss: 35.04187092414269\n",
      "Epoch 15, Loss: 34.42787038362943\n",
      "Epoch 16, Loss: 27.630587797898514\n",
      "Epoch 17, Loss: 23.603817793039177\n",
      "Epoch 18, Loss: 20.83610703394963\n",
      "Epoch 19, Loss: 17.706856104043815\n",
      "Epoch 20, Loss: 15.947655347677378\n",
      "Epoch 21, Loss: 16.07499702160175\n",
      "Epoch 22, Loss: 14.902474330021786\n",
      "Epoch 23, Loss: 12.990090241798988\n",
      "Epoch 24, Loss: 14.473505643697886\n",
      "Epoch 25, Loss: 15.28096233881437\n",
      "Epoch 26, Loss: 11.597295357630802\n",
      "Epoch 27, Loss: 10.522552691973173\n",
      "Epoch 28, Loss: 9.924583233319796\n",
      "Epoch 29, Loss: 10.098553474132832\n",
      "Epoch 30, Loss: 9.277404308319092\n",
      "Epoch 31, Loss: 9.635390776854296\n",
      "Epoch 32, Loss: 10.510852667001577\n",
      "Epoch 33, Loss: 9.158062072900625\n",
      "Epoch 34, Loss: 9.061976157701933\n",
      "Epoch 35, Loss: 7.9588105495159445\n",
      "Epoch 36, Loss: 7.811945236646212\n",
      "Epoch 37, Loss: 7.3877991162813625\n",
      "Epoch 38, Loss: 7.060918074387771\n",
      "Epoch 39, Loss: 8.093542594176073\n",
      "Epoch 40, Loss: 7.579295048346887\n",
      "Epoch 41, Loss: 6.911576454456036\n",
      "Epoch 42, Loss: 6.693277267309336\n",
      "Epoch 43, Loss: 6.718567041250376\n",
      "Epoch 44, Loss: 7.033861013559195\n",
      "Epoch 45, Loss: 7.29572684948261\n",
      "Epoch 46, Loss: 7.746962052125197\n",
      "Epoch 47, Loss: 7.89014115700355\n",
      "Epoch 48, Loss: 9.586287755232592\n",
      "Epoch 49, Loss: 10.682814524723934\n",
      "Epoch 50, Loss: 9.394028241817768\n",
      "Epoch 51, Loss: 10.2799375974215\n",
      "Epoch 52, Loss: 9.573290036274837\n",
      "Epoch 53, Loss: 8.143490057725172\n",
      "Epoch 54, Loss: 8.658679687059843\n",
      "Epoch 55, Loss: 8.840725275186392\n",
      "Epoch 56, Loss: 8.258015394210815\n",
      "Epoch 57, Loss: 7.766964215498704\n",
      "Epoch 58, Loss: 7.234759257389949\n",
      "Epoch 59, Loss: 6.486372360816369\n",
      "Epoch 60, Loss: 6.624972967001108\n",
      "Epoch 61, Loss: 6.4662780945117655\n",
      "Epoch 62, Loss: 6.34778983776386\n",
      "Epoch 63, Loss: 6.99176837847783\n",
      "Epoch 64, Loss: 6.8637647445385275\n",
      "Epoch 65, Loss: 6.483392587074866\n",
      "Epoch 66, Loss: 6.953831911087036\n",
      "Epoch 67, Loss: 11.049289189852201\n",
      "Epoch 68, Loss: 13.2660726583921\n",
      "Epoch 69, Loss: 9.842620556171124\n",
      "Epoch 70, Loss: 9.13798031440148\n",
      "Epoch 71, Loss: 8.507853966492872\n",
      "Epoch 72, Loss: 7.182669071050791\n",
      "Epoch 73, Loss: 8.411370204045223\n",
      "Epoch 74, Loss: 8.372376937132616\n",
      "Epoch 75, Loss: 7.333331016393808\n",
      "Epoch 76, Loss: 6.679391145706177\n",
      "Epoch 77, Loss: 6.889817861410288\n",
      "Epoch 78, Loss: 6.91658064035269\n",
      "Epoch 79, Loss: 7.143029763148381\n",
      "Epoch 80, Loss: 7.240181391055767\n",
      "Epoch 81, Loss: 6.767874864431528\n",
      "Epoch 82, Loss: 6.562043300041785\n",
      "Epoch 83, Loss: 6.8430986771216755\n",
      "Epoch 84, Loss: 7.383060620381282\n",
      "Epoch 85, Loss: 6.677052754622239\n",
      "Epoch 86, Loss: 6.775778807126558\n",
      "Epoch 87, Loss: 6.6164601766146145\n",
      "Epoch 88, Loss: 6.544026814974272\n",
      "Epoch 89, Loss: 7.350960823205801\n",
      "Epoch 90, Loss: 8.90872150201064\n",
      "Epoch 91, Loss: 7.932338421161358\n",
      "Epoch 92, Loss: 8.150056655590351\n",
      "Epoch 93, Loss: 11.252177513562716\n",
      "Epoch 94, Loss: 9.080983767142662\n",
      "Epoch 95, Loss: 7.537913505847637\n",
      "Epoch 96, Loss: 8.797183220203106\n",
      "Epoch 97, Loss: 9.038673841036283\n",
      "Epoch 98, Loss: 7.426681903692392\n",
      "Epoch 99, Loss: 6.568693857926589\n",
      "Epoch 100, Loss: 7.048871535521287\n",
      "Epoch 101, Loss: 6.681670097204355\n",
      "Epoch 102, Loss: 6.466894186460054\n",
      "Epoch 103, Loss: 6.283463826546302\n",
      "Epoch 104, Loss: 6.255507872654841\n",
      "Epoch 105, Loss: 6.379343344615056\n",
      "Epoch 106, Loss: 6.9377743280850925\n",
      "Epoch 107, Loss: 6.714987791501558\n",
      "Epoch 108, Loss: 6.550554495591384\n",
      "Epoch 109, Loss: 6.994143009185791\n",
      "Epoch 110, Loss: 6.94700813293457\n",
      "Epoch 111, Loss: 6.272096358812773\n",
      "Epoch 112, Loss: 5.829975733390222\n",
      "Epoch 113, Loss: 6.1441204364483175\n",
      "Epoch 114, Loss: 6.431366718732393\n",
      "Epoch 115, Loss: 6.219950914382935\n",
      "Epoch 116, Loss: 6.1326353549957275\n",
      "Epoch 117, Loss: 6.0811943090879\n",
      "Epoch 118, Loss: 6.062463925434993\n",
      "Epoch 119, Loss: 6.024474675838764\n",
      "Epoch 120, Loss: 6.4776593171633206\n",
      "Epoch 121, Loss: 6.3618538746467\n",
      "Epoch 122, Loss: 6.211704107431265\n",
      "Epoch 123, Loss: 5.82877751497122\n",
      "Epoch 124, Loss: 5.914692787023691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:16,278] Trial 72 finished with value: 19.185707897309833 and parameters: {'latent_dim_z1': 15, 'latent_dim_z2': 76, 'hidden_dim': 138, 'epochs': 133, 'causal_reg': 0.9520255401848579, 'learning_rate': 0.0015985177623781537}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125, Loss: 6.191118698853713\n",
      "Epoch 126, Loss: 6.39504337310791\n",
      "Epoch 127, Loss: 6.98500394821167\n",
      "Epoch 128, Loss: 7.185269924310537\n",
      "Epoch 129, Loss: 6.92052435874939\n",
      "Epoch 130, Loss: 6.99074231661283\n",
      "Epoch 131, Loss: 7.50068972660945\n",
      "Epoch 132, Loss: 9.016319091503437\n",
      "Epoch 133, Loss: 8.40149582349337\n",
      "Epoch 1, Loss: 394.6066838777982\n",
      "Epoch 2, Loss: 148.17686359698956\n",
      "Epoch 3, Loss: 112.67319958026593\n",
      "Epoch 4, Loss: 97.40627450209398\n",
      "Epoch 5, Loss: 86.97134458101712\n",
      "Epoch 6, Loss: 73.59324645996094\n",
      "Epoch 7, Loss: 67.23264415447528\n",
      "Epoch 8, Loss: 65.5692777633667\n",
      "Epoch 9, Loss: 54.65144392160269\n",
      "Epoch 10, Loss: 49.836328799907974\n",
      "Epoch 11, Loss: 45.27501722482535\n",
      "Epoch 12, Loss: 42.82794108757606\n",
      "Epoch 13, Loss: 41.99511322608361\n",
      "Epoch 14, Loss: 38.66531837903536\n",
      "Epoch 15, Loss: 34.765940739558296\n",
      "Epoch 16, Loss: 32.44176908639761\n",
      "Epoch 17, Loss: 27.396878976088303\n",
      "Epoch 18, Loss: 27.721647849449745\n",
      "Epoch 19, Loss: 23.793118256788986\n",
      "Epoch 20, Loss: 22.076825380325317\n",
      "Epoch 21, Loss: 22.13554419004\n",
      "Epoch 22, Loss: 20.252467265495888\n",
      "Epoch 23, Loss: 17.04538994569045\n",
      "Epoch 24, Loss: 15.938176485208364\n",
      "Epoch 25, Loss: 15.19991243802584\n",
      "Epoch 26, Loss: 15.390275955200195\n",
      "Epoch 27, Loss: 14.057921372927153\n",
      "Epoch 28, Loss: 15.742848836458647\n",
      "Epoch 29, Loss: 16.828147191267746\n",
      "Epoch 30, Loss: 12.524493859364437\n",
      "Epoch 31, Loss: 10.657126646775465\n",
      "Epoch 32, Loss: 10.186355719199547\n",
      "Epoch 33, Loss: 9.440759071936974\n",
      "Epoch 34, Loss: 9.406356921562782\n",
      "Epoch 35, Loss: 13.259666919708252\n",
      "Epoch 36, Loss: 10.11459524814899\n",
      "Epoch 37, Loss: 10.44835272202125\n",
      "Epoch 38, Loss: 10.401628310863789\n",
      "Epoch 39, Loss: 10.915501081026518\n",
      "Epoch 40, Loss: 11.837635168662437\n",
      "Epoch 41, Loss: 9.090434221120981\n",
      "Epoch 42, Loss: 7.685582931225117\n",
      "Epoch 43, Loss: 7.346736651200515\n",
      "Epoch 44, Loss: 6.740082997542161\n",
      "Epoch 45, Loss: 7.563271742600661\n",
      "Epoch 46, Loss: 6.9608794542459345\n",
      "Epoch 47, Loss: 6.807989835739136\n",
      "Epoch 48, Loss: 6.793094048133264\n",
      "Epoch 49, Loss: 6.587111638142512\n",
      "Epoch 50, Loss: 7.034632921218872\n",
      "Epoch 51, Loss: 7.704829876239483\n",
      "Epoch 52, Loss: 8.323901378191435\n",
      "Epoch 53, Loss: 7.580871270253108\n",
      "Epoch 54, Loss: 7.1272550362807054\n",
      "Epoch 55, Loss: 8.235293516745934\n",
      "Epoch 56, Loss: 11.70791482925415\n",
      "Epoch 57, Loss: 13.310878166785606\n",
      "Epoch 58, Loss: 12.130311122307411\n",
      "Epoch 59, Loss: 9.375726864888119\n",
      "Epoch 60, Loss: 7.460173276754526\n",
      "Epoch 61, Loss: 6.601161223191482\n",
      "Epoch 62, Loss: 6.736875222279475\n",
      "Epoch 63, Loss: 6.983735744769756\n",
      "Epoch 64, Loss: 6.1804540157318115\n",
      "Epoch 65, Loss: 6.039964364125178\n",
      "Epoch 66, Loss: 6.035987798984234\n",
      "Epoch 67, Loss: 6.141447690817026\n",
      "Epoch 68, Loss: 6.102203827637893\n",
      "Epoch 69, Loss: 6.050393709769616\n",
      "Epoch 70, Loss: 5.941375475663405\n",
      "Epoch 71, Loss: 5.970964486782368\n",
      "Epoch 72, Loss: 5.797632382466243\n",
      "Epoch 73, Loss: 5.8269602702214165\n",
      "Epoch 74, Loss: 5.68238458266625\n",
      "Epoch 75, Loss: 5.743326939069307\n",
      "Epoch 76, Loss: 5.710872595126812\n",
      "Epoch 77, Loss: 5.9417602465702934\n",
      "Epoch 78, Loss: 6.346938023200402\n",
      "Epoch 79, Loss: 7.403352113870474\n",
      "Epoch 80, Loss: 7.0021494168501635\n",
      "Epoch 81, Loss: 8.535202906681942\n",
      "Epoch 82, Loss: 8.44437916462238\n",
      "Epoch 83, Loss: 9.013559029652523\n",
      "Epoch 84, Loss: 7.43937688607436\n",
      "Epoch 85, Loss: 7.089002682612493\n",
      "Epoch 86, Loss: 6.6506018455211935\n",
      "Epoch 87, Loss: 6.296136837739211\n",
      "Epoch 88, Loss: 6.531781930189866\n",
      "Epoch 89, Loss: 7.377908651645367\n",
      "Epoch 90, Loss: 7.466395928309514\n",
      "Epoch 91, Loss: 7.232041872464693\n",
      "Epoch 92, Loss: 6.999312070699839\n",
      "Epoch 93, Loss: 6.58637521817134\n",
      "Epoch 94, Loss: 6.355423395450298\n",
      "Epoch 95, Loss: 5.922320439265325\n",
      "Epoch 96, Loss: 6.08898219695458\n",
      "Epoch 97, Loss: 6.543382736352774\n",
      "Epoch 98, Loss: 6.448160758385291\n",
      "Epoch 99, Loss: 6.030956580088689\n",
      "Epoch 100, Loss: 6.0370358136984015\n",
      "Epoch 101, Loss: 6.177965494302603\n",
      "Epoch 102, Loss: 6.009306540855994\n",
      "Epoch 103, Loss: 6.199007126001211\n",
      "Epoch 104, Loss: 7.417472270818857\n",
      "Epoch 105, Loss: 8.081105195558989\n",
      "Epoch 106, Loss: 7.955994789416973\n",
      "Epoch 107, Loss: 8.083129681073702\n",
      "Epoch 108, Loss: 9.09834047464224\n",
      "Epoch 109, Loss: 7.853421192902785\n",
      "Epoch 110, Loss: 7.208400084422185\n",
      "Epoch 111, Loss: 6.854349576509916\n",
      "Epoch 112, Loss: 6.138871357991145\n",
      "Epoch 113, Loss: 5.714579417155339\n",
      "Epoch 114, Loss: 5.730516727154072\n",
      "Epoch 115, Loss: 5.627912796460665\n",
      "Epoch 116, Loss: 5.59726097033574\n",
      "Epoch 117, Loss: 5.586729673238901\n",
      "Epoch 118, Loss: 5.543693395761343\n",
      "Epoch 119, Loss: 5.5831243625054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:19,122] Trial 73 finished with value: 19.11419804399882 and parameters: {'latent_dim_z1': 12, 'latent_dim_z2': 70, 'hidden_dim': 135, 'epochs': 122, 'causal_reg': 0.9974600176715764, 'learning_rate': 0.001555228165467721}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: 5.562330851188073\n",
      "Epoch 121, Loss: 5.62538900742164\n",
      "Epoch 122, Loss: 5.927978735703689\n",
      "Epoch 1, Loss: 417.9449272155762\n",
      "Epoch 2, Loss: 161.86022758483887\n",
      "Epoch 3, Loss: 115.5785608291626\n",
      "Epoch 4, Loss: 100.39129770719089\n",
      "Epoch 5, Loss: 85.10596407376804\n",
      "Epoch 6, Loss: 71.42078891167274\n",
      "Epoch 7, Loss: 67.68323993682861\n",
      "Epoch 8, Loss: 61.322516881502594\n",
      "Epoch 9, Loss: 56.29694109696608\n",
      "Epoch 10, Loss: 49.2444391984206\n",
      "Epoch 11, Loss: 43.186425062326286\n",
      "Epoch 12, Loss: 44.46169552436242\n",
      "Epoch 13, Loss: 41.93805269094614\n",
      "Epoch 14, Loss: 38.027631099407486\n",
      "Epoch 15, Loss: 34.31764573317308\n",
      "Epoch 16, Loss: 37.16409639211801\n",
      "Epoch 17, Loss: 31.722771938030537\n",
      "Epoch 18, Loss: 34.2739859360915\n",
      "Epoch 19, Loss: 31.53570032119751\n",
      "Epoch 20, Loss: 31.04925984602708\n",
      "Epoch 21, Loss: 28.132366767296425\n",
      "Epoch 22, Loss: 33.33466346447285\n",
      "Epoch 23, Loss: 23.624208780435417\n",
      "Epoch 24, Loss: 22.815997747274544\n",
      "Epoch 25, Loss: 20.38592925438514\n",
      "Epoch 26, Loss: 19.20303146655743\n",
      "Epoch 27, Loss: 19.60077612216656\n",
      "Epoch 28, Loss: 21.44716901045579\n",
      "Epoch 29, Loss: 23.108731966752273\n",
      "Epoch 30, Loss: 19.013662044818584\n",
      "Epoch 31, Loss: 18.806110822237454\n",
      "Epoch 32, Loss: 18.7196102142334\n",
      "Epoch 33, Loss: 17.540351134080154\n",
      "Epoch 34, Loss: 18.09361742093013\n",
      "Epoch 35, Loss: 17.504928075350247\n",
      "Epoch 36, Loss: 16.491318629338192\n",
      "Epoch 37, Loss: 14.064685638134296\n",
      "Epoch 38, Loss: 14.92771552159236\n",
      "Epoch 39, Loss: 14.206473717322716\n",
      "Epoch 40, Loss: 13.12155846449045\n",
      "Epoch 41, Loss: 14.909359418428862\n",
      "Epoch 42, Loss: 17.723846692305344\n",
      "Epoch 43, Loss: 13.3093277491056\n",
      "Epoch 44, Loss: 12.619738633816059\n",
      "Epoch 45, Loss: 19.502672232114353\n",
      "Epoch 46, Loss: 15.406642326941856\n",
      "Epoch 47, Loss: 16.488641335414005\n",
      "Epoch 48, Loss: 11.639248224405142\n",
      "Epoch 49, Loss: 9.905777509395893\n",
      "Epoch 50, Loss: 10.22998571395874\n",
      "Epoch 51, Loss: 10.061265303538395\n",
      "Epoch 52, Loss: 9.960801326311552\n",
      "Epoch 53, Loss: 8.812774328085093\n",
      "Epoch 54, Loss: 8.961525678634644\n",
      "Epoch 55, Loss: 8.30369523855356\n",
      "Epoch 56, Loss: 8.523836667721088\n",
      "Epoch 57, Loss: 8.429168994610126\n",
      "Epoch 58, Loss: 9.70048420245831\n",
      "Epoch 59, Loss: 9.310998146350567\n",
      "Epoch 60, Loss: 9.089911020719088\n",
      "Epoch 61, Loss: 10.046034867946918\n",
      "Epoch 62, Loss: 8.815973318540133\n",
      "Epoch 63, Loss: 9.494701935694767\n",
      "Epoch 64, Loss: 11.095064860123854\n",
      "Epoch 65, Loss: 16.465113162994385\n",
      "Epoch 66, Loss: 14.35181527871352\n",
      "Epoch 67, Loss: 11.348050924447866\n",
      "Epoch 68, Loss: 9.224386270229633\n",
      "Epoch 69, Loss: 7.835738017008855\n",
      "Epoch 70, Loss: 8.304123951838566\n",
      "Epoch 71, Loss: 8.477173273379986\n",
      "Epoch 72, Loss: 7.548711263216459\n",
      "Epoch 73, Loss: 8.440685510635376\n",
      "Epoch 74, Loss: 11.486061242910532\n",
      "Epoch 75, Loss: 9.723847224162174\n",
      "Epoch 76, Loss: 10.337296339181753\n",
      "Epoch 77, Loss: 8.53425013102018\n",
      "Epoch 78, Loss: 7.925152760285598\n",
      "Epoch 79, Loss: 10.636108288398155\n",
      "Epoch 80, Loss: 9.872539373544546\n",
      "Epoch 81, Loss: 10.715356515004085\n",
      "Epoch 82, Loss: 10.861815947752733\n",
      "Epoch 83, Loss: 9.45325231552124\n",
      "Epoch 84, Loss: 9.158020092890812\n",
      "Epoch 85, Loss: 7.954766493577224\n",
      "Epoch 86, Loss: 8.669476197316097\n",
      "Epoch 87, Loss: 8.103024115929237\n",
      "Epoch 88, Loss: 7.871621902172382\n",
      "Epoch 89, Loss: 7.832125773796668\n",
      "Epoch 90, Loss: 8.38706660270691\n",
      "Epoch 91, Loss: 9.046093005400438\n",
      "Epoch 92, Loss: 7.864110873295711\n",
      "Epoch 93, Loss: 6.887111957256611\n",
      "Epoch 94, Loss: 6.6314936601198635\n",
      "Epoch 95, Loss: 6.809674483079177\n",
      "Epoch 96, Loss: 6.452947598237258\n",
      "Epoch 97, Loss: 6.1321363265697775\n",
      "Epoch 98, Loss: 6.2693116298088665\n",
      "Epoch 99, Loss: 6.664516540674063\n",
      "Epoch 100, Loss: 6.498976047222431\n",
      "Epoch 101, Loss: 7.648489676989042\n",
      "Epoch 102, Loss: 10.92277271930988\n",
      "Epoch 103, Loss: 10.775896109067476\n",
      "Epoch 104, Loss: 10.50881945169889\n",
      "Epoch 105, Loss: 8.299236847804142\n",
      "Epoch 106, Loss: 6.826679834952722\n",
      "Epoch 107, Loss: 7.06866113956158\n",
      "Epoch 108, Loss: 12.804748241717999\n",
      "Epoch 109, Loss: 10.895480486062857\n",
      "Epoch 110, Loss: 11.200481928311861\n",
      "Epoch 111, Loss: 9.589699983596802\n",
      "Epoch 112, Loss: 9.86703480207003\n",
      "Epoch 113, Loss: 8.995441876924955\n",
      "Epoch 114, Loss: 7.9375759821671705\n",
      "Epoch 115, Loss: 7.559345868917612\n",
      "Epoch 116, Loss: 7.839726833196787\n",
      "Epoch 117, Loss: 9.554624777573805\n",
      "Epoch 118, Loss: 8.215091723662157\n",
      "Epoch 119, Loss: 8.304821582940908\n",
      "Epoch 120, Loss: 7.762993372403658\n",
      "Epoch 121, Loss: 6.848213030741765\n",
      "Epoch 122, Loss: 6.815321867282574\n",
      "Epoch 123, Loss: 6.486183129824125\n",
      "Epoch 124, Loss: 6.360133574559138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:22,192] Trial 74 finished with value: 21.370631294676084 and parameters: {'latent_dim_z1': 15, 'latent_dim_z2': 69, 'hidden_dim': 140, 'epochs': 131, 'causal_reg': 0.9982522293794308, 'learning_rate': 0.0023643267110870603}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125, Loss: 6.4161894504840555\n",
      "Epoch 126, Loss: 6.608107089996338\n",
      "Epoch 127, Loss: 6.715765347847571\n",
      "Epoch 128, Loss: 9.419308332296518\n",
      "Epoch 129, Loss: 7.782672386903029\n",
      "Epoch 130, Loss: 7.042285479032076\n",
      "Epoch 131, Loss: 6.696395617264968\n",
      "Epoch 1, Loss: 543.5399824289175\n",
      "Epoch 2, Loss: 217.87717041602502\n",
      "Epoch 3, Loss: 160.80143238947943\n",
      "Epoch 4, Loss: 132.74830796168402\n",
      "Epoch 5, Loss: 113.27285253084622\n",
      "Epoch 6, Loss: 93.32306495079628\n",
      "Epoch 7, Loss: 85.31905680436354\n",
      "Epoch 8, Loss: 70.54240637559157\n",
      "Epoch 9, Loss: 66.00772608243503\n",
      "Epoch 10, Loss: 64.3316484598013\n",
      "Epoch 11, Loss: 57.026390955998345\n",
      "Epoch 12, Loss: 45.50043905698336\n",
      "Epoch 13, Loss: 36.20014711526724\n",
      "Epoch 14, Loss: 30.941965249868538\n",
      "Epoch 15, Loss: 39.11778626075158\n",
      "Epoch 16, Loss: 34.90721423809345\n",
      "Epoch 17, Loss: 30.676611826970028\n",
      "Epoch 18, Loss: 25.764679541954628\n",
      "Epoch 19, Loss: 26.64662661919227\n",
      "Epoch 20, Loss: 23.331945932828464\n",
      "Epoch 21, Loss: 19.232068685384895\n",
      "Epoch 22, Loss: 16.85310118014996\n",
      "Epoch 23, Loss: 14.35190567603478\n",
      "Epoch 24, Loss: 13.767777552971474\n",
      "Epoch 25, Loss: 11.60346739108746\n",
      "Epoch 26, Loss: 11.089129704695482\n",
      "Epoch 27, Loss: 10.187213329168467\n",
      "Epoch 28, Loss: 10.549801092881422\n",
      "Epoch 29, Loss: 11.293648976546068\n",
      "Epoch 30, Loss: 9.575564274421104\n",
      "Epoch 31, Loss: 9.003171590658335\n",
      "Epoch 32, Loss: 9.146737832289476\n",
      "Epoch 33, Loss: 10.00040916296152\n",
      "Epoch 34, Loss: 14.444998300992525\n",
      "Epoch 35, Loss: 12.973379538609432\n",
      "Epoch 36, Loss: 12.174879147456242\n",
      "Epoch 37, Loss: 13.974407746241642\n",
      "Epoch 38, Loss: 14.06712686098539\n",
      "Epoch 39, Loss: 18.032428539716282\n",
      "Epoch 40, Loss: 13.847825857309195\n",
      "Epoch 41, Loss: 11.239527482252855\n",
      "Epoch 42, Loss: 9.625274676543016\n",
      "Epoch 43, Loss: 8.125891282008244\n",
      "Epoch 44, Loss: 7.425284550740169\n",
      "Epoch 45, Loss: 7.228091349968543\n",
      "Epoch 46, Loss: 6.669826104090764\n",
      "Epoch 47, Loss: 6.659249177345862\n",
      "Epoch 48, Loss: 6.740343552369338\n",
      "Epoch 49, Loss: 6.714318202092097\n",
      "Epoch 50, Loss: 7.057150547321026\n",
      "Epoch 51, Loss: 6.604134009434627\n",
      "Epoch 52, Loss: 7.022435848529522\n",
      "Epoch 53, Loss: 7.798359339053814\n",
      "Epoch 54, Loss: 8.780796692921566\n",
      "Epoch 55, Loss: 8.057213324766893\n",
      "Epoch 56, Loss: 8.216411370497484\n",
      "Epoch 57, Loss: 8.864713687163134\n",
      "Epoch 58, Loss: 8.558262769992535\n",
      "Epoch 59, Loss: 8.341870289582472\n",
      "Epoch 60, Loss: 8.501861040408794\n",
      "Epoch 61, Loss: 8.985359778771034\n",
      "Epoch 62, Loss: 11.71492649958684\n",
      "Epoch 63, Loss: 20.178540468215942\n",
      "Epoch 64, Loss: 15.466102636777437\n",
      "Epoch 65, Loss: 12.811529049506554\n",
      "Epoch 66, Loss: 10.940279043637789\n",
      "Epoch 67, Loss: 10.734572593982403\n",
      "Epoch 68, Loss: 9.683102717766396\n",
      "Epoch 69, Loss: 8.867958288926344\n",
      "Epoch 70, Loss: 9.289795820529644\n",
      "Epoch 71, Loss: 8.369299870270948\n",
      "Epoch 72, Loss: 7.326823454636794\n",
      "Epoch 73, Loss: 7.159585787699773\n",
      "Epoch 74, Loss: 8.36926559301523\n",
      "Epoch 75, Loss: 8.034344819875864\n",
      "Epoch 76, Loss: 7.069488837168767\n",
      "Epoch 77, Loss: 6.856435188880334\n",
      "Epoch 78, Loss: 6.64796258853032\n",
      "Epoch 79, Loss: 6.453882089027991\n",
      "Epoch 80, Loss: 6.410850249803984\n",
      "Epoch 81, Loss: 6.506282916435828\n",
      "Epoch 82, Loss: 6.695009011488694\n",
      "Epoch 83, Loss: 6.644785367525541\n",
      "Epoch 84, Loss: 6.424688999469463\n",
      "Epoch 85, Loss: 6.566578333194439\n",
      "Epoch 86, Loss: 6.5751029161306525\n",
      "Epoch 87, Loss: 6.4356082402742825\n",
      "Epoch 88, Loss: 6.355293970841628\n",
      "Epoch 89, Loss: 6.537444719901452\n",
      "Epoch 90, Loss: 6.51740811421321\n",
      "Epoch 91, Loss: 6.6478788669292745\n",
      "Epoch 92, Loss: 6.611079216003418\n",
      "Epoch 93, Loss: 7.83833017716041\n",
      "Epoch 94, Loss: 9.118326242153461\n",
      "Epoch 95, Loss: 11.694636454949013\n",
      "Epoch 96, Loss: 13.822043290505043\n",
      "Epoch 97, Loss: 12.66776886353126\n",
      "Epoch 98, Loss: 10.79520265872662\n",
      "Epoch 99, Loss: 11.069101957174448\n",
      "Epoch 100, Loss: 9.321606929485615\n",
      "Epoch 101, Loss: 10.105766993302565\n",
      "Epoch 102, Loss: 9.165861129760742\n",
      "Epoch 103, Loss: 7.272878720210149\n",
      "Epoch 104, Loss: 6.808183871782743\n",
      "Epoch 105, Loss: 6.728775281172532\n",
      "Epoch 106, Loss: 6.495769225634062\n",
      "Epoch 107, Loss: 7.012431346453154\n",
      "Epoch 108, Loss: 7.597932778871977\n",
      "Epoch 109, Loss: 7.217980256447425\n",
      "Epoch 110, Loss: 6.716409481488741\n",
      "Epoch 111, Loss: 6.709471280758198\n",
      "Epoch 112, Loss: 6.134926484181331\n",
      "Epoch 113, Loss: 6.155989005015447\n",
      "Epoch 114, Loss: 5.972201127272386\n",
      "Epoch 115, Loss: 5.820096859565148\n",
      "Epoch 116, Loss: 6.08754640359145\n",
      "Epoch 117, Loss: 6.302437158731314\n",
      "Epoch 118, Loss: 8.04014306802016\n",
      "Epoch 119, Loss: 8.094545511099009\n",
      "Epoch 120, Loss: 9.399894861074594\n",
      "Epoch 121, Loss: 10.369233626585741\n",
      "Epoch 122, Loss: 11.366367926964394\n",
      "Epoch 123, Loss: 8.561449637779823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:25,260] Trial 75 finished with value: 22.991002337660444 and parameters: {'latent_dim_z1': 20, 'latent_dim_z2': 77, 'hidden_dim': 157, 'epochs': 124, 'causal_reg': 0.9182086135663666, 'learning_rate': 0.0017422021342043213}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124, Loss: 8.25620227593642\n",
      "Epoch 1, Loss: 358.6416320800781\n",
      "Epoch 2, Loss: 130.6384787926307\n",
      "Epoch 3, Loss: 91.17580971351036\n",
      "Epoch 4, Loss: 72.51047020692091\n",
      "Epoch 5, Loss: 59.62409320244422\n",
      "Epoch 6, Loss: 53.51863802396334\n",
      "Epoch 7, Loss: 46.36129702054537\n",
      "Epoch 8, Loss: 40.88010699932392\n",
      "Epoch 9, Loss: 35.29311425869282\n",
      "Epoch 10, Loss: 33.95852991250845\n",
      "Epoch 11, Loss: 30.84349903693566\n",
      "Epoch 12, Loss: 27.2234693307143\n",
      "Epoch 13, Loss: 24.95968693953294\n",
      "Epoch 14, Loss: 25.175491993243877\n",
      "Epoch 15, Loss: 23.25126996407142\n",
      "Epoch 16, Loss: 22.272869220146767\n",
      "Epoch 17, Loss: 20.966540299929104\n",
      "Epoch 18, Loss: 18.589529514312744\n",
      "Epoch 19, Loss: 19.37247896194458\n",
      "Epoch 20, Loss: 20.488746092869686\n",
      "Epoch 21, Loss: 20.789980778327354\n",
      "Epoch 22, Loss: 17.522655211962185\n",
      "Epoch 23, Loss: 14.787483875568096\n",
      "Epoch 24, Loss: 14.774440838740421\n",
      "Epoch 25, Loss: 14.311141655995296\n",
      "Epoch 26, Loss: 13.286334147820106\n",
      "Epoch 27, Loss: 12.844658338106596\n",
      "Epoch 28, Loss: 12.542785241053654\n",
      "Epoch 29, Loss: 12.717329483765822\n",
      "Epoch 30, Loss: 13.035560864668627\n",
      "Epoch 31, Loss: 14.030007729163536\n",
      "Epoch 32, Loss: 12.786050979907696\n",
      "Epoch 33, Loss: 12.108155929125273\n",
      "Epoch 34, Loss: 10.643776581837582\n",
      "Epoch 35, Loss: 10.317151363079365\n",
      "Epoch 36, Loss: 11.604102354783278\n",
      "Epoch 37, Loss: 10.892356138962965\n",
      "Epoch 38, Loss: 10.025951954034658\n",
      "Epoch 39, Loss: 10.729125756483812\n",
      "Epoch 40, Loss: 12.397962515170757\n",
      "Epoch 41, Loss: 10.09797008220966\n",
      "Epoch 42, Loss: 9.285958491838896\n",
      "Epoch 43, Loss: 9.195716967949501\n",
      "Epoch 44, Loss: 11.025860217901377\n",
      "Epoch 45, Loss: 9.528577969624447\n",
      "Epoch 46, Loss: 9.515886050004225\n",
      "Epoch 47, Loss: 9.213454649998592\n",
      "Epoch 48, Loss: 8.728041538825401\n",
      "Epoch 49, Loss: 8.313877765948956\n",
      "Epoch 50, Loss: 8.497789052816538\n",
      "Epoch 51, Loss: 9.627278639720036\n",
      "Epoch 52, Loss: 9.440202547953678\n",
      "Epoch 53, Loss: 9.865913482812735\n",
      "Epoch 54, Loss: 11.616546410780687\n",
      "Epoch 55, Loss: 11.26757735472459\n",
      "Epoch 56, Loss: 9.22193376834576\n",
      "Epoch 57, Loss: 8.63048505783081\n",
      "Epoch 58, Loss: 10.066003579359789\n",
      "Epoch 59, Loss: 9.842154264450073\n",
      "Epoch 60, Loss: 17.859145017770622\n",
      "Epoch 61, Loss: 11.773559460273155\n",
      "Epoch 62, Loss: 10.898745518464308\n",
      "Epoch 63, Loss: 8.7768347630134\n",
      "Epoch 64, Loss: 8.627969356683584\n",
      "Epoch 65, Loss: 8.268491103098942\n",
      "Epoch 66, Loss: 8.414013184033907\n",
      "Epoch 67, Loss: 9.034040964566744\n",
      "Epoch 68, Loss: 7.911154893728403\n",
      "Epoch 69, Loss: 7.998624819975633\n",
      "Epoch 70, Loss: 7.223273809139545\n",
      "Epoch 71, Loss: 6.863433581132155\n",
      "Epoch 72, Loss: 6.767421722412109\n",
      "Epoch 73, Loss: 6.656839554126446\n",
      "Epoch 74, Loss: 7.0358213644761305\n",
      "Epoch 75, Loss: 7.424700241822463\n",
      "Epoch 76, Loss: 8.240320095649132\n",
      "Epoch 77, Loss: 7.698609700569739\n",
      "Epoch 78, Loss: 8.220136734155508\n",
      "Epoch 79, Loss: 7.849950735385601\n",
      "Epoch 80, Loss: 6.9414786375485935\n",
      "Epoch 81, Loss: 7.143978412334736\n",
      "Epoch 82, Loss: 8.916199702482958\n",
      "Epoch 83, Loss: 9.173799166312584\n",
      "Epoch 84, Loss: 8.378235560197096\n",
      "Epoch 85, Loss: 8.036507716545692\n",
      "Epoch 86, Loss: 8.92459724499629\n",
      "Epoch 87, Loss: 8.305517471753634\n",
      "Epoch 88, Loss: 7.329128852257361\n",
      "Epoch 89, Loss: 6.813637971878052\n",
      "Epoch 90, Loss: 6.288325823270357\n",
      "Epoch 91, Loss: 6.283546667832595\n",
      "Epoch 92, Loss: 6.5561170394604025\n",
      "Epoch 93, Loss: 6.849504690903884\n",
      "Epoch 94, Loss: 7.2904112339019775\n",
      "Epoch 95, Loss: 7.519876553462102\n",
      "Epoch 96, Loss: 7.98333635696998\n",
      "Epoch 97, Loss: 7.233862638473511\n",
      "Epoch 98, Loss: 7.2901117985065165\n",
      "Epoch 99, Loss: 7.5545932696415825\n",
      "Epoch 100, Loss: 7.193222596095159\n",
      "Epoch 101, Loss: 6.931963957273043\n",
      "Epoch 102, Loss: 6.799698939690223\n",
      "Epoch 103, Loss: 6.891496988443228\n",
      "Epoch 104, Loss: 8.347413503206694\n",
      "Epoch 105, Loss: 9.80621024278494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:27,825] Trial 76 finished with value: 18.618522477612746 and parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 69, 'hidden_dim': 175, 'epochs': 107, 'causal_reg': 0.9659868518931616, 'learning_rate': 0.0031889780974843846}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106, Loss: 11.804030858553373\n",
      "Epoch 107, Loss: 9.178023393337543\n",
      "Epoch 1, Loss: 414.6715155381423\n",
      "Epoch 2, Loss: 146.775237596952\n",
      "Epoch 3, Loss: 86.6244085752047\n",
      "Epoch 4, Loss: 73.32925048241249\n",
      "Epoch 5, Loss: 60.32693518125094\n",
      "Epoch 6, Loss: 53.153373351463905\n",
      "Epoch 7, Loss: 47.264207986684944\n",
      "Epoch 8, Loss: 46.33716157766489\n",
      "Epoch 9, Loss: 42.55873401348408\n",
      "Epoch 10, Loss: 41.02713632583618\n",
      "Epoch 11, Loss: 37.69956959210909\n",
      "Epoch 12, Loss: 33.392562279334435\n",
      "Epoch 13, Loss: 29.763392008267918\n",
      "Epoch 14, Loss: 25.685782469235935\n",
      "Epoch 15, Loss: 23.327918419471153\n",
      "Epoch 16, Loss: 24.922928186563347\n",
      "Epoch 17, Loss: 21.959718704223633\n",
      "Epoch 18, Loss: 19.63197532066932\n",
      "Epoch 19, Loss: 18.057137159200813\n",
      "Epoch 20, Loss: 19.276224833268387\n",
      "Epoch 21, Loss: 18.06785016793471\n",
      "Epoch 22, Loss: 16.714377036461464\n",
      "Epoch 23, Loss: 16.7480898637038\n",
      "Epoch 24, Loss: 15.384397433354305\n",
      "Epoch 25, Loss: 15.092830437880297\n",
      "Epoch 26, Loss: 14.1635852593642\n",
      "Epoch 27, Loss: 13.042654440953182\n",
      "Epoch 28, Loss: 13.5574880196498\n",
      "Epoch 29, Loss: 13.388781804304857\n",
      "Epoch 30, Loss: 14.293191983149601\n",
      "Epoch 31, Loss: 12.507106065750122\n",
      "Epoch 32, Loss: 11.54136089178232\n",
      "Epoch 33, Loss: 12.51851826447707\n",
      "Epoch 34, Loss: 11.036364078521729\n",
      "Epoch 35, Loss: 11.525064028226412\n",
      "Epoch 36, Loss: 11.058374808384823\n",
      "Epoch 37, Loss: 11.409311918111948\n",
      "Epoch 38, Loss: 11.18850484261146\n",
      "Epoch 39, Loss: 10.277526690409733\n",
      "Epoch 40, Loss: 10.086385873647837\n",
      "Epoch 41, Loss: 9.583716906034029\n",
      "Epoch 42, Loss: 9.41511189020597\n",
      "Epoch 43, Loss: 8.918677788514357\n",
      "Epoch 44, Loss: 8.38286517216609\n",
      "Epoch 45, Loss: 8.856142337505634\n",
      "Epoch 46, Loss: 8.460115029261662\n",
      "Epoch 47, Loss: 8.826906442642212\n",
      "Epoch 48, Loss: 9.346477013367872\n",
      "Epoch 49, Loss: 10.384146672028761\n",
      "Epoch 50, Loss: 12.786117975528423\n",
      "Epoch 51, Loss: 13.295303234687218\n",
      "Epoch 52, Loss: 12.9948961184575\n",
      "Epoch 53, Loss: 11.63544695193951\n",
      "Epoch 54, Loss: 9.51396656036377\n",
      "Epoch 55, Loss: 8.739519137602587\n",
      "Epoch 56, Loss: 8.073687094908495\n",
      "Epoch 57, Loss: 7.6340580353370076\n",
      "Epoch 58, Loss: 7.463718542685876\n",
      "Epoch 59, Loss: 8.410559067359337\n",
      "Epoch 60, Loss: 8.987801019962017\n",
      "Epoch 61, Loss: 8.773666143417358\n",
      "Epoch 62, Loss: 7.9609069640819845\n",
      "Epoch 63, Loss: 8.010478093073917\n",
      "Epoch 64, Loss: 8.002628143017109\n",
      "Epoch 65, Loss: 7.9090259075164795\n",
      "Epoch 66, Loss: 8.162858394476084\n",
      "Epoch 67, Loss: 9.172713939960186\n",
      "Epoch 68, Loss: 7.62776886499845\n",
      "Epoch 69, Loss: 7.310124470637395\n",
      "Epoch 70, Loss: 7.751959965779231\n",
      "Epoch 71, Loss: 7.894151485883272\n",
      "Epoch 72, Loss: 8.296208528371958\n",
      "Epoch 73, Loss: 7.913684973349938\n",
      "Epoch 74, Loss: 7.959760207396287\n",
      "Epoch 75, Loss: 7.272486796745887\n",
      "Epoch 76, Loss: 7.994616178365854\n",
      "Epoch 77, Loss: 10.729572883019081\n",
      "Epoch 78, Loss: 11.971147335492647\n",
      "Epoch 79, Loss: 9.06093612084022\n",
      "Epoch 80, Loss: 10.30713283098661\n",
      "Epoch 81, Loss: 9.086866085345928\n",
      "Epoch 82, Loss: 7.550565462846023\n",
      "Epoch 83, Loss: 7.327321162590613\n",
      "Epoch 84, Loss: 6.898628895099346\n",
      "Epoch 85, Loss: 6.728065875860361\n",
      "Epoch 86, Loss: 6.768757086533767\n",
      "Epoch 87, Loss: 6.437650056985708\n",
      "Epoch 88, Loss: 6.490312631313618\n",
      "Epoch 89, Loss: 6.529406932684092\n",
      "Epoch 90, Loss: 6.482265564111563\n",
      "Epoch 91, Loss: 6.2924484473008375\n",
      "Epoch 92, Loss: 6.3806993044339695\n",
      "Epoch 93, Loss: 6.575395327347976\n",
      "Epoch 94, Loss: 6.634166699189406\n",
      "Epoch 95, Loss: 6.771731688426091\n",
      "Epoch 96, Loss: 7.944474275295551\n",
      "Epoch 97, Loss: 7.762651131703303\n",
      "Epoch 98, Loss: 9.837015225337101\n",
      "Epoch 99, Loss: 11.508841074430025\n",
      "Epoch 100, Loss: 9.620296423251812\n",
      "Epoch 101, Loss: 9.718821782332201\n",
      "Epoch 102, Loss: 8.593375389392559\n",
      "Epoch 103, Loss: 9.520838755827684\n",
      "Epoch 104, Loss: 7.458127865424523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:30,419] Trial 77 finished with value: 19.586894573195213 and parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 74, 'hidden_dim': 187, 'epochs': 107, 'causal_reg': 0.9594842029387853, 'learning_rate': 0.002783925406294854}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105, Loss: 6.614016459538386\n",
      "Epoch 106, Loss: 6.461454299780039\n",
      "Epoch 107, Loss: 6.62553070141719\n",
      "Epoch 1, Loss: 416.9294820932242\n",
      "Epoch 2, Loss: 174.9061468564547\n",
      "Epoch 3, Loss: 127.8182860154372\n",
      "Epoch 4, Loss: 102.2125977736253\n",
      "Epoch 5, Loss: 84.54518655630258\n",
      "Epoch 6, Loss: 80.4172449845534\n",
      "Epoch 7, Loss: 66.21909090188834\n",
      "Epoch 8, Loss: 61.32620591383714\n",
      "Epoch 9, Loss: 59.93853591038631\n",
      "Epoch 10, Loss: 49.7586188683143\n",
      "Epoch 11, Loss: 42.757038850050705\n",
      "Epoch 12, Loss: 39.52751027620756\n",
      "Epoch 13, Loss: 44.14013884617732\n",
      "Epoch 14, Loss: 56.5626587500939\n",
      "Epoch 15, Loss: 64.70213457254263\n",
      "Epoch 16, Loss: 41.481083356417145\n",
      "Epoch 17, Loss: 31.957143746889553\n",
      "Epoch 18, Loss: 25.77522032077496\n",
      "Epoch 19, Loss: 22.623471846947304\n",
      "Epoch 20, Loss: 19.619839209776657\n",
      "Epoch 21, Loss: 19.064479644481953\n",
      "Epoch 22, Loss: 19.494960748232327\n",
      "Epoch 23, Loss: 17.89884952398447\n",
      "Epoch 24, Loss: 17.287453651428223\n",
      "Epoch 25, Loss: 15.85699620613685\n",
      "Epoch 26, Loss: 18.011911612290604\n",
      "Epoch 27, Loss: 18.008760580649742\n",
      "Epoch 28, Loss: 15.84005069732666\n",
      "Epoch 29, Loss: 16.246999593881462\n",
      "Epoch 30, Loss: 17.89504628915053\n",
      "Epoch 31, Loss: 16.745494182293232\n",
      "Epoch 32, Loss: 16.807287362905647\n",
      "Epoch 33, Loss: 14.069347858428955\n",
      "Epoch 34, Loss: 13.261569499969482\n",
      "Epoch 35, Loss: 15.451117717302763\n",
      "Epoch 36, Loss: 12.549108468569242\n",
      "Epoch 37, Loss: 10.926651055996235\n",
      "Epoch 38, Loss: 9.609137278336744\n",
      "Epoch 39, Loss: 9.321945557227501\n",
      "Epoch 40, Loss: 8.516696306375357\n",
      "Epoch 41, Loss: 8.61253967651954\n",
      "Epoch 42, Loss: 8.297256708145142\n",
      "Epoch 43, Loss: 9.080187760866606\n",
      "Epoch 44, Loss: 12.883164625901442\n",
      "Epoch 45, Loss: 12.99115461569566\n",
      "Epoch 46, Loss: 11.599223998876719\n",
      "Epoch 47, Loss: 12.006505966186523\n",
      "Epoch 48, Loss: 12.217505528376652\n",
      "Epoch 49, Loss: 11.05669116973877\n",
      "Epoch 50, Loss: 12.608497601289015\n",
      "Epoch 51, Loss: 13.756596638606144\n",
      "Epoch 52, Loss: 25.1597308745751\n",
      "Epoch 53, Loss: 20.996702157534084\n",
      "Epoch 54, Loss: 16.907521357903114\n",
      "Epoch 55, Loss: 13.055646639603834\n",
      "Epoch 56, Loss: 10.999731063842773\n",
      "Epoch 57, Loss: 9.374159702887901\n",
      "Epoch 58, Loss: 8.281956287530752\n",
      "Epoch 59, Loss: 7.667442285097563\n",
      "Epoch 60, Loss: 7.766889425424429\n",
      "Epoch 61, Loss: 7.239350062150222\n",
      "Epoch 62, Loss: 6.7717444896698\n",
      "Epoch 63, Loss: 6.717180068676289\n",
      "Epoch 64, Loss: 6.816626805525559\n",
      "Epoch 65, Loss: 7.2711564760941725\n",
      "Epoch 66, Loss: 8.71442486689641\n",
      "Epoch 67, Loss: 9.277305694726797\n",
      "Epoch 68, Loss: 8.71655066196735\n",
      "Epoch 69, Loss: 7.439831403585581\n",
      "Epoch 70, Loss: 9.047433798129742\n",
      "Epoch 71, Loss: 9.369764566421509\n",
      "Epoch 72, Loss: 8.169105162987343\n",
      "Epoch 73, Loss: 8.052785396575928\n",
      "Epoch 74, Loss: 7.608850332406851\n",
      "Epoch 75, Loss: 8.357947404568012\n",
      "Epoch 76, Loss: 9.259087764299833\n",
      "Epoch 77, Loss: 8.605828120158268\n",
      "Epoch 78, Loss: 7.800349088815542\n",
      "Epoch 79, Loss: 7.456860615656926\n",
      "Epoch 80, Loss: 7.433202376732459\n",
      "Epoch 81, Loss: 7.343020897645217\n",
      "Epoch 82, Loss: 7.581231575745803\n",
      "Epoch 83, Loss: 8.106429705253014\n",
      "Epoch 84, Loss: 7.979036752994244\n",
      "Epoch 85, Loss: 8.673928095744206\n",
      "Epoch 86, Loss: 8.171205337230976\n",
      "Epoch 87, Loss: 8.25080237021813\n",
      "Epoch 88, Loss: 10.176976864154522\n",
      "Epoch 89, Loss: 10.665614366531372\n",
      "Epoch 90, Loss: 9.54320977284358\n",
      "Epoch 91, Loss: 7.913547259110671\n",
      "Epoch 92, Loss: 10.534844801976131\n",
      "Epoch 93, Loss: 11.144642628156221\n",
      "Epoch 94, Loss: 9.688822892995981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:32,916] Trial 78 finished with value: 24.4939344369092 and parameters: {'latent_dim_z1': 16, 'latent_dim_z2': 69, 'hidden_dim': 172, 'epochs': 103, 'causal_reg': 0.8656412152597492, 'learning_rate': 0.003490223732954303}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, Loss: 9.13733566724337\n",
      "Epoch 96, Loss: 9.71679008924044\n",
      "Epoch 97, Loss: 9.288721011235165\n",
      "Epoch 98, Loss: 8.807519032404972\n",
      "Epoch 99, Loss: 7.470533682749822\n",
      "Epoch 100, Loss: 6.659098643522996\n",
      "Epoch 101, Loss: 7.115629251186665\n",
      "Epoch 102, Loss: 8.789985069861778\n",
      "Epoch 103, Loss: 8.590332508087158\n",
      "Epoch 1, Loss: 569.6156381460337\n",
      "Epoch 2, Loss: 240.5227922292856\n",
      "Epoch 3, Loss: 182.64954728346603\n",
      "Epoch 4, Loss: 155.85784442608173\n",
      "Epoch 5, Loss: 119.29383960137001\n",
      "Epoch 6, Loss: 103.23178980900691\n",
      "Epoch 7, Loss: 86.06686621445876\n",
      "Epoch 8, Loss: 88.6881172473614\n",
      "Epoch 9, Loss: 66.37695752657376\n",
      "Epoch 10, Loss: 53.6283420416025\n",
      "Epoch 11, Loss: 50.70750544621394\n",
      "Epoch 12, Loss: 55.622080729557915\n",
      "Epoch 13, Loss: 42.02135394169734\n",
      "Epoch 14, Loss: 38.3266721872183\n",
      "Epoch 15, Loss: 32.84681510925293\n",
      "Epoch 16, Loss: 25.80489052259005\n",
      "Epoch 17, Loss: 21.87028642801138\n",
      "Epoch 18, Loss: 20.10381926023043\n",
      "Epoch 19, Loss: 17.144208101125862\n",
      "Epoch 20, Loss: 17.01631736755371\n",
      "Epoch 21, Loss: 15.499637420360859\n",
      "Epoch 22, Loss: 15.970152598160963\n",
      "Epoch 23, Loss: 20.529001712799072\n",
      "Epoch 24, Loss: 18.639253249535194\n",
      "Epoch 25, Loss: 25.494706630706787\n",
      "Epoch 26, Loss: 25.65499496459961\n",
      "Epoch 27, Loss: 21.35827288260827\n",
      "Epoch 28, Loss: 16.323810027195858\n",
      "Epoch 29, Loss: 12.48464690721952\n",
      "Epoch 30, Loss: 10.041344165802002\n",
      "Epoch 31, Loss: 10.614711908193735\n",
      "Epoch 32, Loss: 12.369813368870663\n",
      "Epoch 33, Loss: 15.183204100682186\n",
      "Epoch 34, Loss: 18.8401659818796\n",
      "Epoch 35, Loss: 17.511226177215576\n",
      "Epoch 36, Loss: 13.555108345471895\n",
      "Epoch 37, Loss: 9.87477155832144\n",
      "Epoch 38, Loss: 9.263796769655668\n",
      "Epoch 39, Loss: 8.32194053209745\n",
      "Epoch 40, Loss: 8.378732149417583\n",
      "Epoch 41, Loss: 9.431019452901987\n",
      "Epoch 42, Loss: 8.471628775963417\n",
      "Epoch 43, Loss: 7.899546054693369\n",
      "Epoch 44, Loss: 7.308947599851168\n",
      "Epoch 45, Loss: 8.021984833937426\n",
      "Epoch 46, Loss: 8.650533859546368\n",
      "Epoch 47, Loss: 9.434748227779682\n",
      "Epoch 48, Loss: 8.677446787173931\n",
      "Epoch 49, Loss: 10.653512514554537\n",
      "Epoch 50, Loss: 10.243628887029795\n",
      "Epoch 51, Loss: 8.931875118842491\n",
      "Epoch 52, Loss: 8.706938835290762\n",
      "Epoch 53, Loss: 8.996644992094774\n",
      "Epoch 54, Loss: 9.407228671587431\n",
      "Epoch 55, Loss: 9.78946803166316\n",
      "Epoch 56, Loss: 9.1699839922098\n",
      "Epoch 57, Loss: 8.637691075985249\n",
      "Epoch 58, Loss: 8.412085056304932\n",
      "Epoch 59, Loss: 8.626509959881123\n",
      "Epoch 60, Loss: 9.320141333800096\n",
      "Epoch 61, Loss: 7.833718208166269\n",
      "Epoch 62, Loss: 8.052682326390194\n",
      "Epoch 63, Loss: 8.975374900377714\n",
      "Epoch 64, Loss: 8.797587724832388\n",
      "Epoch 65, Loss: 8.280441430898813\n",
      "Epoch 66, Loss: 7.968770265579224\n",
      "Epoch 67, Loss: 9.351617372952974\n",
      "Epoch 68, Loss: 13.954803686875563\n",
      "Epoch 69, Loss: 11.960364543474638\n",
      "Epoch 70, Loss: 9.968293868578398\n",
      "Epoch 71, Loss: 9.908587198991041\n",
      "Epoch 72, Loss: 8.515465644689707\n",
      "Epoch 73, Loss: 9.41842246055603\n",
      "Epoch 74, Loss: 10.34041428565979\n",
      "Epoch 75, Loss: 10.677785213177021\n",
      "Epoch 76, Loss: 11.698136916527382\n",
      "Epoch 77, Loss: 10.150550053669857\n",
      "Epoch 78, Loss: 8.692669354952299\n",
      "Epoch 79, Loss: 7.617877116570106\n",
      "Epoch 80, Loss: 7.358558563085703\n",
      "Epoch 81, Loss: 7.610017446371225\n",
      "Epoch 82, Loss: 7.099340768960806\n",
      "Epoch 83, Loss: 7.112843880286584\n",
      "Epoch 84, Loss: 7.571641298440786\n",
      "Epoch 85, Loss: 7.321460357079139\n",
      "Epoch 86, Loss: 7.517042086674617\n",
      "Epoch 87, Loss: 7.757855763802161\n",
      "Epoch 88, Loss: 7.139764308929443\n",
      "Epoch 89, Loss: 7.03463495694674\n",
      "Epoch 90, Loss: 6.762805938720703\n",
      "Epoch 91, Loss: 6.927127306277935\n",
      "Epoch 92, Loss: 6.832569654171284\n",
      "Epoch 93, Loss: 6.725616033260639\n",
      "Epoch 94, Loss: 6.908650379914504\n",
      "Epoch 95, Loss: 6.72094911795396\n",
      "Epoch 96, Loss: 7.291602006325355\n",
      "Epoch 97, Loss: 8.755091538796059\n",
      "Epoch 98, Loss: 10.074776869553785\n",
      "Epoch 99, Loss: 9.240235585432787\n",
      "Epoch 100, Loss: 9.285338786932138\n",
      "Epoch 101, Loss: 8.862287301283617\n",
      "Epoch 102, Loss: 9.361312976250282\n",
      "Epoch 103, Loss: 10.203279091761662\n",
      "Epoch 104, Loss: 11.291995782118578\n",
      "Epoch 105, Loss: 12.053605446448692\n",
      "Epoch 106, Loss: 10.362666441844059\n",
      "Epoch 107, Loss: 11.424903557850765\n",
      "Epoch 108, Loss: 9.796607989531298\n",
      "Epoch 109, Loss: 8.199630884023813\n",
      "Epoch 110, Loss: 8.345582815317007\n",
      "Epoch 111, Loss: 8.505590402162992\n",
      "Epoch 112, Loss: 9.55601059473478\n",
      "Epoch 113, Loss: 9.693874304111187\n",
      "Epoch 114, Loss: 9.99666637640733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:36,035] Trial 79 finished with value: 25.197217844388422 and parameters: {'latent_dim_z1': 23, 'latent_dim_z2': 77, 'hidden_dim': 204, 'epochs': 120, 'causal_reg': 0.8029922198867732, 'learning_rate': 0.0019759968258721058}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115, Loss: 10.943019940302921\n",
      "Epoch 116, Loss: 17.446825210864727\n",
      "Epoch 117, Loss: 12.421621267612164\n",
      "Epoch 118, Loss: 11.16740740262545\n",
      "Epoch 119, Loss: 10.394941421655508\n",
      "Epoch 120, Loss: 10.330287768290592\n",
      "Epoch 1, Loss: 468.5215919201191\n",
      "Epoch 2, Loss: 159.39871333195612\n",
      "Epoch 3, Loss: 104.67190361022949\n",
      "Epoch 4, Loss: 85.10355083759015\n",
      "Epoch 5, Loss: 80.70091893122746\n",
      "Epoch 6, Loss: 72.84823065537672\n",
      "Epoch 7, Loss: 64.34824107243465\n",
      "Epoch 8, Loss: 60.72370235736553\n",
      "Epoch 9, Loss: 59.286756662222054\n",
      "Epoch 10, Loss: 50.18990553342379\n",
      "Epoch 11, Loss: 45.020510746882515\n",
      "Epoch 12, Loss: 45.118080579317535\n",
      "Epoch 13, Loss: 42.841941540057846\n",
      "Epoch 14, Loss: 39.17860566652738\n",
      "Epoch 15, Loss: 40.22128875438984\n",
      "Epoch 16, Loss: 39.54167439387395\n",
      "Epoch 17, Loss: 35.60890968029316\n",
      "Epoch 18, Loss: 37.925461769104004\n",
      "Epoch 19, Loss: 50.318280293391304\n",
      "Epoch 20, Loss: 38.80230140686035\n",
      "Epoch 21, Loss: 39.375509408804085\n",
      "Epoch 22, Loss: 28.780997643103966\n",
      "Epoch 23, Loss: 26.865257556621845\n",
      "Epoch 24, Loss: 25.936944851508507\n",
      "Epoch 25, Loss: 25.593410161825325\n",
      "Epoch 26, Loss: 23.94102694438054\n",
      "Epoch 27, Loss: 25.6300027187054\n",
      "Epoch 28, Loss: 33.43748591496394\n",
      "Epoch 29, Loss: 24.86276050714346\n",
      "Epoch 30, Loss: 21.653860348921555\n",
      "Epoch 31, Loss: 20.101901421180138\n",
      "Epoch 32, Loss: 23.672146503741924\n",
      "Epoch 33, Loss: 27.161295193892258\n",
      "Epoch 34, Loss: 24.443719093616192\n",
      "Epoch 35, Loss: 19.996189080751858\n",
      "Epoch 36, Loss: 19.441279961512638\n",
      "Epoch 37, Loss: 17.99358221200796\n",
      "Epoch 38, Loss: 16.54201778998742\n",
      "Epoch 39, Loss: 17.130765914916992\n",
      "Epoch 40, Loss: 19.561281754420353\n",
      "Epoch 41, Loss: 18.17945384979248\n",
      "Epoch 42, Loss: 17.133406932537373\n",
      "Epoch 43, Loss: 14.9868745803833\n",
      "Epoch 44, Loss: 14.904683333176832\n",
      "Epoch 45, Loss: 13.412293764261099\n",
      "Epoch 46, Loss: 12.701358795166016\n",
      "Epoch 47, Loss: 13.0784961443681\n",
      "Epoch 48, Loss: 13.307123807760386\n",
      "Epoch 49, Loss: 13.21147318986746\n",
      "Epoch 50, Loss: 11.400120148291954\n",
      "Epoch 51, Loss: 12.898554911980263\n",
      "Epoch 52, Loss: 15.842027975962711\n",
      "Epoch 53, Loss: 21.081948170295128\n",
      "Epoch 54, Loss: 16.227179784041184\n",
      "Epoch 55, Loss: 13.69827468578632\n",
      "Epoch 56, Loss: 12.25794949898353\n",
      "Epoch 57, Loss: 10.829342163526094\n",
      "Epoch 58, Loss: 10.138398262170645\n",
      "Epoch 59, Loss: 11.836225986480713\n",
      "Epoch 60, Loss: 13.025380428020771\n",
      "Epoch 61, Loss: 13.168489162738506\n",
      "Epoch 62, Loss: 22.05814504623413\n",
      "Epoch 63, Loss: 21.485939521055954\n",
      "Epoch 64, Loss: 14.585895043153029\n",
      "Epoch 65, Loss: 11.423814333402193\n",
      "Epoch 66, Loss: 11.281441046641422\n",
      "Epoch 67, Loss: 10.269610129869902\n",
      "Epoch 68, Loss: 9.82440290084252\n",
      "Epoch 69, Loss: 10.220851292976967\n",
      "Epoch 70, Loss: 10.540495157241821\n",
      "Epoch 71, Loss: 10.498080785457905\n",
      "Epoch 72, Loss: 13.56029174878047\n",
      "Epoch 73, Loss: 18.811060997155998\n",
      "Epoch 74, Loss: 13.32412573007437\n",
      "Epoch 75, Loss: 10.477498604701115\n",
      "Epoch 76, Loss: 9.431351790061363\n",
      "Epoch 77, Loss: 9.896366174404438\n",
      "Epoch 78, Loss: 8.578202229279738\n",
      "Epoch 79, Loss: 8.111227604059073\n",
      "Epoch 80, Loss: 8.374239481412447\n",
      "Epoch 81, Loss: 9.695298671722412\n",
      "Epoch 82, Loss: 9.165130596894484\n",
      "Epoch 83, Loss: 8.74725446334252\n",
      "Epoch 84, Loss: 8.39670757146982\n",
      "Epoch 85, Loss: 8.165717308337872\n",
      "Epoch 86, Loss: 9.173260340323814\n",
      "Epoch 87, Loss: 12.36890015235314\n",
      "Epoch 88, Loss: 10.488547581892748\n",
      "Epoch 89, Loss: 9.049521794685951\n",
      "Epoch 90, Loss: 9.256441153012789\n",
      "Epoch 91, Loss: 8.944652135555561\n",
      "Epoch 92, Loss: 12.824292659759521\n",
      "Epoch 93, Loss: 8.705589129374577\n",
      "Epoch 94, Loss: 10.35180794275724\n",
      "Epoch 95, Loss: 10.546798981153048\n",
      "Epoch 96, Loss: 10.165506509634165\n",
      "Epoch 97, Loss: 9.336787040417011\n",
      "Epoch 98, Loss: 8.487454120929424\n",
      "Epoch 99, Loss: 9.281203178259043\n",
      "Epoch 100, Loss: 8.035598388084999\n",
      "Epoch 101, Loss: 8.102554889825674\n",
      "Epoch 102, Loss: 7.585886184985821\n",
      "Epoch 103, Loss: 7.9210561238802395\n",
      "Epoch 104, Loss: 9.27876091003418\n",
      "Epoch 105, Loss: 10.158862590789795\n",
      "Epoch 106, Loss: 8.635645793034481\n",
      "Epoch 107, Loss: 7.543032517799964\n",
      "Epoch 108, Loss: 7.7871410846710205\n",
      "Epoch 109, Loss: 7.387442625485933\n",
      "Epoch 110, Loss: 6.720066639093252\n",
      "Epoch 111, Loss: 7.0700178513160115\n",
      "Epoch 112, Loss: 7.21785462819613\n",
      "Epoch 113, Loss: 7.311835802518404\n",
      "Epoch 114, Loss: 8.476044214688814\n",
      "Epoch 115, Loss: 8.047217589158278\n",
      "Epoch 116, Loss: 8.50449301646306\n",
      "Epoch 117, Loss: 8.906376343507032\n",
      "Epoch 118, Loss: 9.357699339206402\n",
      "Epoch 119, Loss: 9.931741017561693\n",
      "Epoch 120, Loss: 8.476713437300463\n",
      "Epoch 121, Loss: 7.6888640110309305\n",
      "Epoch 122, Loss: 8.00497421851525\n",
      "Epoch 123, Loss: 8.073290531451885\n",
      "Epoch 124, Loss: 8.411616362058199\n",
      "Epoch 125, Loss: 11.166047939887413\n",
      "Epoch 126, Loss: 9.766266841154833\n",
      "Epoch 127, Loss: 8.52476873764625\n",
      "Epoch 128, Loss: 8.803060311537523\n",
      "Epoch 129, Loss: 8.258199031536396\n",
      "Epoch 130, Loss: 8.982534060111412\n",
      "Epoch 131, Loss: 8.655522749974178\n",
      "Epoch 132, Loss: 10.209376903680655\n",
      "Epoch 133, Loss: 11.208442522929264\n",
      "Epoch 134, Loss: 8.914387757961567\n",
      "Epoch 135, Loss: 7.793535470962524\n",
      "Epoch 136, Loss: 8.219808908609243\n",
      "Epoch 137, Loss: 7.330418476691613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:39,539] Trial 80 finished with value: 21.910194212945676 and parameters: {'latent_dim_z1': 12, 'latent_dim_z2': 74, 'hidden_dim': 155, 'epochs': 146, 'causal_reg': 0.9603449426563367, 'learning_rate': 0.006024539214033466}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138, Loss: 7.061573817179753\n",
      "Epoch 139, Loss: 7.012681796000554\n",
      "Epoch 140, Loss: 7.361562728881836\n",
      "Epoch 141, Loss: 7.05352493432852\n",
      "Epoch 142, Loss: 6.6171863079071045\n",
      "Epoch 143, Loss: 6.757609422390278\n",
      "Epoch 144, Loss: 7.689907972629253\n",
      "Epoch 145, Loss: 7.699343571296105\n",
      "Epoch 146, Loss: 7.4528374671936035\n",
      "Epoch 1, Loss: 325.99668209369366\n",
      "Epoch 2, Loss: 116.17108990595891\n",
      "Epoch 3, Loss: 84.40574528620793\n",
      "Epoch 4, Loss: 64.24579649705153\n",
      "Epoch 5, Loss: 55.06514043074388\n",
      "Epoch 6, Loss: 50.53784722548265\n",
      "Epoch 7, Loss: 51.53460766718938\n",
      "Epoch 8, Loss: 41.55468727992131\n",
      "Epoch 9, Loss: 37.643518741314224\n",
      "Epoch 10, Loss: 35.2324763077956\n",
      "Epoch 11, Loss: 39.98708761655367\n",
      "Epoch 12, Loss: 37.26515872661884\n",
      "Epoch 13, Loss: 34.43814453711877\n",
      "Epoch 14, Loss: 30.084192202641415\n",
      "Epoch 15, Loss: 26.73832475222074\n",
      "Epoch 16, Loss: 26.13523769378662\n",
      "Epoch 17, Loss: 24.654893911801853\n",
      "Epoch 18, Loss: 26.127386753375713\n",
      "Epoch 19, Loss: 22.787783512702354\n",
      "Epoch 20, Loss: 21.45761277125432\n",
      "Epoch 21, Loss: 22.742164134979248\n",
      "Epoch 22, Loss: 19.782962817412155\n",
      "Epoch 23, Loss: 18.539359936347374\n",
      "Epoch 24, Loss: 16.31736082297105\n",
      "Epoch 25, Loss: 18.799384080446682\n",
      "Epoch 26, Loss: 20.008458907787617\n",
      "Epoch 27, Loss: 21.583578439859245\n",
      "Epoch 28, Loss: 18.181707859039307\n",
      "Epoch 29, Loss: 14.785981068244347\n",
      "Epoch 30, Loss: 13.751987934112549\n",
      "Epoch 31, Loss: 13.07019413434542\n",
      "Epoch 32, Loss: 11.931367177229662\n",
      "Epoch 33, Loss: 11.982121064112736\n",
      "Epoch 34, Loss: 13.292063933152418\n",
      "Epoch 35, Loss: 12.567120753801786\n",
      "Epoch 36, Loss: 11.91392652805035\n",
      "Epoch 37, Loss: 15.617693002407368\n",
      "Epoch 38, Loss: 16.343727955451378\n",
      "Epoch 39, Loss: 12.376374391409067\n",
      "Epoch 40, Loss: 10.235106156422543\n",
      "Epoch 41, Loss: 12.65039139527541\n",
      "Epoch 42, Loss: 10.68590809748723\n",
      "Epoch 43, Loss: 12.87869273699247\n",
      "Epoch 44, Loss: 12.219653037878183\n",
      "Epoch 45, Loss: 12.167354106903076\n",
      "Epoch 46, Loss: 11.9423979979295\n",
      "Epoch 47, Loss: 12.176460082714375\n",
      "Epoch 48, Loss: 12.556458803323599\n",
      "Epoch 49, Loss: 12.950491905212402\n",
      "Epoch 50, Loss: 12.362988600364098\n",
      "Epoch 51, Loss: 11.632674510662373\n",
      "Epoch 52, Loss: 9.311705405895527\n",
      "Epoch 53, Loss: 8.338008605516874\n",
      "Epoch 54, Loss: 11.419792358691875\n",
      "Epoch 55, Loss: 12.790150514015785\n",
      "Epoch 56, Loss: 8.937087260759794\n",
      "Epoch 57, Loss: 7.919595663364117\n",
      "Epoch 58, Loss: 7.48590377660898\n",
      "Epoch 59, Loss: 7.750322268559382\n",
      "Epoch 60, Loss: 7.735948911079993\n",
      "Epoch 61, Loss: 7.850994385205782\n",
      "Epoch 62, Loss: 7.731077817770151\n",
      "Epoch 63, Loss: 7.66571516257066\n",
      "Epoch 64, Loss: 8.198215117821327\n",
      "Epoch 65, Loss: 9.614335005099957\n",
      "Epoch 66, Loss: 13.114823524768536\n",
      "Epoch 67, Loss: 9.978693338540884\n",
      "Epoch 68, Loss: 9.217520347008339\n",
      "Epoch 69, Loss: 9.891883538319515\n",
      "Epoch 70, Loss: 8.528108945259682\n",
      "Epoch 71, Loss: 8.91043048638564\n",
      "Epoch 72, Loss: 10.311440174396221\n",
      "Epoch 73, Loss: 9.929397674707266\n",
      "Epoch 74, Loss: 8.842633522473848\n",
      "Epoch 75, Loss: 7.4443825758420505\n",
      "Epoch 76, Loss: 7.574214623524592\n",
      "Epoch 77, Loss: 7.384865045547485\n",
      "Epoch 78, Loss: 7.546413550010095\n",
      "Epoch 79, Loss: 7.142944702735314\n",
      "Epoch 80, Loss: 7.1319322769458475\n",
      "Epoch 81, Loss: 6.983043652314406\n",
      "Epoch 82, Loss: 7.196849181101872\n",
      "Epoch 83, Loss: 6.981655267568735\n",
      "Epoch 84, Loss: 7.668531876343947\n",
      "Epoch 85, Loss: 8.154599208098192\n",
      "Epoch 86, Loss: 7.8581919853503885\n",
      "Epoch 87, Loss: 6.872121003957895\n",
      "Epoch 88, Loss: 7.085532005016621\n",
      "Epoch 89, Loss: 9.705100316267748\n",
      "Epoch 90, Loss: 9.766210115872896\n",
      "Epoch 91, Loss: 10.338728244488056\n",
      "Epoch 92, Loss: 8.831212153801552\n",
      "Epoch 93, Loss: 8.992095635487484\n",
      "Epoch 94, Loss: 7.646064446522639\n",
      "Epoch 95, Loss: 7.085434180039626\n",
      "Epoch 96, Loss: 7.211011042961707\n",
      "Epoch 97, Loss: 6.662199332163884\n",
      "Epoch 98, Loss: 6.618336017315205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:42,148] Trial 81 finished with value: 19.27651968283417 and parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 74, 'hidden_dim': 186, 'epochs': 107, 'causal_reg': 0.9486263225263195, 'learning_rate': 0.0027710698539843114}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, Loss: 7.145517936119666\n",
      "Epoch 100, Loss: 7.763869964159452\n",
      "Epoch 101, Loss: 8.046677369337816\n",
      "Epoch 102, Loss: 7.780890721541184\n",
      "Epoch 103, Loss: 7.0643059473771315\n",
      "Epoch 104, Loss: 6.604853538366465\n",
      "Epoch 105, Loss: 6.545082899240347\n",
      "Epoch 106, Loss: 6.605795603532058\n",
      "Epoch 107, Loss: 7.565633553725022\n",
      "Epoch 1, Loss: 338.75333243149976\n",
      "Epoch 2, Loss: 108.38103851905235\n",
      "Epoch 3, Loss: 78.08185144571158\n",
      "Epoch 4, Loss: 72.54683854029729\n",
      "Epoch 5, Loss: 59.16071679041936\n",
      "Epoch 6, Loss: 55.077882473285385\n",
      "Epoch 7, Loss: 48.28930473327637\n",
      "Epoch 8, Loss: 47.825671416062576\n",
      "Epoch 9, Loss: 39.68587655287523\n",
      "Epoch 10, Loss: 33.543413602388824\n",
      "Epoch 11, Loss: 31.89502606025109\n",
      "Epoch 12, Loss: 30.92662264750554\n",
      "Epoch 13, Loss: 29.408000982724705\n",
      "Epoch 14, Loss: 27.26499605178833\n",
      "Epoch 15, Loss: 24.13608426314134\n",
      "Epoch 16, Loss: 24.868439747737003\n",
      "Epoch 17, Loss: 23.483630033639763\n",
      "Epoch 18, Loss: 23.32961500608004\n",
      "Epoch 19, Loss: 21.546414595383865\n",
      "Epoch 20, Loss: 19.39877858528724\n",
      "Epoch 21, Loss: 18.74107393851647\n",
      "Epoch 22, Loss: 16.616671048677883\n",
      "Epoch 23, Loss: 16.15933293562669\n",
      "Epoch 24, Loss: 19.795611124772293\n",
      "Epoch 25, Loss: 22.318984581873966\n",
      "Epoch 26, Loss: 19.867701090299168\n",
      "Epoch 27, Loss: 15.40313874758207\n",
      "Epoch 28, Loss: 14.959963578444262\n",
      "Epoch 29, Loss: 18.477476083315334\n",
      "Epoch 30, Loss: 14.488245267134447\n",
      "Epoch 31, Loss: 14.422788143157959\n",
      "Epoch 32, Loss: 13.002072187570425\n",
      "Epoch 33, Loss: 12.31341083233173\n",
      "Epoch 34, Loss: 12.210143015934872\n",
      "Epoch 35, Loss: 11.300749448629526\n",
      "Epoch 36, Loss: 11.474160010998066\n",
      "Epoch 37, Loss: 11.641600388746996\n",
      "Epoch 38, Loss: 11.280520659226637\n",
      "Epoch 39, Loss: 11.623800204350399\n",
      "Epoch 40, Loss: 11.294839859008789\n",
      "Epoch 41, Loss: 11.095966174052311\n",
      "Epoch 42, Loss: 13.9226888693296\n",
      "Epoch 43, Loss: 14.207238930922289\n",
      "Epoch 44, Loss: 12.4832412646367\n",
      "Epoch 45, Loss: 10.779463621286245\n",
      "Epoch 46, Loss: 9.378991071994488\n",
      "Epoch 47, Loss: 10.175723901161781\n",
      "Epoch 48, Loss: 10.793939480414757\n",
      "Epoch 49, Loss: 11.181347406827486\n",
      "Epoch 50, Loss: 11.21909680733314\n",
      "Epoch 51, Loss: 12.305301116063045\n",
      "Epoch 52, Loss: 10.663590577932505\n",
      "Epoch 53, Loss: 10.248715437375582\n",
      "Epoch 54, Loss: 13.449041329897367\n",
      "Epoch 55, Loss: 12.354943880668053\n",
      "Epoch 56, Loss: 10.083727066333477\n",
      "Epoch 57, Loss: 10.754980985934917\n",
      "Epoch 58, Loss: 9.411396485108595\n",
      "Epoch 59, Loss: 10.00259516789363\n",
      "Epoch 60, Loss: 11.4044708105234\n",
      "Epoch 61, Loss: 10.310583426402165\n",
      "Epoch 62, Loss: 9.439441974346455\n",
      "Epoch 63, Loss: 9.51170616883498\n",
      "Epoch 64, Loss: 8.633143424987793\n",
      "Epoch 65, Loss: 7.862861358202421\n",
      "Epoch 66, Loss: 7.592545582697942\n",
      "Epoch 67, Loss: 7.253687381744385\n",
      "Epoch 68, Loss: 7.875034148876484\n",
      "Epoch 69, Loss: 9.55702752333421\n",
      "Epoch 70, Loss: 9.5577536546267\n",
      "Epoch 71, Loss: 8.026278477448683\n",
      "Epoch 72, Loss: 7.401904931435218\n",
      "Epoch 73, Loss: 6.872330482189472\n",
      "Epoch 74, Loss: 6.769354783571684\n",
      "Epoch 75, Loss: 7.546390441747812\n",
      "Epoch 76, Loss: 7.727251658072839\n",
      "Epoch 77, Loss: 7.816213479408851\n",
      "Epoch 78, Loss: 7.577277146852934\n",
      "Epoch 79, Loss: 7.510110231546255\n",
      "Epoch 80, Loss: 7.560658399875347\n",
      "Epoch 81, Loss: 7.950634204424345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:44,268] Trial 82 finished with value: 21.574739668455724 and parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 68, 'hidden_dim': 187, 'epochs': 87, 'causal_reg': 0.9161242939677816, 'learning_rate': 0.004274899934220961}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, Loss: 8.029166826835045\n",
      "Epoch 83, Loss: 7.432009568581214\n",
      "Epoch 84, Loss: 7.233515207584087\n",
      "Epoch 85, Loss: 7.7962248141948995\n",
      "Epoch 86, Loss: 7.633029204148513\n",
      "Epoch 87, Loss: 7.613938643382146\n",
      "Epoch 1, Loss: 553.0885863670936\n",
      "Epoch 2, Loss: 219.65906319251428\n",
      "Epoch 3, Loss: 165.3605783902682\n",
      "Epoch 4, Loss: 138.33138553912823\n",
      "Epoch 5, Loss: 122.25894671220045\n",
      "Epoch 6, Loss: 99.29005578848032\n",
      "Epoch 7, Loss: 88.11476634098933\n",
      "Epoch 8, Loss: 78.49685038053073\n",
      "Epoch 9, Loss: 76.66476440429688\n",
      "Epoch 10, Loss: 60.89457101088304\n",
      "Epoch 11, Loss: 57.31679802674513\n",
      "Epoch 12, Loss: 45.96580520043006\n",
      "Epoch 13, Loss: 39.713605660658615\n",
      "Epoch 14, Loss: 38.0285467001108\n",
      "Epoch 15, Loss: 31.019552414233868\n",
      "Epoch 16, Loss: 26.322556495666504\n",
      "Epoch 17, Loss: 22.617268085479736\n",
      "Epoch 18, Loss: 21.380944582132194\n",
      "Epoch 19, Loss: 18.62206169275137\n",
      "Epoch 20, Loss: 16.837947552020733\n",
      "Epoch 21, Loss: 14.880938383249136\n",
      "Epoch 22, Loss: 12.828648750598614\n",
      "Epoch 23, Loss: 12.264799961676964\n",
      "Epoch 24, Loss: 10.499291621721708\n",
      "Epoch 25, Loss: 9.72178288606497\n",
      "Epoch 26, Loss: 9.730346771386953\n",
      "Epoch 27, Loss: 9.770319810280434\n",
      "Epoch 28, Loss: 9.248653980401846\n",
      "Epoch 29, Loss: 8.154117235770592\n",
      "Epoch 30, Loss: 7.619739165672889\n",
      "Epoch 31, Loss: 7.500294263546284\n",
      "Epoch 32, Loss: 7.131156499569233\n",
      "Epoch 33, Loss: 7.280511856079102\n",
      "Epoch 34, Loss: 7.5764923462500935\n",
      "Epoch 35, Loss: 7.524217513891367\n",
      "Epoch 36, Loss: 8.173043727874756\n",
      "Epoch 37, Loss: 8.453979290448702\n",
      "Epoch 38, Loss: 7.989460248213548\n",
      "Epoch 39, Loss: 11.57129276715792\n",
      "Epoch 40, Loss: 14.671701981471134\n",
      "Epoch 41, Loss: 14.951447156759409\n",
      "Epoch 42, Loss: 15.086334962111254\n",
      "Epoch 43, Loss: 21.097782831925613\n",
      "Epoch 44, Loss: 17.696420742915226\n",
      "Epoch 45, Loss: 19.648705555842472\n",
      "Epoch 46, Loss: 13.375276418832632\n",
      "Epoch 47, Loss: 9.136075845131508\n",
      "Epoch 48, Loss: 7.615638806269719\n",
      "Epoch 49, Loss: 6.874970179337722\n",
      "Epoch 50, Loss: 6.811145672431359\n",
      "Epoch 51, Loss: 6.630789720095121\n",
      "Epoch 52, Loss: 6.376613176785982\n",
      "Epoch 53, Loss: 6.284823601062481\n",
      "Epoch 54, Loss: 6.025582460256723\n",
      "Epoch 55, Loss: 5.972251947109516\n",
      "Epoch 56, Loss: 5.780577329488901\n",
      "Epoch 57, Loss: 5.641500656421368\n",
      "Epoch 58, Loss: 5.677986200039204\n",
      "Epoch 59, Loss: 5.814593168405386\n",
      "Epoch 60, Loss: 5.974555033903855\n",
      "Epoch 61, Loss: 6.257945427527795\n",
      "Epoch 62, Loss: 6.544119871579683\n",
      "Epoch 63, Loss: 6.5434100811298075\n",
      "Epoch 64, Loss: 6.691199412712684\n",
      "Epoch 65, Loss: 6.475193665577815\n",
      "Epoch 66, Loss: 6.54809073301462\n",
      "Epoch 67, Loss: 6.490640016702505\n",
      "Epoch 68, Loss: 6.945759333097017\n",
      "Epoch 69, Loss: 7.016059472010686\n",
      "Epoch 70, Loss: 7.498913214756892\n",
      "Epoch 71, Loss: 7.5780995442317085\n",
      "Epoch 72, Loss: 9.516032970868624\n",
      "Epoch 73, Loss: 11.263660669326782\n",
      "Epoch 74, Loss: 13.811427116394043\n",
      "Epoch 75, Loss: 16.0772785957043\n",
      "Epoch 76, Loss: 12.51223872258113\n",
      "Epoch 77, Loss: 10.214097151389488\n",
      "Epoch 78, Loss: 8.985227749897884\n",
      "Epoch 79, Loss: 7.982122421264648\n",
      "Epoch 80, Loss: 7.838573785928579\n",
      "Epoch 81, Loss: 8.469744205474854\n",
      "Epoch 82, Loss: 8.746545498187725\n",
      "Epoch 83, Loss: 7.26370514356173\n",
      "Epoch 84, Loss: 7.695866346359253\n",
      "Epoch 85, Loss: 7.600690841674805\n",
      "Epoch 86, Loss: 6.498849703715398\n",
      "Epoch 87, Loss: 6.1031772173368015\n",
      "Epoch 88, Loss: 5.939986430681669\n",
      "Epoch 89, Loss: 5.677881424243633\n",
      "Epoch 90, Loss: 5.620987451993502\n",
      "Epoch 91, Loss: 5.644606736990122\n",
      "Epoch 92, Loss: 5.552758657015287\n",
      "Epoch 93, Loss: 5.685631990432739\n",
      "Epoch 94, Loss: 5.734761751615084\n",
      "Epoch 95, Loss: 5.791790577081533\n",
      "Epoch 96, Loss: 5.725972780814538\n",
      "Epoch 97, Loss: 5.641909746023325\n",
      "Epoch 98, Loss: 5.689575121952937\n",
      "Epoch 99, Loss: 5.578880034960234\n",
      "Epoch 100, Loss: 5.762384671431321\n",
      "Epoch 101, Loss: 5.787631346629216\n",
      "Epoch 102, Loss: 5.8383026489844685\n",
      "Epoch 103, Loss: 5.930108492191021\n",
      "Epoch 104, Loss: 5.885755172142615\n",
      "Epoch 105, Loss: 5.870747786301833\n",
      "Epoch 106, Loss: 6.011843938093919\n",
      "Epoch 107, Loss: 6.550467289411104\n",
      "Epoch 108, Loss: 6.722962086017315\n",
      "Epoch 109, Loss: 6.699190378189087\n",
      "Epoch 110, Loss: 8.431159496307373\n",
      "Epoch 111, Loss: 14.033437105325552\n",
      "Epoch 112, Loss: 14.099835780950693\n",
      "Epoch 113, Loss: 12.415153356698843\n",
      "Epoch 114, Loss: 9.98843031663161\n",
      "Epoch 115, Loss: 8.437397956848145\n",
      "Epoch 116, Loss: 7.375783021633442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:47,487] Trial 83 finished with value: 22.633533716201786 and parameters: {'latent_dim_z1': 18, 'latent_dim_z2': 73, 'hidden_dim': 233, 'epochs': 123, 'causal_reg': 0.9830629519902453, 'learning_rate': 0.0014282943988640352}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117, Loss: 6.918672433266273\n",
      "Epoch 118, Loss: 6.986319065093994\n",
      "Epoch 119, Loss: 7.26281369649447\n",
      "Epoch 120, Loss: 6.6545990613790655\n",
      "Epoch 121, Loss: 7.087301125893226\n",
      "Epoch 122, Loss: 7.424652906564566\n",
      "Epoch 123, Loss: 7.247771281462449\n",
      "Epoch 1, Loss: 480.63934150108923\n",
      "Epoch 2, Loss: 162.639063174908\n",
      "Epoch 3, Loss: 122.16018823476938\n",
      "Epoch 4, Loss: 96.25053038963905\n",
      "Epoch 5, Loss: 78.1213992192195\n",
      "Epoch 6, Loss: 71.77217798966628\n",
      "Epoch 7, Loss: 66.69706902137169\n",
      "Epoch 8, Loss: 61.2696261039147\n",
      "Epoch 9, Loss: 51.89502305250902\n",
      "Epoch 10, Loss: 47.57051130441519\n",
      "Epoch 11, Loss: 45.73357002551739\n",
      "Epoch 12, Loss: 39.02316078772912\n",
      "Epoch 13, Loss: 39.94195762047401\n",
      "Epoch 14, Loss: 38.38066372504601\n",
      "Epoch 15, Loss: 31.713208491985615\n",
      "Epoch 16, Loss: 29.403072907374455\n",
      "Epoch 17, Loss: 27.333288999704216\n",
      "Epoch 18, Loss: 21.673533476316013\n",
      "Epoch 19, Loss: 19.75482694919293\n",
      "Epoch 20, Loss: 20.201065613673283\n",
      "Epoch 21, Loss: 17.675665928767277\n",
      "Epoch 22, Loss: 20.807709547189567\n",
      "Epoch 23, Loss: 21.588474126962517\n",
      "Epoch 24, Loss: 17.210690608391396\n",
      "Epoch 25, Loss: 18.742728233337402\n",
      "Epoch 26, Loss: 17.096408110398514\n",
      "Epoch 27, Loss: 13.95956930747399\n",
      "Epoch 28, Loss: 13.38750175329355\n",
      "Epoch 29, Loss: 12.249579356266903\n",
      "Epoch 30, Loss: 11.570598015418419\n",
      "Epoch 31, Loss: 11.308361970461332\n",
      "Epoch 32, Loss: 11.751980268038237\n",
      "Epoch 33, Loss: 11.335609601094173\n",
      "Epoch 34, Loss: 10.492823142271776\n",
      "Epoch 35, Loss: 10.683987122315626\n",
      "Epoch 36, Loss: 10.688075689169077\n",
      "Epoch 37, Loss: 9.698431601891151\n",
      "Epoch 38, Loss: 10.278804485614483\n",
      "Epoch 39, Loss: 8.975641158910898\n",
      "Epoch 40, Loss: 9.757333792172945\n",
      "Epoch 41, Loss: 10.63631193454449\n",
      "Epoch 42, Loss: 10.660621826465313\n",
      "Epoch 43, Loss: 10.985982234661396\n",
      "Epoch 44, Loss: 11.244239073533278\n",
      "Epoch 45, Loss: 9.823562768789438\n",
      "Epoch 46, Loss: 8.910283437142006\n",
      "Epoch 47, Loss: 8.061796738551212\n",
      "Epoch 48, Loss: 8.54857087135315\n",
      "Epoch 49, Loss: 10.612945263202374\n",
      "Epoch 50, Loss: 14.72637282885038\n",
      "Epoch 51, Loss: 12.532495205218975\n",
      "Epoch 52, Loss: 13.39808412698599\n",
      "Epoch 53, Loss: 15.101598006028395\n",
      "Epoch 54, Loss: 13.237150155580961\n",
      "Epoch 55, Loss: 12.258381568468534\n",
      "Epoch 56, Loss: 10.734015666521513\n",
      "Epoch 57, Loss: 10.379891047110924\n",
      "Epoch 58, Loss: 9.99606677202078\n",
      "Epoch 59, Loss: 10.54760349713839\n",
      "Epoch 60, Loss: 11.507013944479136\n",
      "Epoch 61, Loss: 11.53296081836407\n",
      "Epoch 62, Loss: 10.09737310042748\n",
      "Epoch 63, Loss: 8.42820926812979\n",
      "Epoch 64, Loss: 7.4678105757786675\n",
      "Epoch 65, Loss: 7.1440307543827934\n",
      "Epoch 66, Loss: 6.810378019626324\n",
      "Epoch 67, Loss: 6.586188022906963\n",
      "Epoch 68, Loss: 6.763874512452346\n",
      "Epoch 69, Loss: 6.724163147119375\n",
      "Epoch 70, Loss: 6.587340409939106\n",
      "Epoch 71, Loss: 6.397235540243296\n",
      "Epoch 72, Loss: 6.494037059637217\n",
      "Epoch 73, Loss: 6.483901885839609\n",
      "Epoch 74, Loss: 8.340938476415781\n",
      "Epoch 75, Loss: 9.174701452255249\n",
      "Epoch 76, Loss: 9.347602679179264\n",
      "Epoch 77, Loss: 9.890107283225426\n",
      "Epoch 78, Loss: 11.422039160361656\n",
      "Epoch 79, Loss: 10.788012577937199\n",
      "Epoch 80, Loss: 9.591351490754347\n",
      "Epoch 81, Loss: 8.01849757708036\n",
      "Epoch 82, Loss: 7.05234538591825\n",
      "Epoch 83, Loss: 6.7514229737795315\n",
      "Epoch 84, Loss: 6.7683539023766155\n",
      "Epoch 85, Loss: 7.932418401424702\n",
      "Epoch 86, Loss: 7.416705406629122\n",
      "Epoch 87, Loss: 6.879051336875329\n",
      "Epoch 88, Loss: 7.652570541088398\n",
      "Epoch 89, Loss: 8.234015666521513\n",
      "Epoch 90, Loss: 8.380198625417856\n",
      "Epoch 91, Loss: 7.956246467737051\n",
      "Epoch 92, Loss: 8.07334067271306\n",
      "Epoch 93, Loss: 8.367406276556162\n",
      "Epoch 94, Loss: 9.850499336536114\n",
      "Epoch 95, Loss: 9.409096240997314\n",
      "Epoch 96, Loss: 8.916076971934391\n",
      "Epoch 97, Loss: 9.653345401470478\n",
      "Epoch 98, Loss: 9.038232014729427\n",
      "Epoch 99, Loss: 7.82938893024738\n",
      "Epoch 100, Loss: 7.770197483209463\n",
      "Epoch 101, Loss: 7.756677114046537\n",
      "Epoch 102, Loss: 7.011862259644729\n",
      "Epoch 103, Loss: 7.065600010064932\n",
      "Epoch 104, Loss: 7.2631878302647515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:50,303] Trial 84 finished with value: 21.967132744553314 and parameters: {'latent_dim_z1': 15, 'latent_dim_z2': 79, 'hidden_dim': 197, 'epochs': 112, 'causal_reg': 0.8296262924180474, 'learning_rate': 0.0028553600766737953}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105, Loss: 8.249340864328238\n",
      "Epoch 106, Loss: 7.352129936218262\n",
      "Epoch 107, Loss: 6.961859189547026\n",
      "Epoch 108, Loss: 6.960959122731135\n",
      "Epoch 109, Loss: 6.7133977229778585\n",
      "Epoch 110, Loss: 6.960786636059101\n",
      "Epoch 111, Loss: 7.482616461240328\n",
      "Epoch 112, Loss: 7.182545020030095\n",
      "Epoch 1, Loss: 461.73476908757135\n",
      "Epoch 2, Loss: 235.4379143348107\n",
      "Epoch 3, Loss: 152.62726857112006\n",
      "Epoch 4, Loss: 125.41123280158409\n",
      "Epoch 5, Loss: 108.56571476276105\n",
      "Epoch 6, Loss: 98.14196117107684\n",
      "Epoch 7, Loss: 89.24590807694655\n",
      "Epoch 8, Loss: 81.60692068246695\n",
      "Epoch 9, Loss: 78.82206065838153\n",
      "Epoch 10, Loss: 71.39053036616399\n",
      "Epoch 11, Loss: 67.61078900557298\n",
      "Epoch 12, Loss: 66.98598729647122\n",
      "Epoch 13, Loss: 59.749604371877815\n",
      "Epoch 14, Loss: 55.44149200732891\n",
      "Epoch 15, Loss: 52.13205469571627\n",
      "Epoch 16, Loss: 49.03527311178354\n",
      "Epoch 17, Loss: 45.908559799194336\n",
      "Epoch 18, Loss: 42.80676680344801\n",
      "Epoch 19, Loss: 41.24154193584736\n",
      "Epoch 20, Loss: 38.085260317875786\n",
      "Epoch 21, Loss: 35.077234781705414\n",
      "Epoch 22, Loss: 32.83698892593384\n",
      "Epoch 23, Loss: 31.05863145681528\n",
      "Epoch 24, Loss: 27.925035550044132\n",
      "Epoch 25, Loss: 26.626384258270264\n",
      "Epoch 26, Loss: 25.006945353287918\n",
      "Epoch 27, Loss: 23.207690422351543\n",
      "Epoch 28, Loss: 21.57234811782837\n",
      "Epoch 29, Loss: 20.07479293529804\n",
      "Epoch 30, Loss: 18.943470808175896\n",
      "Epoch 31, Loss: 18.898899445166954\n",
      "Epoch 32, Loss: 18.16099060498751\n",
      "Epoch 33, Loss: 17.265610511486347\n",
      "Epoch 34, Loss: 15.472030529609093\n",
      "Epoch 35, Loss: 14.688259528233456\n",
      "Epoch 36, Loss: 14.047489092900204\n",
      "Epoch 37, Loss: 13.234240605280949\n",
      "Epoch 38, Loss: 12.53924615566547\n",
      "Epoch 39, Loss: 12.226731043595533\n",
      "Epoch 40, Loss: 12.092194593869722\n",
      "Epoch 41, Loss: 11.74279128588163\n",
      "Epoch 42, Loss: 11.33760771384606\n",
      "Epoch 43, Loss: 10.846526769491343\n",
      "Epoch 44, Loss: 10.323704169346737\n",
      "Epoch 45, Loss: 10.176865155880268\n",
      "Epoch 46, Loss: 9.615376637532162\n",
      "Epoch 47, Loss: 9.257730318949772\n",
      "Epoch 48, Loss: 9.12984611437871\n",
      "Epoch 49, Loss: 9.337940289424015\n",
      "Epoch 50, Loss: 9.262169489493736\n",
      "Epoch 51, Loss: 8.77827028127817\n",
      "Epoch 52, Loss: 8.155052148378813\n",
      "Epoch 53, Loss: 8.08803178713872\n",
      "Epoch 54, Loss: 8.559755600415743\n",
      "Epoch 55, Loss: 7.954709126399114\n",
      "Epoch 56, Loss: 7.530907704279973\n",
      "Epoch 57, Loss: 7.509124664159922\n",
      "Epoch 58, Loss: 7.287297120461097\n",
      "Epoch 59, Loss: 7.276147989126352\n",
      "Epoch 60, Loss: 7.187693449167105\n",
      "Epoch 61, Loss: 7.055286939327534\n",
      "Epoch 62, Loss: 6.732079817698552\n",
      "Epoch 63, Loss: 6.570269841414231\n",
      "Epoch 64, Loss: 6.422817285244282\n",
      "Epoch 65, Loss: 6.373620950258696\n",
      "Epoch 66, Loss: 6.303431822703435\n",
      "Epoch 67, Loss: 6.221827653738169\n",
      "Epoch 68, Loss: 6.245702046614427\n",
      "Epoch 69, Loss: 6.321585123355572\n",
      "Epoch 70, Loss: 6.241387477287879\n",
      "Epoch 71, Loss: 6.391627586804903\n",
      "Epoch 72, Loss: 6.397898563971887\n",
      "Epoch 73, Loss: 6.220784865892851\n",
      "Epoch 74, Loss: 6.461851064975445\n",
      "Epoch 75, Loss: 6.3038814801436205\n",
      "Epoch 76, Loss: 6.179238557815552\n",
      "Epoch 77, Loss: 6.049215445151696\n",
      "Epoch 78, Loss: 6.037099324739897\n",
      "Epoch 79, Loss: 5.88865065574646\n",
      "Epoch 80, Loss: 5.748484501471887\n",
      "Epoch 81, Loss: 5.651958208817702\n",
      "Epoch 82, Loss: 5.615032562842736\n",
      "Epoch 83, Loss: 5.704000858160166\n",
      "Epoch 84, Loss: 5.676483154296875\n",
      "Epoch 85, Loss: 5.592023427669819\n",
      "Epoch 86, Loss: 5.535123476615319\n",
      "Epoch 87, Loss: 5.539245385390061\n",
      "Epoch 88, Loss: 5.5240918306203985\n",
      "Epoch 89, Loss: 5.531315253331111\n",
      "Epoch 90, Loss: 5.54955267906189\n",
      "Epoch 91, Loss: 5.938404376690205\n",
      "Epoch 92, Loss: 5.932712830030001\n",
      "Epoch 93, Loss: 6.022151506864107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:52,588] Trial 85 finished with value: 28.998022293745084 and parameters: {'latent_dim_z1': 11, 'latent_dim_z2': 71, 'hidden_dim': 108, 'epochs': 98, 'causal_reg': 0.9414455316477408, 'learning_rate': 0.0005743656116046292}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, Loss: 5.792850164266733\n",
      "Epoch 95, Loss: 5.7604214411515455\n",
      "Epoch 96, Loss: 5.693044203978318\n",
      "Epoch 97, Loss: 5.974993742429293\n",
      "Epoch 98, Loss: 6.311666286908663\n",
      "Epoch 1, Loss: 1259.520977313702\n",
      "Epoch 2, Loss: 1103.6336881197417\n",
      "Epoch 3, Loss: 988.5723489614634\n",
      "Epoch 4, Loss: 901.9026729877179\n",
      "Epoch 5, Loss: 834.8488206129807\n",
      "Epoch 6, Loss: 781.8254664494441\n",
      "Epoch 7, Loss: 739.3512749305138\n",
      "Epoch 8, Loss: 704.5134958120493\n",
      "Epoch 9, Loss: 675.7268711970403\n",
      "Epoch 10, Loss: 651.7496971717247\n",
      "Epoch 11, Loss: 631.2765743549054\n",
      "Epoch 12, Loss: 613.4643073448768\n",
      "Epoch 13, Loss: 597.712746840257\n",
      "Epoch 14, Loss: 583.4438928457407\n",
      "Epoch 15, Loss: 570.120966397799\n",
      "Epoch 16, Loss: 557.6472338162936\n",
      "Epoch 17, Loss: 545.8296191875751\n",
      "Epoch 18, Loss: 534.7914340679462\n",
      "Epoch 19, Loss: 524.2940257145808\n",
      "Epoch 20, Loss: 514.3246864905724\n",
      "Epoch 21, Loss: 504.6265035775992\n",
      "Epoch 22, Loss: 494.8547146136944\n",
      "Epoch 23, Loss: 485.39789053109973\n",
      "Epoch 24, Loss: 475.7510311420147\n",
      "Epoch 25, Loss: 466.42628772442157\n",
      "Epoch 26, Loss: 456.8662338256836\n",
      "Epoch 27, Loss: 447.72219613882214\n",
      "Epoch 28, Loss: 438.5189854548528\n",
      "Epoch 29, Loss: 429.51912747896637\n",
      "Epoch 30, Loss: 420.7602814894456\n",
      "Epoch 31, Loss: 411.9354723416842\n",
      "Epoch 32, Loss: 403.3433145376352\n",
      "Epoch 33, Loss: 394.9227042564979\n",
      "Epoch 34, Loss: 386.8308959374061\n",
      "Epoch 35, Loss: 378.52935908390924\n",
      "Epoch 36, Loss: 370.96254084660455\n",
      "Epoch 37, Loss: 363.33204885629505\n",
      "Epoch 38, Loss: 356.1264149592473\n",
      "Epoch 39, Loss: 349.24498103215143\n",
      "Epoch 40, Loss: 342.69467984713043\n",
      "Epoch 41, Loss: 336.42117603008563\n",
      "Epoch 42, Loss: 330.46528713519757\n",
      "Epoch 43, Loss: 324.59413264347955\n",
      "Epoch 44, Loss: 319.1960020798903\n",
      "Epoch 45, Loss: 313.9611772390512\n",
      "Epoch 46, Loss: 309.05495658287634\n",
      "Epoch 47, Loss: 304.29583857609674\n",
      "Epoch 48, Loss: 299.7418902470515\n",
      "Epoch 49, Loss: 295.47393065232495\n",
      "Epoch 50, Loss: 291.55777270977313\n",
      "Epoch 51, Loss: 287.38676247229944\n",
      "Epoch 52, Loss: 283.6848878126878\n",
      "Epoch 53, Loss: 280.0215562673715\n",
      "Epoch 54, Loss: 276.5540498586801\n",
      "Epoch 55, Loss: 273.20946942842926\n",
      "Epoch 56, Loss: 270.0076651939979\n",
      "Epoch 57, Loss: 266.9262979947604\n",
      "Epoch 58, Loss: 263.92765279916614\n",
      "Epoch 59, Loss: 261.0773036663349\n",
      "Epoch 60, Loss: 258.2296377328726\n",
      "Epoch 61, Loss: 255.6294455895057\n",
      "Epoch 62, Loss: 253.02591411884015\n",
      "Epoch 63, Loss: 250.5005766061636\n",
      "Epoch 64, Loss: 248.0666016798753\n",
      "Epoch 65, Loss: 245.67365528987006\n",
      "Epoch 66, Loss: 243.45936291034405\n",
      "Epoch 67, Loss: 241.30390959519607\n",
      "Epoch 68, Loss: 239.0962762099046\n",
      "Epoch 69, Loss: 237.01438404963568\n",
      "Epoch 70, Loss: 235.0353601895846\n",
      "Epoch 71, Loss: 233.04384348942682\n",
      "Epoch 72, Loss: 231.12945938110352\n",
      "Epoch 73, Loss: 229.32798532339243\n",
      "Epoch 74, Loss: 227.47808251014123\n",
      "Epoch 75, Loss: 225.744568164532\n",
      "Epoch 76, Loss: 223.98941802978516\n",
      "Epoch 77, Loss: 222.3693337073693\n",
      "Epoch 78, Loss: 220.73779443594125\n",
      "Epoch 79, Loss: 219.2137342599722\n",
      "Epoch 80, Loss: 217.6070706294133\n",
      "Epoch 81, Loss: 216.09804388192984\n",
      "Epoch 82, Loss: 214.57554978590744\n",
      "Epoch 83, Loss: 213.1153828547551\n",
      "Epoch 84, Loss: 211.74943102323093\n",
      "Epoch 85, Loss: 210.35202055711014\n",
      "Epoch 86, Loss: 208.9941802391639\n",
      "Epoch 87, Loss: 207.62534875136154\n",
      "Epoch 88, Loss: 206.36142319899338\n",
      "Epoch 89, Loss: 205.03207749586838\n",
      "Epoch 90, Loss: 203.79911892230695\n",
      "Epoch 91, Loss: 202.63946269108698\n",
      "Epoch 92, Loss: 201.31266285822943\n",
      "Epoch 93, Loss: 200.194884960468\n",
      "Epoch 94, Loss: 199.06899760319635\n",
      "Epoch 95, Loss: 197.9011403597318\n",
      "Epoch 96, Loss: 196.72178620558518\n",
      "Epoch 97, Loss: 195.60147505540115\n",
      "Epoch 98, Loss: 194.6148223876953\n",
      "Epoch 99, Loss: 193.4160649226262\n",
      "Epoch 100, Loss: 192.4401403573843\n",
      "Epoch 101, Loss: 191.39947832547702\n",
      "Epoch 102, Loss: 190.27314787644607\n",
      "Epoch 103, Loss: 189.3234352698693\n",
      "Epoch 104, Loss: 188.2835980928861\n",
      "Epoch 105, Loss: 187.31449684729944\n",
      "Epoch 106, Loss: 186.30050776554987\n",
      "Epoch 107, Loss: 185.34839806189905\n",
      "Epoch 108, Loss: 184.4010834327111\n",
      "Epoch 109, Loss: 183.49259435213528\n",
      "Epoch 110, Loss: 182.5057722238394\n",
      "Epoch 111, Loss: 181.5761266855093\n",
      "Epoch 112, Loss: 180.65403366088867\n",
      "Epoch 113, Loss: 179.78433139507587\n",
      "Epoch 114, Loss: 178.84299439650314\n",
      "Epoch 115, Loss: 177.97306148822491\n",
      "Epoch 116, Loss: 177.1437105032114\n",
      "Epoch 117, Loss: 176.20828217726486\n",
      "Epoch 118, Loss: 175.35791279719427\n",
      "Epoch 119, Loss: 174.6042377765362\n",
      "Epoch 120, Loss: 173.6662424527682\n",
      "Epoch 121, Loss: 172.8574453500601\n",
      "Epoch 122, Loss: 171.99934621957632\n",
      "Epoch 123, Loss: 171.24767567561224\n",
      "Epoch 124, Loss: 170.41565205500677\n",
      "Epoch 125, Loss: 169.69214542095477\n",
      "Epoch 126, Loss: 168.73670886113092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:55,849] Trial 86 finished with value: 324.35125359667046 and parameters: {'latent_dim_z1': 14, 'latent_dim_z2': 69, 'hidden_dim': 167, 'epochs': 133, 'causal_reg': 0.9961947628902257, 'learning_rate': 1.1259271095967962e-05}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127, Loss: 167.96888967660757\n",
      "Epoch 128, Loss: 167.15111013559195\n",
      "Epoch 129, Loss: 166.38745087843674\n",
      "Epoch 130, Loss: 165.63998853243314\n",
      "Epoch 131, Loss: 164.85481467613806\n",
      "Epoch 132, Loss: 164.19566697340744\n",
      "Epoch 133, Loss: 163.35168662438025\n",
      "Epoch 1, Loss: 848.7897444504958\n",
      "Epoch 2, Loss: 589.7442838228666\n",
      "Epoch 3, Loss: 485.7144728440505\n",
      "Epoch 4, Loss: 414.155760544997\n",
      "Epoch 5, Loss: 364.3757054255559\n",
      "Epoch 6, Loss: 332.66865568894605\n",
      "Epoch 7, Loss: 313.5275206932655\n",
      "Epoch 8, Loss: 302.19781787578876\n",
      "Epoch 9, Loss: 295.858893761268\n",
      "Epoch 10, Loss: 292.6202416053185\n",
      "Epoch 11, Loss: 290.9297379713792\n",
      "Epoch 12, Loss: 289.9947897104117\n",
      "Epoch 13, Loss: 289.80928655771106\n",
      "Epoch 14, Loss: 289.48958587646484\n",
      "Epoch 15, Loss: 289.4675005399264\n",
      "Epoch 16, Loss: 289.7078971862793\n",
      "Epoch 17, Loss: 289.4747469975398\n",
      "Epoch 18, Loss: 289.44728939349835\n",
      "Epoch 19, Loss: 289.42222771277795\n",
      "Epoch 20, Loss: 289.61731485220105\n",
      "Epoch 21, Loss: 289.46750112680286\n",
      "Epoch 22, Loss: 289.455504490779\n",
      "Epoch 23, Loss: 289.39821888850287\n",
      "Epoch 24, Loss: 289.44351078913763\n",
      "Epoch 25, Loss: 289.37362318772534\n",
      "Epoch 26, Loss: 289.5330238342285\n",
      "Epoch 27, Loss: 289.3854176447942\n",
      "Epoch 28, Loss: 289.4431266784668\n",
      "Epoch 29, Loss: 289.44331447894757\n",
      "Epoch 30, Loss: 289.6501021751991\n",
      "Epoch 31, Loss: 289.22438489473785\n",
      "Epoch 32, Loss: 289.3259805532602\n",
      "Epoch 33, Loss: 287.8733919583834\n",
      "Epoch 34, Loss: 283.42474658672626\n",
      "Epoch 35, Loss: 257.2641789362981\n",
      "Epoch 36, Loss: 235.08162161020132\n",
      "Epoch 37, Loss: 209.5963369516226\n",
      "Epoch 38, Loss: 175.4850284869854\n",
      "Epoch 39, Loss: 147.42834912813626\n",
      "Epoch 40, Loss: 141.68209251990686\n",
      "Epoch 41, Loss: 119.43887226398175\n",
      "Epoch 42, Loss: 118.16353592505821\n",
      "Epoch 43, Loss: 96.9382266998291\n",
      "Epoch 44, Loss: 84.53430997408353\n",
      "Epoch 45, Loss: 72.90339088439941\n",
      "Epoch 46, Loss: 70.10347659771259\n",
      "Epoch 47, Loss: 69.41852936377892\n",
      "Epoch 48, Loss: 60.265389075646034\n",
      "Epoch 49, Loss: 49.07213335770827\n",
      "Epoch 50, Loss: 60.47731139109685\n",
      "Epoch 51, Loss: 63.02222266564002\n",
      "Epoch 52, Loss: 47.724782723646896\n",
      "Epoch 53, Loss: 44.665059969975395\n",
      "Epoch 54, Loss: 40.81106633406419\n",
      "Epoch 55, Loss: 35.478417763343224\n",
      "Epoch 56, Loss: 32.78880478785588\n",
      "Epoch 57, Loss: 31.605446155254658\n",
      "Epoch 58, Loss: 30.21914397753202\n",
      "Epoch 59, Loss: 30.75631988965548\n",
      "Epoch 60, Loss: 28.174201781933125\n",
      "Epoch 61, Loss: 25.543111324310303\n",
      "Epoch 62, Loss: 26.14999980192918\n",
      "Epoch 63, Loss: 30.499958258408768\n",
      "Epoch 64, Loss: 29.196769530956562\n",
      "Epoch 65, Loss: 27.801042776841385\n",
      "Epoch 66, Loss: 22.874964200533352\n",
      "Epoch 67, Loss: 32.83945167981661\n",
      "Epoch 68, Loss: 30.923286988185\n",
      "Epoch 69, Loss: 23.21604398580698\n",
      "Epoch 70, Loss: 33.19068637261024\n",
      "Epoch 71, Loss: 27.32795139459463\n",
      "Epoch 72, Loss: 18.991262289193962\n",
      "Epoch 73, Loss: 20.50406367962177\n",
      "Epoch 74, Loss: 25.7412166595459\n",
      "Epoch 75, Loss: 38.587809599362885\n",
      "Epoch 76, Loss: 29.435319350315975\n",
      "Epoch 77, Loss: 21.733774992135857\n",
      "Epoch 78, Loss: 17.28317612868089\n",
      "Epoch 79, Loss: 15.300564949329083\n",
      "Epoch 80, Loss: 15.26648736000061\n",
      "Epoch 81, Loss: 14.700785196744478\n",
      "Epoch 82, Loss: 12.714868032015287\n",
      "Epoch 83, Loss: 12.070572981467613\n",
      "Epoch 84, Loss: 11.51710893557622\n",
      "Epoch 85, Loss: 12.656751009134146\n",
      "Epoch 86, Loss: 13.86553641465994\n",
      "Epoch 87, Loss: 12.811968069810133\n",
      "Epoch 88, Loss: 14.792337142504179\n",
      "Epoch 89, Loss: 16.609475759359505\n",
      "Epoch 90, Loss: 12.740168534792387\n",
      "Epoch 91, Loss: 11.26767400594858\n",
      "Epoch 92, Loss: 12.04054386799152\n",
      "Epoch 93, Loss: 12.297350314947275\n",
      "Epoch 94, Loss: 10.842538796938383\n",
      "Epoch 95, Loss: 11.385009894004234\n",
      "Epoch 96, Loss: 12.392958109195416\n",
      "Epoch 97, Loss: 11.906450455005352\n",
      "Epoch 98, Loss: 10.082911858191856\n",
      "Epoch 99, Loss: 12.87408263866718\n",
      "Epoch 100, Loss: 24.849606880774864\n",
      "Epoch 101, Loss: 13.849634482310368\n",
      "Epoch 102, Loss: 10.62786524112408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:53:58,568] Trial 87 finished with value: 135.60001466530326 and parameters: {'latent_dim_z1': 20, 'latent_dim_z2': 76, 'hidden_dim': 226, 'epochs': 106, 'causal_reg': 0.887091768397082, 'learning_rate': 0.008288993342568423}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103, Loss: 9.322774887084961\n",
      "Epoch 104, Loss: 10.381085670911348\n",
      "Epoch 105, Loss: 9.38069721368643\n",
      "Epoch 106, Loss: 8.660286793342003\n",
      "Epoch 1, Loss: 492145.18957989034\n",
      "Epoch 2, Loss: 3660.1551302396333\n",
      "Epoch 3, Loss: 1892.4359882061299\n",
      "Epoch 4, Loss: 1358.9779651348408\n",
      "Epoch 5, Loss: 1140.0987396240234\n",
      "Epoch 6, Loss: 1040.0052478496846\n",
      "Epoch 7, Loss: 1009.7108565110427\n",
      "Epoch 8, Loss: 1171.6764491154597\n",
      "Epoch 9, Loss: 1321.1452847994292\n",
      "Epoch 10, Loss: 1512.3409869854268\n",
      "Epoch 11, Loss: 1618.8629150390625\n",
      "Epoch 12, Loss: 1363.5604506272537\n",
      "Epoch 13, Loss: 2037.888439471905\n",
      "Epoch 14, Loss: 3278.270287146935\n",
      "Epoch 15, Loss: 1996.6282348632812\n",
      "Epoch 16, Loss: 1054.3532022329478\n",
      "Epoch 17, Loss: 937.5376387376052\n",
      "Epoch 18, Loss: 1456.5457129845252\n",
      "Epoch 19, Loss: 2523.4048837515024\n",
      "Epoch 20, Loss: 3551.3123638446514\n",
      "Epoch 21, Loss: 2885.6078045184795\n",
      "Epoch 22, Loss: 2147.259845440204\n",
      "Epoch 23, Loss: 1581.166269155649\n",
      "Epoch 24, Loss: 1832.9671302208533\n",
      "Epoch 25, Loss: 2634.4490966796875\n",
      "Epoch 26, Loss: 2346.0570866511416\n",
      "Epoch 27, Loss: 2288.6258920522837\n",
      "Epoch 28, Loss: 2575.194087101863\n",
      "Epoch 29, Loss: 2943.014878493089\n",
      "Epoch 30, Loss: 1662.550025353065\n",
      "Epoch 31, Loss: 929.9845557579628\n",
      "Epoch 32, Loss: 804.6952362060547\n",
      "Epoch 33, Loss: 1149.494880089393\n",
      "Epoch 34, Loss: 2532.87785926232\n",
      "Epoch 35, Loss: 2396.2247173602764\n",
      "Epoch 36, Loss: 3359.5689603365386\n",
      "Epoch 37, Loss: 3451.9046959510215\n",
      "Epoch 38, Loss: 2606.9339458759014\n",
      "Epoch 39, Loss: 1669.7563652625452\n",
      "Epoch 40, Loss: 1011.8173112135667\n",
      "Epoch 41, Loss: 728.7267755361704\n",
      "Epoch 42, Loss: 662.2466436532827\n",
      "Epoch 43, Loss: 636.1637197641226\n",
      "Epoch 44, Loss: 649.9636160043569\n",
      "Epoch 45, Loss: 634.3712615966797\n",
      "Epoch 46, Loss: 625.9632028432993\n",
      "Epoch 47, Loss: 620.9800567626953\n",
      "Epoch 48, Loss: 632.5615956233098\n",
      "Epoch 49, Loss: 638.3788088285006\n",
      "Epoch 50, Loss: 685.0082456148588\n",
      "Epoch 51, Loss: 1048.916008582482\n",
      "Epoch 52, Loss: 741.0884229219877\n",
      "Epoch 53, Loss: 652.4038837139423\n",
      "Epoch 54, Loss: 1094.2022118201623\n",
      "Epoch 55, Loss: 1668.8741478553186\n",
      "Epoch 56, Loss: 1732.2936471792368\n",
      "Epoch 57, Loss: 1746.5362947904146\n",
      "Epoch 58, Loss: 1876.9748042179988\n",
      "Epoch 59, Loss: 1954.085944542518\n",
      "Epoch 60, Loss: 1920.2765232966497\n",
      "Epoch 61, Loss: 1318.1375943697417\n",
      "Epoch 62, Loss: 1821.0757328913762\n",
      "Epoch 63, Loss: 1812.972170316256\n",
      "Epoch 64, Loss: 1581.200458233173\n",
      "Epoch 65, Loss: 1415.2998938927283\n",
      "Epoch 66, Loss: 1255.5470815805288\n",
      "Epoch 67, Loss: 1102.9257202148438\n",
      "Epoch 68, Loss: 940.249512892503\n",
      "Epoch 69, Loss: 812.909561744103\n",
      "Epoch 70, Loss: 652.7616219153771\n",
      "Epoch 71, Loss: 521.5430626502404\n",
      "Epoch 72, Loss: 485.3696705744817\n",
      "Epoch 73, Loss: 479.18519709660455\n",
      "Epoch 74, Loss: 487.92591681847205\n",
      "Epoch 75, Loss: 478.94637885460486\n",
      "Epoch 76, Loss: 459.17211151123047\n",
      "Epoch 77, Loss: 462.4118130023663\n",
      "Epoch 78, Loss: 481.0648651123047\n",
      "Epoch 79, Loss: 828.3043013352615\n",
      "Epoch 80, Loss: 1777.58740234375\n",
      "Epoch 81, Loss: 1425.5046691894531\n",
      "Epoch 82, Loss: 1424.6493225097656\n",
      "Epoch 83, Loss: 1031.4522963303787\n",
      "Epoch 84, Loss: 637.7982817429763\n",
      "Epoch 85, Loss: 442.1046353853666\n",
      "Epoch 86, Loss: 425.1680380014273\n",
      "Epoch 87, Loss: 412.79841789832483\n",
      "Epoch 88, Loss: 581.0427950345553\n",
      "Epoch 89, Loss: 1295.2304358849158\n",
      "Epoch 90, Loss: 1364.5191192626953\n",
      "Epoch 91, Loss: 1155.610110943134\n",
      "Epoch 92, Loss: 774.0677513709435\n",
      "Epoch 93, Loss: 664.2643620417668\n",
      "Epoch 94, Loss: 427.0368094811073\n",
      "Epoch 95, Loss: 379.7037928654597\n",
      "Epoch 96, Loss: 375.22387049748346\n",
      "Epoch 97, Loss: 394.470217191256\n",
      "Epoch 98, Loss: 895.9608318622296\n",
      "Epoch 99, Loss: 570.1075950035682\n",
      "Epoch 100, Loss: 385.01697951096753\n",
      "Epoch 101, Loss: 362.69156177227313\n",
      "Epoch 102, Loss: 358.2800366328313\n",
      "Epoch 103, Loss: 418.0845900315505\n",
      "Epoch 104, Loss: 804.2097672682542\n",
      "Epoch 105, Loss: 1177.2568781926082\n",
      "Epoch 106, Loss: 845.7417226938101\n",
      "Epoch 107, Loss: 433.57731540386493\n",
      "Epoch 108, Loss: 347.95377584604114\n",
      "Epoch 109, Loss: 337.87264251708984\n",
      "Epoch 110, Loss: 396.4061176593487\n",
      "Epoch 111, Loss: 342.4075094369742\n",
      "Epoch 112, Loss: 331.7300420907828\n",
      "Epoch 113, Loss: 337.5879226097694\n",
      "Epoch 114, Loss: 323.4785602276142\n",
      "Epoch 115, Loss: 320.1041702857384\n",
      "Epoch 116, Loss: 326.6039569561298\n",
      "Epoch 117, Loss: 351.3905921349159\n",
      "Epoch 118, Loss: 378.6003538278433\n",
      "Epoch 119, Loss: 528.3433286226713\n",
      "Epoch 120, Loss: 365.99407372107873\n",
      "Epoch 121, Loss: 652.2665581336388\n",
      "Epoch 122, Loss: 1774.356461745042\n",
      "Epoch 123, Loss: 1134.7023479755107\n",
      "Epoch 124, Loss: 365.9769715529222\n",
      "Epoch 125, Loss: 305.85570467435394\n",
      "Epoch 126, Loss: 289.62733048659106\n",
      "Epoch 127, Loss: 290.6730064978966\n",
      "Epoch 128, Loss: 290.4232752873347\n",
      "Epoch 129, Loss: 282.8502657963679\n",
      "Epoch 130, Loss: 367.04734802246094\n",
      "Epoch 131, Loss: 816.2603595440204\n",
      "Epoch 132, Loss: 387.9359788161058\n",
      "Epoch 133, Loss: 441.27260530911957\n",
      "Epoch 134, Loss: 366.36281497661884\n",
      "Epoch 135, Loss: 665.8609845088079\n",
      "Epoch 136, Loss: 610.9406292255109\n",
      "Epoch 137, Loss: 304.05165100097656\n",
      "Epoch 138, Loss: 259.6931337209848\n",
      "Epoch 139, Loss: 251.47516309298\n",
      "Epoch 140, Loss: 249.47791583721454\n",
      "Epoch 141, Loss: 248.18601843026968\n",
      "Epoch 142, Loss: 245.74571462777945\n",
      "Epoch 143, Loss: 258.7924707852877\n",
      "Epoch 144, Loss: 246.16894912719727\n",
      "Epoch 145, Loss: 260.0147129939153\n",
      "Epoch 146, Loss: 260.58543572059045\n",
      "Epoch 147, Loss: 243.9339347252479\n",
      "Epoch 148, Loss: 237.1735942547138\n",
      "Epoch 149, Loss: 236.39575224656326\n",
      "Epoch 150, Loss: 233.2307260953463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:54:02,348] Trial 88 finished with value: 4251.330341192386 and parameters: {'latent_dim_z1': 17, 'latent_dim_z2': 78, 'hidden_dim': 177, 'epochs': 152, 'causal_reg': 0.949971773219384, 'learning_rate': 0.016221717868097146}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151, Loss: 233.69889978262094\n",
      "Epoch 152, Loss: 236.27301377516525\n",
      "Epoch 1, Loss: 432.8967100290152\n",
      "Epoch 2, Loss: 205.4758638235239\n",
      "Epoch 3, Loss: 141.29530715942383\n",
      "Epoch 4, Loss: 114.68396553626427\n",
      "Epoch 5, Loss: 99.6347753084623\n",
      "Epoch 6, Loss: 87.38129733158992\n",
      "Epoch 7, Loss: 79.02457178556003\n",
      "Epoch 8, Loss: 70.75891890892616\n",
      "Epoch 9, Loss: 62.75888927166279\n",
      "Epoch 10, Loss: 54.93845140016996\n",
      "Epoch 11, Loss: 48.2621033008282\n",
      "Epoch 12, Loss: 43.26464575987596\n",
      "Epoch 13, Loss: 40.720862241891716\n",
      "Epoch 14, Loss: 34.91777420043945\n",
      "Epoch 15, Loss: 31.218843570122353\n",
      "Epoch 16, Loss: 28.661139011383057\n",
      "Epoch 17, Loss: 24.99174506847675\n",
      "Epoch 18, Loss: 22.49273234147292\n",
      "Epoch 19, Loss: 21.558888838841366\n",
      "Epoch 20, Loss: 18.454932139470028\n",
      "Epoch 21, Loss: 18.45937285056481\n",
      "Epoch 22, Loss: 16.81695817067073\n",
      "Epoch 23, Loss: 14.043872833251953\n",
      "Epoch 24, Loss: 13.155747927152193\n",
      "Epoch 25, Loss: 12.01237496962914\n",
      "Epoch 26, Loss: 11.323865780463585\n",
      "Epoch 27, Loss: 11.076260603391207\n",
      "Epoch 28, Loss: 10.480603456497192\n",
      "Epoch 29, Loss: 10.821951315953182\n",
      "Epoch 30, Loss: 9.590062416516817\n",
      "Epoch 31, Loss: 9.9719269825862\n",
      "Epoch 32, Loss: 9.162646953876202\n",
      "Epoch 33, Loss: 8.423117050757774\n",
      "Epoch 34, Loss: 7.804799685111413\n",
      "Epoch 35, Loss: 8.125975297047543\n",
      "Epoch 36, Loss: 9.390087861281176\n",
      "Epoch 37, Loss: 9.015666063015278\n",
      "Epoch 38, Loss: 7.888905433508066\n",
      "Epoch 39, Loss: 7.905111147807195\n",
      "Epoch 40, Loss: 7.56796750655541\n",
      "Epoch 41, Loss: 6.831176794492281\n",
      "Epoch 42, Loss: 6.812410721412072\n",
      "Epoch 43, Loss: 7.082468693072979\n",
      "Epoch 44, Loss: 6.828105156238262\n",
      "Epoch 45, Loss: 6.594294364635761\n",
      "Epoch 46, Loss: 6.399727913049551\n",
      "Epoch 47, Loss: 6.326095085877639\n",
      "Epoch 48, Loss: 6.422554658009456\n",
      "Epoch 49, Loss: 6.639267884767973\n",
      "Epoch 50, Loss: 6.477570662131677\n",
      "Epoch 51, Loss: 6.559379907754751\n",
      "Epoch 52, Loss: 6.428090480657724\n",
      "Epoch 53, Loss: 6.456160160211416\n",
      "Epoch 54, Loss: 6.888380179038415\n",
      "Epoch 55, Loss: 7.0112086076002855\n",
      "Epoch 56, Loss: 6.776875037413377\n",
      "Epoch 57, Loss: 7.254326765353863\n",
      "Epoch 58, Loss: 7.063630507542537\n",
      "Epoch 59, Loss: 6.6475923244769755\n",
      "Epoch 60, Loss: 6.970789597584651\n",
      "Epoch 61, Loss: 7.96347531905541\n",
      "Epoch 62, Loss: 7.095095689480122\n",
      "Epoch 63, Loss: 6.579024131481464\n",
      "Epoch 64, Loss: 6.325235641919649\n",
      "Epoch 65, Loss: 6.027486984546368\n",
      "Epoch 66, Loss: 5.9439880664532\n",
      "Epoch 67, Loss: 5.879720174349272\n",
      "Epoch 68, Loss: 5.901341695051927\n",
      "Epoch 69, Loss: 5.953816799017099\n",
      "Epoch 70, Loss: 6.232540644132174\n",
      "Epoch 71, Loss: 6.269169770754301\n",
      "Epoch 72, Loss: 6.347254551373995\n",
      "Epoch 73, Loss: 6.240613753979023\n",
      "Epoch 74, Loss: 6.298295717972976\n",
      "Epoch 75, Loss: 7.321430426377517\n",
      "Epoch 76, Loss: 7.8014244849865255\n",
      "Epoch 77, Loss: 10.21627583870521\n",
      "Epoch 78, Loss: 11.564154789997982\n",
      "Epoch 79, Loss: 11.48627545283391\n",
      "Epoch 80, Loss: 11.273960003486046\n",
      "Epoch 81, Loss: 9.999988977725689\n",
      "Epoch 82, Loss: 9.370966874636137\n",
      "Epoch 83, Loss: 8.054643630981445\n",
      "Epoch 84, Loss: 6.97517362007728\n",
      "Epoch 85, Loss: 6.434157243141761\n",
      "Epoch 86, Loss: 6.576214716984675\n",
      "Epoch 87, Loss: 6.269215767200176\n",
      "Epoch 88, Loss: 6.041807981637808\n",
      "Epoch 89, Loss: 5.847731516911433\n",
      "Epoch 90, Loss: 5.632768594301664\n",
      "Epoch 91, Loss: 5.587599149117103\n",
      "Epoch 92, Loss: 5.580989837646484\n",
      "Epoch 93, Loss: 5.547546881895799\n",
      "Epoch 94, Loss: 5.4931024221273566\n",
      "Epoch 95, Loss: 5.531239087765034\n",
      "Epoch 96, Loss: 5.533178971363948\n",
      "Epoch 97, Loss: 5.453400025000939\n",
      "Epoch 98, Loss: 5.602439678632296\n",
      "Epoch 99, Loss: 5.693646467649019\n",
      "Epoch 100, Loss: 5.6653586350954495\n",
      "Epoch 101, Loss: 5.719615954619187\n",
      "Epoch 102, Loss: 5.701082247954148\n",
      "Epoch 103, Loss: 5.745212701650766\n",
      "Epoch 104, Loss: 5.894856269542988\n",
      "Epoch 105, Loss: 6.195322605279776\n",
      "Epoch 106, Loss: 6.696939376684336\n",
      "Epoch 107, Loss: 7.285164979787973\n",
      "Epoch 108, Loss: 7.653123873930711\n",
      "Epoch 109, Loss: 7.9066610886500435\n",
      "Epoch 110, Loss: 7.872970562714797\n",
      "Epoch 111, Loss: 7.114885348540086\n",
      "Epoch 112, Loss: 6.760565317594088\n",
      "Epoch 113, Loss: 6.717141976723304\n",
      "Epoch 114, Loss: 6.917364065463726\n",
      "Epoch 115, Loss: 6.426794583980854\n",
      "Epoch 116, Loss: 6.267672300338745\n",
      "Epoch 117, Loss: 6.229751880352314\n",
      "Epoch 118, Loss: 6.4070712603055515\n",
      "Epoch 119, Loss: 6.537026772132287\n",
      "Epoch 120, Loss: 6.6903851215655985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:54:05,212] Trial 89 finished with value: 19.406308852564862 and parameters: {'latent_dim_z1': 12, 'latent_dim_z2': 62, 'hidden_dim': 125, 'epochs': 127, 'causal_reg': 0.8115843059645204, 'learning_rate': 0.0008222443926624415}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121, Loss: 6.69609442124\n",
      "Epoch 122, Loss: 6.652429012151865\n",
      "Epoch 123, Loss: 6.41484154187716\n",
      "Epoch 124, Loss: 6.262400847214919\n",
      "Epoch 125, Loss: 6.300750530683077\n",
      "Epoch 126, Loss: 6.341300395818857\n",
      "Epoch 127, Loss: 6.682252388734084\n",
      "Epoch 1, Loss: 533.7304361783541\n",
      "Epoch 2, Loss: 144.48549798818735\n",
      "Epoch 3, Loss: 105.94295824491061\n",
      "Epoch 4, Loss: 96.58475685119629\n",
      "Epoch 5, Loss: 85.60442733764648\n",
      "Epoch 6, Loss: 82.41838990725003\n",
      "Epoch 7, Loss: 69.47389624669002\n",
      "Epoch 8, Loss: 63.3431428028987\n",
      "Epoch 9, Loss: 60.60739590571477\n",
      "Epoch 10, Loss: 53.01740136513343\n",
      "Epoch 11, Loss: 49.15052978809063\n",
      "Epoch 12, Loss: 45.85479710652278\n",
      "Epoch 13, Loss: 43.70389072711651\n",
      "Epoch 14, Loss: 43.642748429225044\n",
      "Epoch 15, Loss: 42.4646193430974\n",
      "Epoch 16, Loss: 48.60207293583797\n",
      "Epoch 17, Loss: 37.37799406051636\n",
      "Epoch 18, Loss: 36.621880531311035\n",
      "Epoch 19, Loss: 34.67472531245305\n",
      "Epoch 20, Loss: 30.81281981101403\n",
      "Epoch 21, Loss: 33.12678887293889\n",
      "Epoch 22, Loss: 41.63952739422138\n",
      "Epoch 23, Loss: 42.56040730843177\n",
      "Epoch 24, Loss: 35.9940406359159\n",
      "Epoch 25, Loss: 41.93298002389761\n",
      "Epoch 26, Loss: 30.47456011405358\n",
      "Epoch 27, Loss: 26.534864058861366\n",
      "Epoch 28, Loss: 26.341031918158897\n",
      "Epoch 29, Loss: 22.497414332169754\n",
      "Epoch 30, Loss: 23.01136093873244\n",
      "Epoch 31, Loss: 20.130426076742317\n",
      "Epoch 32, Loss: 22.483455107762264\n",
      "Epoch 33, Loss: 25.65080275902381\n",
      "Epoch 34, Loss: 21.610671887030968\n",
      "Epoch 35, Loss: 20.414822981907772\n",
      "Epoch 36, Loss: 23.52484648044293\n",
      "Epoch 37, Loss: 20.220941213461067\n",
      "Epoch 38, Loss: 20.631880503434402\n",
      "Epoch 39, Loss: 17.329718112945557\n",
      "Epoch 40, Loss: 20.271663115574764\n",
      "Epoch 41, Loss: 16.691000131460335\n",
      "Epoch 42, Loss: 17.471982405735897\n",
      "Epoch 43, Loss: 18.162940979003906\n",
      "Epoch 44, Loss: 19.725030715648945\n",
      "Epoch 45, Loss: 23.939484192774845\n",
      "Epoch 46, Loss: 16.03640659038837\n",
      "Epoch 47, Loss: 15.784313311943642\n",
      "Epoch 48, Loss: 13.601323201106144\n",
      "Epoch 49, Loss: 13.760954820192778\n",
      "Epoch 50, Loss: 13.78432901089008\n",
      "Epoch 51, Loss: 12.300714749556322\n",
      "Epoch 52, Loss: 21.898643383613\n",
      "Epoch 53, Loss: 37.862382668715256\n",
      "Epoch 54, Loss: 18.51750142757709\n",
      "Epoch 55, Loss: 14.67302967951848\n",
      "Epoch 56, Loss: 11.713635664719801\n",
      "Epoch 57, Loss: 12.596848029356737\n",
      "Epoch 58, Loss: 11.852722094609188\n",
      "Epoch 59, Loss: 11.846894154181847\n",
      "Epoch 60, Loss: 12.09466527058528\n",
      "Epoch 61, Loss: 11.310183066588182\n",
      "Epoch 62, Loss: 15.956199719355656\n",
      "Epoch 63, Loss: 12.939550289740929\n",
      "Epoch 64, Loss: 11.57533939068134\n",
      "Epoch 65, Loss: 12.795036957814144\n",
      "Epoch 66, Loss: 10.717861047157875\n",
      "Epoch 67, Loss: 9.510130937282856\n",
      "Epoch 68, Loss: 9.841998430398794\n",
      "Epoch 69, Loss: 11.01862327869122\n",
      "Epoch 70, Loss: 10.186814601604755\n",
      "Epoch 71, Loss: 9.834129461875328\n",
      "Epoch 72, Loss: 11.24625275685237\n",
      "Epoch 73, Loss: 10.322647938361534\n",
      "Epoch 74, Loss: 9.66719676898076\n",
      "Epoch 75, Loss: 10.675438110645\n",
      "Epoch 76, Loss: 10.126077505258413\n",
      "Epoch 77, Loss: 10.85972941838778\n",
      "Epoch 78, Loss: 11.774559589532705\n",
      "Epoch 79, Loss: 13.918794778677134\n",
      "Epoch 80, Loss: 10.335377308038565\n",
      "Epoch 81, Loss: 9.598096829194288\n",
      "Epoch 82, Loss: 9.392898321151733\n",
      "Epoch 83, Loss: 9.713635811438927\n",
      "Epoch 84, Loss: 9.985211775853085\n",
      "Epoch 85, Loss: 9.114338324620174\n",
      "Epoch 86, Loss: 8.466572266358595\n",
      "Epoch 87, Loss: 9.287773480782143\n",
      "Epoch 88, Loss: 8.811157630040096\n",
      "Epoch 89, Loss: 12.42946483538701\n",
      "Epoch 90, Loss: 13.969424981337328\n",
      "Epoch 91, Loss: 11.948919993180494\n",
      "Epoch 92, Loss: 13.723486881989698\n",
      "Epoch 93, Loss: 12.24528562105619\n",
      "Epoch 94, Loss: 12.794424570523775\n",
      "Epoch 95, Loss: 11.525450981580294\n",
      "Epoch 96, Loss: 10.890677195328932\n",
      "Epoch 97, Loss: 10.093137337611271\n",
      "Epoch 98, Loss: 10.460753697615404\n",
      "Epoch 99, Loss: 12.87955876497122\n",
      "Epoch 100, Loss: 10.326517691979042\n",
      "Epoch 101, Loss: 10.635569609128511\n",
      "Epoch 102, Loss: 9.434824741803682\n",
      "Epoch 103, Loss: 10.79902168420645\n",
      "Epoch 104, Loss: 14.454594722160927\n",
      "Epoch 105, Loss: 13.623463025459877\n",
      "Epoch 106, Loss: 10.586674781946035\n",
      "Epoch 107, Loss: 9.573272338280312\n",
      "Epoch 108, Loss: 9.820462648685162\n",
      "Epoch 109, Loss: 9.861697325339684\n",
      "Epoch 110, Loss: 9.228014010649462\n",
      "Epoch 111, Loss: 8.197383147019606\n",
      "Epoch 112, Loss: 7.856756357046274\n",
      "Epoch 113, Loss: 9.639930358299843\n",
      "Epoch 114, Loss: 9.829685412920439\n",
      "Epoch 115, Loss: 8.275323684398945\n",
      "Epoch 116, Loss: 8.339752527383657\n",
      "Epoch 117, Loss: 7.417275483791645\n",
      "Epoch 118, Loss: 6.993167528739343\n",
      "Epoch 119, Loss: 7.245478465006902\n",
      "Epoch 120, Loss: 7.860871370022114\n",
      "Epoch 121, Loss: 7.565183584506695\n",
      "Epoch 122, Loss: 7.406027151988103\n",
      "Epoch 123, Loss: 7.287994072987483\n",
      "Epoch 124, Loss: 8.047606523220356\n",
      "Epoch 125, Loss: 14.480657320756178\n",
      "Epoch 126, Loss: 17.22902272297786\n",
      "Epoch 127, Loss: 12.90797129044166\n",
      "Epoch 128, Loss: 12.237053834475004\n",
      "Epoch 129, Loss: 9.234571933746338\n",
      "Epoch 130, Loss: 7.990732889909011\n",
      "Epoch 131, Loss: 8.843034139046303\n",
      "Epoch 132, Loss: 7.893542032975417\n",
      "Epoch 133, Loss: 7.718422211133516\n",
      "Epoch 134, Loss: 9.197287486149715\n",
      "Epoch 135, Loss: 12.96675836122953\n",
      "Epoch 136, Loss: 15.573307734269362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:54:08,463] Trial 90 finished with value: 24.01245410461325 and parameters: {'latent_dim_z1': 14, 'latent_dim_z2': 75, 'hidden_dim': 84, 'epochs': 145, 'causal_reg': 0.8912548202356402, 'learning_rate': 0.00631879279030299}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137, Loss: 21.57943344116211\n",
      "Epoch 138, Loss: 13.380628384076632\n",
      "Epoch 139, Loss: 10.943862804999718\n",
      "Epoch 140, Loss: 10.789762735366821\n",
      "Epoch 141, Loss: 9.55374237207266\n",
      "Epoch 142, Loss: 9.710087244327251\n",
      "Epoch 143, Loss: 11.029607699467586\n",
      "Epoch 144, Loss: 8.866625657448402\n",
      "Epoch 145, Loss: 8.619586779521061\n",
      "Epoch 1, Loss: 386.47304446880634\n",
      "Epoch 2, Loss: 150.75078758826623\n",
      "Epoch 3, Loss: 105.9954930818998\n",
      "Epoch 4, Loss: 89.89369590465839\n",
      "Epoch 5, Loss: 76.146636229295\n",
      "Epoch 6, Loss: 61.61159324645996\n",
      "Epoch 7, Loss: 55.95305809607873\n",
      "Epoch 8, Loss: 51.776769124544586\n",
      "Epoch 9, Loss: 45.2558410351093\n",
      "Epoch 10, Loss: 42.34102447216328\n",
      "Epoch 11, Loss: 49.87834886404184\n",
      "Epoch 12, Loss: 39.07639239384578\n",
      "Epoch 13, Loss: 32.318159140073334\n",
      "Epoch 14, Loss: 28.734855468456562\n",
      "Epoch 15, Loss: 26.762209452115574\n",
      "Epoch 16, Loss: 25.195377753331112\n",
      "Epoch 17, Loss: 22.616195678710938\n",
      "Epoch 18, Loss: 20.286217120977547\n",
      "Epoch 19, Loss: 21.405262983762302\n",
      "Epoch 20, Loss: 22.560915103325478\n",
      "Epoch 21, Loss: 24.367579019986668\n",
      "Epoch 22, Loss: 20.6799714748676\n",
      "Epoch 23, Loss: 20.643393186422493\n",
      "Epoch 24, Loss: 19.00877908559946\n",
      "Epoch 25, Loss: 16.530927731440617\n",
      "Epoch 26, Loss: 15.89116096496582\n",
      "Epoch 27, Loss: 16.627490777235764\n",
      "Epoch 28, Loss: 14.805018571706919\n",
      "Epoch 29, Loss: 12.202745584341196\n",
      "Epoch 30, Loss: 12.943166402670053\n",
      "Epoch 31, Loss: 13.993705382713905\n",
      "Epoch 32, Loss: 11.45076770048875\n",
      "Epoch 33, Loss: 11.414701535151554\n",
      "Epoch 34, Loss: 14.011945926226103\n",
      "Epoch 35, Loss: 12.242612141829271\n",
      "Epoch 36, Loss: 11.602828649374155\n",
      "Epoch 37, Loss: 10.150299237324642\n",
      "Epoch 38, Loss: 9.814282655715942\n",
      "Epoch 39, Loss: 12.556695277874287\n",
      "Epoch 40, Loss: 23.764995611630955\n",
      "Epoch 41, Loss: 17.342258545068596\n",
      "Epoch 42, Loss: 13.321295756560106\n",
      "Epoch 43, Loss: 10.973398740474995\n",
      "Epoch 44, Loss: 9.731707169459415\n",
      "Epoch 45, Loss: 8.967890390982994\n",
      "Epoch 46, Loss: 7.946758948839628\n",
      "Epoch 47, Loss: 7.756676270411565\n",
      "Epoch 48, Loss: 7.305532767222478\n",
      "Epoch 49, Loss: 7.598398245297945\n",
      "Epoch 50, Loss: 10.585406761903029\n",
      "Epoch 51, Loss: 9.798983372174776\n",
      "Epoch 52, Loss: 7.932299027076135\n",
      "Epoch 53, Loss: 7.527932460491474\n",
      "Epoch 54, Loss: 7.026986635648287\n",
      "Epoch 55, Loss: 7.1124396690955525\n",
      "Epoch 56, Loss: 7.0486261661236105\n",
      "Epoch 57, Loss: 7.052361598381629\n",
      "Epoch 58, Loss: 9.222640257615309\n",
      "Epoch 59, Loss: 8.297064249332134\n",
      "Epoch 60, Loss: 7.832440339601957\n",
      "Epoch 61, Loss: 8.499956406079805\n",
      "Epoch 62, Loss: 8.35651447222783\n",
      "Epoch 63, Loss: 7.341573513471163\n",
      "Epoch 64, Loss: 6.901683678993812\n",
      "Epoch 65, Loss: 6.433102094210112\n",
      "Epoch 66, Loss: 6.540972746335543\n",
      "Epoch 67, Loss: 6.239073716677153\n",
      "Epoch 68, Loss: 6.205862247026884\n",
      "Epoch 69, Loss: 6.171522067143367\n",
      "Epoch 70, Loss: 6.025112097079937\n",
      "Epoch 71, Loss: 6.075591582518357\n",
      "Epoch 72, Loss: 6.24534823344304\n",
      "Epoch 73, Loss: 7.271207992847149\n",
      "Epoch 74, Loss: 7.207605031820444\n",
      "Epoch 75, Loss: 7.061780874545757\n",
      "Epoch 76, Loss: 6.706365255209116\n",
      "Epoch 77, Loss: 6.4643381375532885\n",
      "Epoch 78, Loss: 6.281990784865159\n",
      "Epoch 79, Loss: 6.201181815220759\n",
      "Epoch 80, Loss: 6.377746783770048\n",
      "Epoch 81, Loss: 6.26890831727248\n",
      "Epoch 82, Loss: 6.137091306539682\n",
      "Epoch 83, Loss: 6.664116510978112\n",
      "Epoch 84, Loss: 7.533588281044593\n",
      "Epoch 85, Loss: 7.1685731410980225\n",
      "Epoch 86, Loss: 7.024325462488028\n",
      "Epoch 87, Loss: 8.357774642797617\n",
      "Epoch 88, Loss: 8.906657750789936\n",
      "Epoch 89, Loss: 12.63175604893611\n",
      "Epoch 90, Loss: 9.404486326070932\n",
      "Epoch 91, Loss: 9.951201658982496\n",
      "Epoch 92, Loss: 9.73751610975999\n",
      "Epoch 93, Loss: 9.569387619311993\n",
      "Epoch 94, Loss: 8.65924936074477\n",
      "Epoch 95, Loss: 9.538703991816593\n",
      "Epoch 96, Loss: 7.532030435708853\n",
      "Epoch 97, Loss: 8.01514588869535\n",
      "Epoch 98, Loss: 10.571773015535795\n",
      "Epoch 99, Loss: 8.332018191997822\n",
      "Epoch 100, Loss: 7.808586193965032\n",
      "Epoch 101, Loss: 6.75820882503803\n",
      "Epoch 102, Loss: 6.316440637295063\n",
      "Epoch 103, Loss: 5.933464472110455\n",
      "Epoch 104, Loss: 5.689521697851328\n",
      "Epoch 105, Loss: 5.788502014600313\n",
      "Epoch 106, Loss: 5.732246655684251\n",
      "Epoch 107, Loss: 5.633221167784471\n",
      "Epoch 108, Loss: 5.637531812374409\n",
      "Epoch 109, Loss: 5.704908939508291\n",
      "Epoch 110, Loss: 6.341337864215557\n",
      "Epoch 111, Loss: 7.4866916583134575\n",
      "Epoch 112, Loss: 7.678246388068566\n",
      "Epoch 113, Loss: 9.546145384128277\n",
      "Epoch 114, Loss: 13.613372234197763\n",
      "Epoch 115, Loss: 13.272748635365414\n",
      "Epoch 116, Loss: 9.510570287704468\n",
      "Epoch 117, Loss: 8.481705390490019\n",
      "Epoch 118, Loss: 7.784367928138146\n",
      "Epoch 119, Loss: 8.775794689471905\n",
      "Epoch 120, Loss: 7.532722253065843\n",
      "Epoch 121, Loss: 7.158806654123159\n",
      "Epoch 122, Loss: 6.591693914853609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:54:11,349] Trial 91 finished with value: 18.331700274864065 and parameters: {'latent_dim_z1': 12, 'latent_dim_z2': 68, 'hidden_dim': 121, 'epochs': 128, 'causal_reg': 0.8233410532165069, 'learning_rate': 0.0016725302738372996}. Best is trial 67 with value: 17.832464751316955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123, Loss: 6.256454486113328\n",
      "Epoch 124, Loss: 5.922394624123206\n",
      "Epoch 125, Loss: 5.754471118633564\n",
      "Epoch 126, Loss: 5.679006264759944\n",
      "Epoch 127, Loss: 5.644831602389996\n",
      "Epoch 128, Loss: 5.694766301375169\n",
      "Epoch 1, Loss: 324.0336192204402\n",
      "Epoch 2, Loss: 124.45318808922401\n",
      "Epoch 3, Loss: 85.64378753075233\n",
      "Epoch 4, Loss: 77.2649349799523\n",
      "Epoch 5, Loss: 67.95630293626051\n",
      "Epoch 6, Loss: 61.27056664686937\n",
      "Epoch 7, Loss: 50.54262711451604\n",
      "Epoch 8, Loss: 45.553160667419434\n",
      "Epoch 9, Loss: 39.74699900700496\n",
      "Epoch 10, Loss: 40.5665168762207\n",
      "Epoch 11, Loss: 41.27293652754564\n",
      "Epoch 12, Loss: 34.392737938807564\n",
      "Epoch 13, Loss: 29.576256385216347\n",
      "Epoch 14, Loss: 26.976996898651123\n",
      "Epoch 15, Loss: 26.49627575507531\n",
      "Epoch 16, Loss: 24.09947417332576\n",
      "Epoch 17, Loss: 21.44573189662053\n",
      "Epoch 18, Loss: 19.126411804786095\n",
      "Epoch 19, Loss: 20.847531648782585\n",
      "Epoch 20, Loss: 18.84598312011132\n",
      "Epoch 21, Loss: 17.757904272813064\n",
      "Epoch 22, Loss: 17.803064548052273\n",
      "Epoch 23, Loss: 14.56163894213163\n",
      "Epoch 24, Loss: 14.051999037082378\n",
      "Epoch 25, Loss: 13.912576638735258\n",
      "Epoch 26, Loss: 16.334078531998856\n",
      "Epoch 27, Loss: 21.58068352479201\n",
      "Epoch 28, Loss: 16.219782242408165\n",
      "Epoch 29, Loss: 13.409145501943735\n",
      "Epoch 30, Loss: 11.998136025208693\n",
      "Epoch 31, Loss: 11.533090169613178\n",
      "Epoch 32, Loss: 10.653354443036593\n",
      "Epoch 33, Loss: 10.10791514470027\n",
      "Epoch 34, Loss: 9.835718466685368\n",
      "Epoch 35, Loss: 10.643961851413433\n",
      "Epoch 36, Loss: 10.111269785807682\n",
      "Epoch 37, Loss: 9.937645251934345\n",
      "Epoch 38, Loss: 9.299782074414766\n",
      "Epoch 39, Loss: 8.939976068643423\n",
      "Epoch 40, Loss: 8.396126343653751\n",
      "Epoch 41, Loss: 10.07704523893503\n",
      "Epoch 42, Loss: 10.020838480729322\n",
      "Epoch 43, Loss: 9.081158858079176\n",
      "Epoch 44, Loss: 8.255028761350191\n",
      "Epoch 45, Loss: 7.980170029860276\n",
      "Epoch 46, Loss: 7.926964374688955\n",
      "Epoch 47, Loss: 7.958461413016686\n",
      "Epoch 48, Loss: 8.303391456604004\n",
      "Epoch 49, Loss: 8.41096879885747\n",
      "Epoch 50, Loss: 11.049782863030067\n",
      "Epoch 51, Loss: 9.978365402955275\n",
      "Epoch 52, Loss: 8.361211648354164\n",
      "Epoch 53, Loss: 7.358721329615666\n",
      "Epoch 54, Loss: 7.404981007942786\n",
      "Epoch 55, Loss: 7.982577562332153\n",
      "Epoch 56, Loss: 8.147405807788555\n",
      "Epoch 57, Loss: 9.523884186377892\n",
      "Epoch 58, Loss: 11.940164180902334\n",
      "Epoch 59, Loss: 10.508045214873095\n",
      "Epoch 60, Loss: 8.56675934791565\n",
      "Epoch 61, Loss: 7.204254590548002\n",
      "Epoch 62, Loss: 6.58802501971905\n",
      "Epoch 63, Loss: 6.425203671822181\n",
      "Epoch 64, Loss: 6.492849900172307\n",
      "Epoch 65, Loss: 6.849880273525532\n",
      "Epoch 66, Loss: 6.859955585919893\n",
      "Epoch 67, Loss: 7.427352098318247\n",
      "Epoch 68, Loss: 7.337199669617873\n",
      "Epoch 69, Loss: 8.146215732281025\n",
      "Epoch 70, Loss: 8.672279999806332\n",
      "Epoch 71, Loss: 8.66263407927293\n",
      "Epoch 72, Loss: 7.26026307619535\n",
      "Epoch 73, Loss: 7.19413683964656\n",
      "Epoch 74, Loss: 6.534378546934861\n",
      "Epoch 75, Loss: 6.106877583723802\n",
      "Epoch 76, Loss: 5.996398724042452\n",
      "Epoch 77, Loss: 5.751606867863582\n",
      "Epoch 78, Loss: 5.686839122038621\n",
      "Epoch 79, Loss: 5.629122715729934\n",
      "Epoch 80, Loss: 5.839655692760761\n",
      "Epoch 81, Loss: 6.1799085690424995\n",
      "Epoch 82, Loss: 7.052011086390569\n",
      "Epoch 83, Loss: 6.9451051491957445\n",
      "Epoch 84, Loss: 6.6470863819122314\n",
      "Epoch 85, Loss: 6.7135553910182075\n",
      "Epoch 86, Loss: 6.971760236299955\n",
      "Epoch 87, Loss: 6.549885584757878\n",
      "Epoch 88, Loss: 6.227009369776799\n",
      "Epoch 89, Loss: 6.380018215913039\n",
      "Epoch 90, Loss: 6.460879637644841\n",
      "Epoch 91, Loss: 6.59603241773752\n",
      "Epoch 92, Loss: 7.935391389406645\n",
      "Epoch 93, Loss: 7.442037527377789\n",
      "Epoch 94, Loss: 8.849241953629713\n",
      "Epoch 95, Loss: 9.619063744178185\n",
      "Epoch 96, Loss: 9.640930615938627\n",
      "Epoch 97, Loss: 8.500998973846436\n",
      "Epoch 98, Loss: 7.08272987145644\n",
      "Epoch 99, Loss: 6.280984401702881\n",
      "Epoch 100, Loss: 6.385923513999352\n",
      "Epoch 101, Loss: 6.102681104953472\n",
      "Epoch 102, Loss: 6.00034605539762\n",
      "Epoch 103, Loss: 5.975863016568697\n",
      "Epoch 104, Loss: 6.219198832145104\n",
      "Epoch 105, Loss: 6.2510902514824505\n",
      "Epoch 106, Loss: 6.343549655033992\n",
      "Epoch 107, Loss: 6.344653441355779\n",
      "Epoch 108, Loss: 6.519943860860971\n",
      "Epoch 109, Loss: 6.4223816211407\n",
      "Epoch 110, Loss: 6.3064099458547735\n",
      "Epoch 111, Loss: 6.07855145747845\n",
      "Epoch 112, Loss: 5.78464702459482\n",
      "Epoch 113, Loss: 6.141354175714346\n",
      "Epoch 114, Loss: 6.064195486215445\n",
      "Epoch 115, Loss: 6.760970225700965\n",
      "Epoch 116, Loss: 7.075751488025372\n",
      "Epoch 117, Loss: 8.657396408227774\n",
      "Epoch 118, Loss: 8.853975204321054\n",
      "Epoch 119, Loss: 7.942491292953491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:54:14,036] Trial 92 finished with value: 16.88326444886553 and parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 67, 'hidden_dim': 70, 'epochs': 121, 'causal_reg': 0.9325055584750703, 'learning_rate': 0.0020515766054521826}. Best is trial 92 with value: 16.88326444886553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: 9.830711438105656\n",
      "Epoch 121, Loss: 10.037338458574736\n",
      "Epoch 1, Loss: 555.6294734661395\n",
      "Epoch 2, Loss: 205.59301493718073\n",
      "Epoch 3, Loss: 137.61982081486627\n",
      "Epoch 4, Loss: 113.46753707298866\n",
      "Epoch 5, Loss: 96.07737849308894\n",
      "Epoch 6, Loss: 85.32012616671048\n",
      "Epoch 7, Loss: 67.63539050175594\n",
      "Epoch 8, Loss: 58.749519935021034\n",
      "Epoch 9, Loss: 53.23621001610389\n",
      "Epoch 10, Loss: 54.39553334162785\n",
      "Epoch 11, Loss: 42.934009038485016\n",
      "Epoch 12, Loss: 37.89259800544152\n",
      "Epoch 13, Loss: 41.57429944551908\n",
      "Epoch 14, Loss: 39.136793063237114\n",
      "Epoch 15, Loss: 27.637421681330753\n",
      "Epoch 16, Loss: 24.12848898080679\n",
      "Epoch 17, Loss: 21.055854100447434\n",
      "Epoch 18, Loss: 19.414221433492806\n",
      "Epoch 19, Loss: 18.61020990518423\n",
      "Epoch 20, Loss: 16.512713468991794\n",
      "Epoch 21, Loss: 15.902314131076519\n",
      "Epoch 22, Loss: 15.144360578977144\n",
      "Epoch 23, Loss: 13.029032413776104\n",
      "Epoch 24, Loss: 12.85472352688129\n",
      "Epoch 25, Loss: 11.839174123910757\n",
      "Epoch 26, Loss: 11.064683198928833\n",
      "Epoch 27, Loss: 11.1398933667403\n",
      "Epoch 28, Loss: 12.16030034652123\n",
      "Epoch 29, Loss: 11.094400992760292\n",
      "Epoch 30, Loss: 11.659975400337807\n",
      "Epoch 31, Loss: 17.44426254125742\n",
      "Epoch 32, Loss: 15.02531832915086\n",
      "Epoch 33, Loss: 11.540575779401339\n",
      "Epoch 34, Loss: 11.944350957870483\n",
      "Epoch 35, Loss: 8.9683693922483\n",
      "Epoch 36, Loss: 7.713024872999925\n",
      "Epoch 37, Loss: 7.522588289701021\n",
      "Epoch 38, Loss: 9.324651809839102\n",
      "Epoch 39, Loss: 8.39191697194026\n",
      "Epoch 40, Loss: 7.703439712524414\n",
      "Epoch 41, Loss: 7.639504579397348\n",
      "Epoch 42, Loss: 7.412780651679406\n",
      "Epoch 43, Loss: 6.90615256016071\n",
      "Epoch 44, Loss: 6.815408945083618\n",
      "Epoch 45, Loss: 6.503837787188017\n",
      "Epoch 46, Loss: 7.533501313282893\n",
      "Epoch 47, Loss: 7.729817482141348\n",
      "Epoch 48, Loss: 8.0873504785391\n",
      "Epoch 49, Loss: 10.965368600992056\n",
      "Epoch 50, Loss: 13.076403654538668\n",
      "Epoch 51, Loss: 17.08233774625338\n",
      "Epoch 52, Loss: 21.81524350092961\n",
      "Epoch 53, Loss: 19.560159261410053\n",
      "Epoch 54, Loss: 12.903826273404635\n",
      "Epoch 55, Loss: 11.122158729113066\n",
      "Epoch 56, Loss: 9.627052032030546\n",
      "Epoch 57, Loss: 8.45167983495272\n",
      "Epoch 58, Loss: 8.716866108087393\n",
      "Epoch 59, Loss: 7.615133322202242\n",
      "Epoch 60, Loss: 7.199718677080595\n",
      "Epoch 61, Loss: 6.5395962091592645\n",
      "Epoch 62, Loss: 6.120643579042875\n",
      "Epoch 63, Loss: 6.047887068528396\n",
      "Epoch 64, Loss: 6.0284890578343315\n",
      "Epoch 65, Loss: 5.917099732619065\n",
      "Epoch 66, Loss: 5.756038207274217\n",
      "Epoch 67, Loss: 5.665852143214299\n",
      "Epoch 68, Loss: 5.497956184240488\n",
      "Epoch 69, Loss: 5.3904829025268555\n",
      "Epoch 70, Loss: 5.3853062849778395\n",
      "Epoch 71, Loss: 5.4006000115321235\n",
      "Epoch 72, Loss: 5.355701739971455\n",
      "Epoch 73, Loss: 5.48953104019165\n",
      "Epoch 74, Loss: 5.447588737194355\n",
      "Epoch 75, Loss: 5.718093358553373\n",
      "Epoch 76, Loss: 5.888246939732478\n",
      "Epoch 77, Loss: 5.932975457264827\n",
      "Epoch 78, Loss: 6.011752385359544\n",
      "Epoch 79, Loss: 5.957269448500413\n",
      "Epoch 80, Loss: 5.911095050665049\n",
      "Epoch 81, Loss: 6.236706165167002\n",
      "Epoch 82, Loss: 6.539959522394033\n",
      "Epoch 83, Loss: 6.9188785919776326\n",
      "Epoch 84, Loss: 6.838406214347253\n",
      "Epoch 85, Loss: 7.719787285878108\n",
      "Epoch 86, Loss: 8.23989385824937\n",
      "Epoch 87, Loss: 9.679699145830595\n",
      "Epoch 88, Loss: 11.561066224024845\n",
      "Epoch 89, Loss: 9.294017021472637\n",
      "Epoch 90, Loss: 11.283315273431631\n",
      "Epoch 91, Loss: 12.90927817271306\n",
      "Epoch 92, Loss: 11.353313556084267\n",
      "Epoch 93, Loss: 12.331646020595844\n",
      "Epoch 94, Loss: 10.009862074485191\n",
      "Epoch 95, Loss: 8.994695535072914\n",
      "Epoch 96, Loss: 7.8679762069995585\n",
      "Epoch 97, Loss: 7.167308257176326\n",
      "Epoch 98, Loss: 6.420245188933152\n",
      "Epoch 99, Loss: 5.956621280083289\n",
      "Epoch 100, Loss: 6.191326783253596\n",
      "Epoch 101, Loss: 6.040849869067852\n",
      "Epoch 102, Loss: 6.316323573772724\n",
      "Epoch 103, Loss: 6.258826567576482\n",
      "Epoch 104, Loss: 6.039946464391855\n",
      "Epoch 105, Loss: 5.888589400511521\n",
      "Epoch 106, Loss: 5.758958651469304\n",
      "Epoch 107, Loss: 6.568184210703923\n",
      "Epoch 108, Loss: 8.277993348928598\n",
      "Epoch 109, Loss: 7.731188994187575\n",
      "Epoch 110, Loss: 8.082707056632408\n",
      "Epoch 111, Loss: 8.69437157190763\n",
      "Epoch 112, Loss: 8.248476633658775\n",
      "Epoch 113, Loss: 7.1652658719282885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:54:16,726] Trial 93 finished with value: 21.3987612745524 and parameters: {'latent_dim_z1': 16, 'latent_dim_z2': 67, 'hidden_dim': 66, 'epochs': 121, 'causal_reg': 0.9761285275690275, 'learning_rate': 0.0017396671480458849}. Best is trial 92 with value: 16.88326444886553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114, Loss: 6.530542428676899\n",
      "Epoch 115, Loss: 6.424865264158982\n",
      "Epoch 116, Loss: 6.960464569238516\n",
      "Epoch 117, Loss: 7.200348413907564\n",
      "Epoch 118, Loss: 6.733204639874971\n",
      "Epoch 119, Loss: 6.676953462453989\n",
      "Epoch 120, Loss: 7.119132335369404\n",
      "Epoch 121, Loss: 6.7044408321380615\n",
      "Epoch 1, Loss: 457.00434494018555\n",
      "Epoch 2, Loss: 185.81717153695914\n",
      "Epoch 3, Loss: 133.17176840855524\n",
      "Epoch 4, Loss: 111.33734130859375\n",
      "Epoch 5, Loss: 93.04908913832445\n",
      "Epoch 6, Loss: 83.84063955453726\n",
      "Epoch 7, Loss: 75.35024056067833\n",
      "Epoch 8, Loss: 71.02774634728065\n",
      "Epoch 9, Loss: 60.492774963378906\n",
      "Epoch 10, Loss: 53.89927475269024\n",
      "Epoch 11, Loss: 50.58283893878643\n",
      "Epoch 12, Loss: 47.73341787778414\n",
      "Epoch 13, Loss: 40.90102591881385\n",
      "Epoch 14, Loss: 37.67524704566369\n",
      "Epoch 15, Loss: 33.98618767811702\n",
      "Epoch 16, Loss: 30.102473662449764\n",
      "Epoch 17, Loss: 29.121841760782097\n",
      "Epoch 18, Loss: 26.783491391402023\n",
      "Epoch 19, Loss: 23.72255593079787\n",
      "Epoch 20, Loss: 23.3155659528879\n",
      "Epoch 21, Loss: 21.24746553714459\n",
      "Epoch 22, Loss: 20.557853588691124\n",
      "Epoch 23, Loss: 21.00254146869366\n",
      "Epoch 24, Loss: 19.41992572637705\n",
      "Epoch 25, Loss: 16.879945791684666\n",
      "Epoch 26, Loss: 18.451115644895115\n",
      "Epoch 27, Loss: 16.414946024234478\n",
      "Epoch 28, Loss: 16.452087732461784\n",
      "Epoch 29, Loss: 15.78520683141855\n",
      "Epoch 30, Loss: 14.448163582728458\n",
      "Epoch 31, Loss: 16.40212128712581\n",
      "Epoch 32, Loss: 13.670702897585356\n",
      "Epoch 33, Loss: 11.406073203453651\n",
      "Epoch 34, Loss: 11.470823306303759\n",
      "Epoch 35, Loss: 9.845860793040348\n",
      "Epoch 36, Loss: 10.747367858886719\n",
      "Epoch 37, Loss: 10.084027161965004\n",
      "Epoch 38, Loss: 9.016155866476206\n",
      "Epoch 39, Loss: 8.844912675710825\n",
      "Epoch 40, Loss: 8.243409413557787\n",
      "Epoch 41, Loss: 9.612211832633385\n",
      "Epoch 42, Loss: 10.119610731418316\n",
      "Epoch 43, Loss: 9.529674475009625\n",
      "Epoch 44, Loss: 9.058274232424223\n",
      "Epoch 45, Loss: 10.04019363109882\n",
      "Epoch 46, Loss: 9.099228858947754\n",
      "Epoch 47, Loss: 9.690331532404972\n",
      "Epoch 48, Loss: 8.195841715886043\n",
      "Epoch 49, Loss: 7.655488014221191\n",
      "Epoch 50, Loss: 10.041188588509193\n",
      "Epoch 51, Loss: 10.471370623661922\n",
      "Epoch 52, Loss: 8.834519441311176\n",
      "Epoch 53, Loss: 8.009591322678785\n",
      "Epoch 54, Loss: 7.394601785219633\n",
      "Epoch 55, Loss: 7.55226703790518\n",
      "Epoch 56, Loss: 7.337259751099807\n",
      "Epoch 57, Loss: 6.934772839913001\n",
      "Epoch 58, Loss: 6.552395270420955\n",
      "Epoch 59, Loss: 6.602498568021334\n",
      "Epoch 60, Loss: 6.503162622451782\n",
      "Epoch 61, Loss: 6.514497426839975\n",
      "Epoch 62, Loss: 6.996483491017268\n",
      "Epoch 63, Loss: 7.288433129970844\n",
      "Epoch 64, Loss: 8.565842316700863\n",
      "Epoch 65, Loss: 8.427889365416307\n",
      "Epoch 66, Loss: 8.105668251331036\n",
      "Epoch 67, Loss: 8.69174078794626\n",
      "Epoch 68, Loss: 9.674176747982319\n",
      "Epoch 69, Loss: 8.642706174116869\n",
      "Epoch 70, Loss: 11.342900734681349\n",
      "Epoch 71, Loss: 11.112725001115065\n",
      "Epoch 72, Loss: 11.090168531124409\n",
      "Epoch 73, Loss: 13.455382640545185\n",
      "Epoch 74, Loss: 12.877667298683754\n",
      "Epoch 75, Loss: 10.246922401281504\n",
      "Epoch 76, Loss: 8.1653981758998\n",
      "Epoch 77, Loss: 6.686486390920786\n",
      "Epoch 78, Loss: 6.467950674203726\n",
      "Epoch 79, Loss: 6.438342754657452\n",
      "Epoch 80, Loss: 6.369183998841506\n",
      "Epoch 81, Loss: 6.745444609568669\n",
      "Epoch 82, Loss: 6.195677738923293\n",
      "Epoch 83, Loss: 6.01178161914532\n",
      "Epoch 84, Loss: 5.688181657057542\n",
      "Epoch 85, Loss: 5.625330264751728\n",
      "Epoch 86, Loss: 5.5937774914961595\n",
      "Epoch 87, Loss: 5.62384322973398\n",
      "Epoch 88, Loss: 5.661982407936683\n",
      "Epoch 89, Loss: 5.645269742378821\n",
      "Epoch 90, Loss: 5.562458698566143\n",
      "Epoch 91, Loss: 5.5122641416696405\n",
      "Epoch 92, Loss: 5.518023069088276\n",
      "Epoch 93, Loss: 5.499371858743521\n",
      "Epoch 94, Loss: 5.558743898685162\n",
      "Epoch 95, Loss: 5.506769638795119\n",
      "Epoch 96, Loss: 5.499376718814556\n",
      "Epoch 97, Loss: 5.6246729630690355\n",
      "Epoch 98, Loss: 5.952372037447416\n",
      "Epoch 99, Loss: 6.109812259674072\n",
      "Epoch 100, Loss: 6.148650371111357\n",
      "Epoch 101, Loss: 6.20911352451031\n",
      "Epoch 102, Loss: 6.214590824567354\n",
      "Epoch 103, Loss: 6.262260290292593\n",
      "Epoch 104, Loss: 6.368098735809326\n",
      "Epoch 105, Loss: 7.130133775564341\n",
      "Epoch 106, Loss: 7.975216865539551\n",
      "Epoch 107, Loss: 8.909231681090136\n",
      "Epoch 108, Loss: 17.308432469001183\n",
      "Epoch 109, Loss: 15.08283569262578\n",
      "Epoch 110, Loss: 10.44670137992272\n",
      "Epoch 111, Loss: 8.243965479043814\n",
      "Epoch 112, Loss: 9.261090260285597\n",
      "Epoch 113, Loss: 8.502455582985512\n",
      "Epoch 114, Loss: 9.891638077222384\n",
      "Epoch 115, Loss: 7.544362306594849\n",
      "Epoch 116, Loss: 6.818423968095046\n",
      "Epoch 117, Loss: 6.7789940650646505\n",
      "Epoch 118, Loss: 6.623894508068378\n",
      "Epoch 119, Loss: 6.7944530340341425\n",
      "Epoch 120, Loss: 7.025696424337534\n",
      "Epoch 121, Loss: 7.9494751049922066\n",
      "Epoch 122, Loss: 7.944431176552405\n",
      "Epoch 123, Loss: 7.164440870285034\n",
      "Epoch 124, Loss: 6.242640568659856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:54:19,741] Trial 94 finished with value: 20.007233138746848 and parameters: {'latent_dim_z1': 14, 'latent_dim_z2': 71, 'hidden_dim': 72, 'epochs': 134, 'causal_reg': 0.931062024091344, 'learning_rate': 0.0013188808864086933}. Best is trial 92 with value: 16.88326444886553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125, Loss: 5.8998888822702265\n",
      "Epoch 126, Loss: 6.147747589991643\n",
      "Epoch 127, Loss: 6.03122588304373\n",
      "Epoch 128, Loss: 5.844730652295626\n",
      "Epoch 129, Loss: 5.960900545120239\n",
      "Epoch 130, Loss: 6.132616079770601\n",
      "Epoch 131, Loss: 6.456311317590567\n",
      "Epoch 132, Loss: 6.332865623327402\n",
      "Epoch 133, Loss: 6.305380949607263\n",
      "Epoch 134, Loss: 6.180398757641132\n",
      "Epoch 1, Loss: 404.88931186382587\n",
      "Epoch 2, Loss: 145.62482819190393\n",
      "Epoch 3, Loss: 105.4258810190054\n",
      "Epoch 4, Loss: 85.29927136347844\n",
      "Epoch 5, Loss: 70.54647423670842\n",
      "Epoch 6, Loss: 64.65270233154297\n",
      "Epoch 7, Loss: 57.02242374420166\n",
      "Epoch 8, Loss: 46.03811396085299\n",
      "Epoch 9, Loss: 41.68970254751352\n",
      "Epoch 10, Loss: 43.21085541064922\n",
      "Epoch 11, Loss: 49.83166606609638\n",
      "Epoch 12, Loss: 41.160223740797775\n",
      "Epoch 13, Loss: 32.179335777576156\n",
      "Epoch 14, Loss: 28.014640184549187\n",
      "Epoch 15, Loss: 23.965817744915302\n",
      "Epoch 16, Loss: 21.581791694347675\n",
      "Epoch 17, Loss: 19.631129044752853\n",
      "Epoch 18, Loss: 18.587607090289776\n",
      "Epoch 19, Loss: 19.70442372102004\n",
      "Epoch 20, Loss: 22.691918409787693\n",
      "Epoch 21, Loss: 16.620911799944363\n",
      "Epoch 22, Loss: 14.53729798243596\n",
      "Epoch 23, Loss: 15.468410345224234\n",
      "Epoch 24, Loss: 13.45469984641442\n",
      "Epoch 25, Loss: 16.83860547725971\n",
      "Epoch 26, Loss: 17.213347288278435\n",
      "Epoch 27, Loss: 13.20224642753601\n",
      "Epoch 28, Loss: 12.071918120751015\n",
      "Epoch 29, Loss: 11.192084569197435\n",
      "Epoch 30, Loss: 13.017214408287636\n",
      "Epoch 31, Loss: 10.822294491987963\n",
      "Epoch 32, Loss: 9.146018009919386\n",
      "Epoch 33, Loss: 8.44118178807772\n",
      "Epoch 34, Loss: 9.050588552768414\n",
      "Epoch 35, Loss: 9.229097366333008\n",
      "Epoch 36, Loss: 8.932486259020292\n",
      "Epoch 37, Loss: 8.0080941090217\n",
      "Epoch 38, Loss: 7.31227339231051\n",
      "Epoch 39, Loss: 7.765927644876333\n",
      "Epoch 40, Loss: 7.642850857514602\n",
      "Epoch 41, Loss: 7.299690301601704\n",
      "Epoch 42, Loss: 7.42292306973384\n",
      "Epoch 43, Loss: 8.992780245267427\n",
      "Epoch 44, Loss: 8.08463975099417\n",
      "Epoch 45, Loss: 7.194990048041711\n",
      "Epoch 46, Loss: 6.870804658302894\n",
      "Epoch 47, Loss: 6.764357236715464\n",
      "Epoch 48, Loss: 6.710637716146616\n",
      "Epoch 49, Loss: 6.7915589809417725\n",
      "Epoch 50, Loss: 6.728874646700346\n",
      "Epoch 51, Loss: 7.442268444941594\n",
      "Epoch 52, Loss: 8.418798208236694\n",
      "Epoch 53, Loss: 7.7521323057321405\n",
      "Epoch 54, Loss: 7.117745876312256\n",
      "Epoch 55, Loss: 6.663438907036414\n",
      "Epoch 56, Loss: 6.598259449005127\n",
      "Epoch 57, Loss: 6.380376577377319\n",
      "Epoch 58, Loss: 6.603167552214402\n",
      "Epoch 59, Loss: 6.535078103725727\n",
      "Epoch 60, Loss: 6.435052468226506\n",
      "Epoch 61, Loss: 6.4723487083728495\n",
      "Epoch 62, Loss: 7.416446300653311\n",
      "Epoch 63, Loss: 6.980353098649245\n",
      "Epoch 64, Loss: 8.48181493465717\n",
      "Epoch 65, Loss: 11.37312966126662\n",
      "Epoch 66, Loss: 10.8061594412877\n",
      "Epoch 67, Loss: 9.27737912764916\n",
      "Epoch 68, Loss: 8.54478826889625\n",
      "Epoch 69, Loss: 11.731181988349327\n",
      "Epoch 70, Loss: 10.497363567352295\n",
      "Epoch 71, Loss: 8.95539221396813\n",
      "Epoch 72, Loss: 7.626729598412147\n",
      "Epoch 73, Loss: 7.438601072017963\n",
      "Epoch 74, Loss: 8.084313502678505\n",
      "Epoch 75, Loss: 8.803612342247597\n",
      "Epoch 76, Loss: 8.003522817905132\n",
      "Epoch 77, Loss: 7.107415896195632\n",
      "Epoch 78, Loss: 6.3389715781578655\n",
      "Epoch 79, Loss: 6.496053310541006\n",
      "Epoch 80, Loss: 6.502066080386822\n",
      "Epoch 81, Loss: 6.46081282542302\n",
      "Epoch 82, Loss: 6.287824318959163\n",
      "Epoch 83, Loss: 6.065380738331721\n",
      "Epoch 84, Loss: 5.757031844212459\n",
      "Epoch 85, Loss: 5.673172088769766\n",
      "Epoch 86, Loss: 5.886678200501662\n",
      "Epoch 87, Loss: 5.789448554699238\n",
      "Epoch 88, Loss: 5.967978569177481\n",
      "Epoch 89, Loss: 5.817952064367441\n",
      "Epoch 90, Loss: 5.764920179660503\n",
      "Epoch 91, Loss: 5.821004133958083\n",
      "Epoch 92, Loss: 6.279055485358605\n",
      "Epoch 93, Loss: 6.47355761894813\n",
      "Epoch 94, Loss: 6.8478573102217455\n",
      "Epoch 95, Loss: 6.841235215847309\n",
      "Epoch 96, Loss: 6.3039683011861944\n",
      "Epoch 97, Loss: 6.3531045363499565\n",
      "Epoch 98, Loss: 7.056440610152024\n",
      "Epoch 99, Loss: 7.091543124272273\n",
      "Epoch 100, Loss: 6.914409454052265\n",
      "Epoch 101, Loss: 7.288484078187209\n",
      "Epoch 102, Loss: 6.79162859916687\n",
      "Epoch 103, Loss: 6.233388002102192\n",
      "Epoch 104, Loss: 6.216796728280874\n",
      "Epoch 105, Loss: 6.09302463898292\n",
      "Epoch 106, Loss: 5.951693644890418\n",
      "Epoch 107, Loss: 5.921905792676485\n",
      "Epoch 108, Loss: 6.059373177014864\n",
      "Epoch 109, Loss: 6.028639775056106\n",
      "Epoch 110, Loss: 5.9299468627342815\n",
      "Epoch 111, Loss: 6.6665610166696405\n",
      "Epoch 112, Loss: 7.244301887658926\n",
      "Epoch 113, Loss: 7.4981102760021505\n",
      "Epoch 114, Loss: 7.683293709388146\n",
      "Epoch 115, Loss: 7.587373751860398\n",
      "Epoch 116, Loss: 7.571267659847553\n",
      "Epoch 117, Loss: 7.197366127601037\n",
      "Epoch 118, Loss: 6.38461892421429\n",
      "Epoch 119, Loss: 5.9270722682659445\n",
      "Epoch 120, Loss: 6.329794608629667\n",
      "Epoch 121, Loss: 6.676900496849647\n",
      "Epoch 122, Loss: 6.227287219120906\n",
      "Epoch 123, Loss: 8.631711996518648\n",
      "Epoch 124, Loss: 10.195316553115845\n",
      "Epoch 125, Loss: 8.288091494486881\n",
      "Epoch 126, Loss: 7.848280925017137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:54:22,767] Trial 95 finished with value: 17.115995625761485 and parameters: {'latent_dim_z1': 12, 'latent_dim_z2': 71, 'hidden_dim': 98, 'epochs': 129, 'causal_reg': 0.8917907085945633, 'learning_rate': 0.002101520483450245}. Best is trial 92 with value: 16.88326444886553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127, Loss: 7.28709915968088\n",
      "Epoch 128, Loss: 6.626122401310847\n",
      "Epoch 129, Loss: 6.663723835578332\n",
      "Epoch 1, Loss: 371.79372611412634\n",
      "Epoch 2, Loss: 135.23174755389874\n",
      "Epoch 3, Loss: 100.23019130413348\n",
      "Epoch 4, Loss: 84.8050975065965\n",
      "Epoch 5, Loss: 74.83870917100172\n",
      "Epoch 6, Loss: 67.21231196476863\n",
      "Epoch 7, Loss: 59.861351820138786\n",
      "Epoch 8, Loss: 53.927839719332184\n",
      "Epoch 9, Loss: 48.78545621725229\n",
      "Epoch 10, Loss: 41.94671938969539\n",
      "Epoch 11, Loss: 36.52804499406081\n",
      "Epoch 12, Loss: 32.18650157635029\n",
      "Epoch 13, Loss: 33.66879338484544\n",
      "Epoch 14, Loss: 30.63444761129526\n",
      "Epoch 15, Loss: 29.373291455782375\n",
      "Epoch 16, Loss: 24.786578728602482\n",
      "Epoch 17, Loss: 21.27848405104417\n",
      "Epoch 18, Loss: 20.720452492053692\n",
      "Epoch 19, Loss: 18.749412059783936\n",
      "Epoch 20, Loss: 20.09362947023832\n",
      "Epoch 21, Loss: 15.786292222829966\n",
      "Epoch 22, Loss: 16.250733045431282\n",
      "Epoch 23, Loss: 14.601737755995531\n",
      "Epoch 24, Loss: 14.688770294189453\n",
      "Epoch 25, Loss: 16.05343613257775\n",
      "Epoch 26, Loss: 14.106522156642033\n",
      "Epoch 27, Loss: 17.562574973473183\n",
      "Epoch 28, Loss: 13.541944063626802\n",
      "Epoch 29, Loss: 12.208892272068905\n",
      "Epoch 30, Loss: 13.23455027433542\n",
      "Epoch 31, Loss: 11.849524149527916\n",
      "Epoch 32, Loss: 9.699726746632503\n",
      "Epoch 33, Loss: 11.793081760406494\n",
      "Epoch 34, Loss: 11.228555532602163\n",
      "Epoch 35, Loss: 10.923039197921753\n",
      "Epoch 36, Loss: 10.842084022668692\n",
      "Epoch 37, Loss: 10.105679438664364\n",
      "Epoch 38, Loss: 9.067150684503408\n",
      "Epoch 39, Loss: 8.452438592910767\n",
      "Epoch 40, Loss: 8.84596558717581\n",
      "Epoch 41, Loss: 10.343622042582584\n",
      "Epoch 42, Loss: 9.619322556715746\n",
      "Epoch 43, Loss: 8.549115437727709\n",
      "Epoch 44, Loss: 11.767669861133282\n",
      "Epoch 45, Loss: 13.87973715708806\n",
      "Epoch 46, Loss: 12.56786295083853\n",
      "Epoch 47, Loss: 9.45414191025954\n",
      "Epoch 48, Loss: 8.479246873121996\n",
      "Epoch 49, Loss: 8.548988030507015\n",
      "Epoch 50, Loss: 7.9789249163407545\n",
      "Epoch 51, Loss: 9.774623394012451\n",
      "Epoch 52, Loss: 9.573401872928326\n",
      "Epoch 53, Loss: 9.00602780855619\n",
      "Epoch 54, Loss: 7.609901501582219\n",
      "Epoch 55, Loss: 7.668175770686223\n",
      "Epoch 56, Loss: 7.21019119482774\n",
      "Epoch 57, Loss: 6.904462282474224\n",
      "Epoch 58, Loss: 7.02959053332989\n",
      "Epoch 59, Loss: 7.078987946877112\n",
      "Epoch 60, Loss: 7.0469291393573465\n",
      "Epoch 61, Loss: 6.688605913749108\n",
      "Epoch 62, Loss: 6.503397868229793\n",
      "Epoch 63, Loss: 7.954874552213228\n",
      "Epoch 64, Loss: 8.797486378596378\n",
      "Epoch 65, Loss: 8.207993213947002\n",
      "Epoch 66, Loss: 7.5804308744577265\n",
      "Epoch 67, Loss: 8.061061639052172\n",
      "Epoch 68, Loss: 8.215731914226826\n",
      "Epoch 69, Loss: 7.600049128899207\n",
      "Epoch 70, Loss: 7.4932744319622335\n",
      "Epoch 71, Loss: 9.275335476948666\n",
      "Epoch 72, Loss: 9.989274116662832\n",
      "Epoch 73, Loss: 11.032808743990385\n",
      "Epoch 74, Loss: 10.010455828446608\n",
      "Epoch 75, Loss: 8.09260161106403\n",
      "Epoch 76, Loss: 7.426693256084736\n",
      "Epoch 77, Loss: 6.9733037398411675\n",
      "Epoch 78, Loss: 7.854595367725079\n",
      "Epoch 79, Loss: 8.064233541488647\n",
      "Epoch 80, Loss: 7.732351028002226\n",
      "Epoch 81, Loss: 8.15931067099938\n",
      "Epoch 82, Loss: 8.190990466337938\n",
      "Epoch 83, Loss: 7.909982864673321\n",
      "Epoch 84, Loss: 7.435702397273137\n",
      "Epoch 85, Loss: 6.985295387414785\n",
      "Epoch 86, Loss: 7.018660306930542\n",
      "Epoch 87, Loss: 6.724989487574651\n",
      "Epoch 88, Loss: 6.252108317155105\n",
      "Epoch 89, Loss: 6.08555819438054\n",
      "Epoch 90, Loss: 6.210038588597224\n",
      "Epoch 91, Loss: 7.565495289289034\n",
      "Epoch 92, Loss: 7.118834770642794\n",
      "Epoch 93, Loss: 6.409718495148879\n",
      "Epoch 94, Loss: 6.421711481534517\n",
      "Epoch 95, Loss: 6.183992660962618\n",
      "Epoch 96, Loss: 7.413725431148823\n",
      "Epoch 97, Loss: 8.397005007817196\n",
      "Epoch 98, Loss: 10.325837410413301\n",
      "Epoch 99, Loss: 9.85463054363544\n",
      "Epoch 100, Loss: 9.03506381695087\n",
      "Epoch 101, Loss: 7.915129808279184\n",
      "Epoch 102, Loss: 7.249162508891179\n",
      "Epoch 103, Loss: 7.299414946482732\n",
      "Epoch 104, Loss: 7.695162167915931\n",
      "Epoch 105, Loss: 7.163840257204496\n",
      "Epoch 106, Loss: 6.872525013410128\n",
      "Epoch 107, Loss: 6.655157015873836\n",
      "Epoch 108, Loss: 7.275494685539832\n",
      "Epoch 109, Loss: 7.301756033530602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:54:25,328] Trial 96 finished with value: 18.96085546777879 and parameters: {'latent_dim_z1': 12, 'latent_dim_z2': 70, 'hidden_dim': 98, 'epochs': 113, 'causal_reg': 0.8377303733392968, 'learning_rate': 0.0021537231595526365}. Best is trial 92 with value: 16.88326444886553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110, Loss: 7.309375066023606\n",
      "Epoch 111, Loss: 6.593088370103103\n",
      "Epoch 112, Loss: 6.229478671000554\n",
      "Epoch 113, Loss: 6.177464430148785\n",
      "Epoch 1, Loss: 477.2474409250113\n",
      "Epoch 2, Loss: 194.226018172044\n",
      "Epoch 3, Loss: 141.40641843355618\n",
      "Epoch 4, Loss: 118.76459781940167\n",
      "Epoch 5, Loss: 98.29866159879245\n",
      "Epoch 6, Loss: 87.07196851877066\n",
      "Epoch 7, Loss: 82.44601249694824\n",
      "Epoch 8, Loss: 71.30510550278883\n",
      "Epoch 9, Loss: 60.459978066957916\n",
      "Epoch 10, Loss: 59.83138150435228\n",
      "Epoch 11, Loss: 50.38491857968844\n",
      "Epoch 12, Loss: 49.47019863128662\n",
      "Epoch 13, Loss: 38.384777289170486\n",
      "Epoch 14, Loss: 30.914579831636868\n",
      "Epoch 15, Loss: 28.34289081280048\n",
      "Epoch 16, Loss: 28.15231803747324\n",
      "Epoch 17, Loss: 27.20131694353544\n",
      "Epoch 18, Loss: 24.747832114879902\n",
      "Epoch 19, Loss: 19.954877853393555\n",
      "Epoch 20, Loss: 18.41727906007033\n",
      "Epoch 21, Loss: 16.82295377437885\n",
      "Epoch 22, Loss: 18.448960524338943\n",
      "Epoch 23, Loss: 17.245703036968525\n",
      "Epoch 24, Loss: 16.92871834681584\n",
      "Epoch 25, Loss: 13.80824767626249\n",
      "Epoch 26, Loss: 13.261001696953407\n",
      "Epoch 27, Loss: 14.686693998483511\n",
      "Epoch 28, Loss: 17.309141085698055\n",
      "Epoch 29, Loss: 17.303183555603027\n",
      "Epoch 30, Loss: 14.695059409508339\n",
      "Epoch 31, Loss: 12.289439916610718\n",
      "Epoch 32, Loss: 11.445730649507963\n",
      "Epoch 33, Loss: 14.86859185879047\n",
      "Epoch 34, Loss: 14.969428465916561\n",
      "Epoch 35, Loss: 12.028629229618954\n",
      "Epoch 36, Loss: 10.798209263728214\n",
      "Epoch 37, Loss: 10.100731611251831\n",
      "Epoch 38, Loss: 9.760763443433321\n",
      "Epoch 39, Loss: 11.442059095089252\n",
      "Epoch 40, Loss: 11.19450697532067\n",
      "Epoch 41, Loss: 10.764139047035805\n",
      "Epoch 42, Loss: 10.986989534818209\n",
      "Epoch 43, Loss: 9.844053396811852\n",
      "Epoch 44, Loss: 13.013627657523521\n",
      "Epoch 45, Loss: 12.031397526080791\n",
      "Epoch 46, Loss: 10.715230960112352\n",
      "Epoch 47, Loss: 8.684098628851084\n",
      "Epoch 48, Loss: 8.030586205996\n",
      "Epoch 49, Loss: 7.48454108605018\n",
      "Epoch 50, Loss: 7.370702578471257\n",
      "Epoch 51, Loss: 7.324692249298096\n",
      "Epoch 52, Loss: 7.714328949268047\n",
      "Epoch 53, Loss: 7.676774813578679\n",
      "Epoch 54, Loss: 7.117248571836031\n",
      "Epoch 55, Loss: 6.686977056356577\n",
      "Epoch 56, Loss: 7.498579190327571\n",
      "Epoch 57, Loss: 7.532115037624653\n",
      "Epoch 58, Loss: 8.447856041101309\n",
      "Epoch 59, Loss: 9.494378328323364\n",
      "Epoch 60, Loss: 11.360761770835289\n",
      "Epoch 61, Loss: 11.070934148935171\n",
      "Epoch 62, Loss: 11.34654377057002\n",
      "Epoch 63, Loss: 9.950534435418936\n",
      "Epoch 64, Loss: 10.169652553705069\n",
      "Epoch 65, Loss: 9.654012771753164\n",
      "Epoch 66, Loss: 9.086461910834679\n",
      "Epoch 67, Loss: 9.207272914739756\n",
      "Epoch 68, Loss: 8.46703947507418\n",
      "Epoch 69, Loss: 9.516193811710064\n",
      "Epoch 70, Loss: 8.771625867256752\n",
      "Epoch 71, Loss: 8.436096759942862\n",
      "Epoch 72, Loss: 9.137761446145864\n",
      "Epoch 73, Loss: 9.789491506723257\n",
      "Epoch 74, Loss: 10.992480571453388\n",
      "Epoch 75, Loss: 11.694374653009268\n",
      "Epoch 76, Loss: 15.222054848304161\n",
      "Epoch 77, Loss: 16.126113488123966\n",
      "Epoch 78, Loss: 12.848796624403734\n",
      "Epoch 79, Loss: 10.154647717109093\n",
      "Epoch 80, Loss: 8.243599891662598\n",
      "Epoch 81, Loss: 7.023316896878756\n",
      "Epoch 82, Loss: 7.100020243571355\n",
      "Epoch 83, Loss: 7.532881039839524\n",
      "Epoch 84, Loss: 7.950556534987229\n",
      "Epoch 85, Loss: 7.46417419727032\n",
      "Epoch 86, Loss: 7.153970424945538\n",
      "Epoch 87, Loss: 6.575574489740225\n",
      "Epoch 88, Loss: 6.720003549869244\n",
      "Epoch 89, Loss: 6.575837245354285\n",
      "Epoch 90, Loss: 6.293569766558134\n",
      "Epoch 91, Loss: 6.239762379572942\n",
      "Epoch 92, Loss: 6.320301331006563\n",
      "Epoch 93, Loss: 6.631588715773362\n",
      "Epoch 94, Loss: 6.420552437122051\n",
      "Epoch 95, Loss: 6.212431302437415\n",
      "Epoch 96, Loss: 6.544815356914814\n",
      "Epoch 97, Loss: 6.911527248529287\n",
      "Epoch 98, Loss: 7.391608073161199\n",
      "Epoch 99, Loss: 6.983042771999653\n",
      "Epoch 100, Loss: 6.783745032090407\n",
      "Epoch 101, Loss: 6.685563784379226\n",
      "Epoch 102, Loss: 6.9563442927140455\n",
      "Epoch 103, Loss: 6.800089065845196\n",
      "Epoch 104, Loss: 7.0773856823260965\n",
      "Epoch 105, Loss: 7.585098119882437\n",
      "Epoch 106, Loss: 8.736190539139967\n",
      "Epoch 107, Loss: 9.484350516245915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:54:27,975] Trial 97 finished with value: 23.04166270637639 and parameters: {'latent_dim_z1': 19, 'latent_dim_z2': 56, 'hidden_dim': 98, 'epochs': 114, 'causal_reg': 0.7719913388118658, 'learning_rate': 0.0022647010383832437}. Best is trial 92 with value: 16.88326444886553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108, Loss: 10.385980441020084\n",
      "Epoch 109, Loss: 9.693172308114859\n",
      "Epoch 110, Loss: 8.767926619603085\n",
      "Epoch 111, Loss: 8.613183956879835\n",
      "Epoch 112, Loss: 8.003444451552172\n",
      "Epoch 113, Loss: 7.721076451815092\n",
      "Epoch 114, Loss: 7.30936965575585\n",
      "Epoch 1, Loss: 296.2862812922551\n",
      "Epoch 2, Loss: 110.10136611645038\n",
      "Epoch 3, Loss: 78.54887221409724\n",
      "Epoch 4, Loss: 65.91952463296744\n",
      "Epoch 5, Loss: 57.72602345393254\n",
      "Epoch 6, Loss: 58.94891731555645\n",
      "Epoch 7, Loss: 52.60964797093318\n",
      "Epoch 8, Loss: 58.04936544711773\n",
      "Epoch 9, Loss: 44.477341211759125\n",
      "Epoch 10, Loss: 39.23617392319899\n",
      "Epoch 11, Loss: 35.9539213180542\n",
      "Epoch 12, Loss: 33.14836120605469\n",
      "Epoch 13, Loss: 30.31531755740826\n",
      "Epoch 14, Loss: 34.93520399240347\n",
      "Epoch 15, Loss: 33.491503312037544\n",
      "Epoch 16, Loss: 29.134415809924786\n",
      "Epoch 17, Loss: 25.794834797198956\n",
      "Epoch 18, Loss: 23.10548081764808\n",
      "Epoch 19, Loss: 22.796964663725632\n",
      "Epoch 20, Loss: 20.74856075873742\n",
      "Epoch 21, Loss: 26.949381571549637\n",
      "Epoch 22, Loss: 34.08703026404748\n",
      "Epoch 23, Loss: 22.291738473452053\n",
      "Epoch 24, Loss: 18.89395436873803\n",
      "Epoch 25, Loss: 17.680498600006104\n",
      "Epoch 26, Loss: 17.11204217030452\n",
      "Epoch 27, Loss: 17.592205120966984\n",
      "Epoch 28, Loss: 17.42867400096013\n",
      "Epoch 29, Loss: 15.401835074791542\n",
      "Epoch 30, Loss: 16.18290875508235\n",
      "Epoch 31, Loss: 14.350702047348022\n",
      "Epoch 32, Loss: 15.033481487861046\n",
      "Epoch 33, Loss: 14.840482528393085\n",
      "Epoch 34, Loss: 14.288154381972094\n",
      "Epoch 35, Loss: 14.484996025378887\n",
      "Epoch 36, Loss: 14.507191804739145\n",
      "Epoch 37, Loss: 12.969695531404936\n",
      "Epoch 38, Loss: 16.237471910623405\n",
      "Epoch 39, Loss: 13.762431438152607\n",
      "Epoch 40, Loss: 12.65191283592811\n",
      "Epoch 41, Loss: 12.202267059913048\n",
      "Epoch 42, Loss: 17.739648818969727\n",
      "Epoch 43, Loss: 20.19420568759625\n",
      "Epoch 44, Loss: 15.030146488776573\n",
      "Epoch 45, Loss: 11.205484977135292\n",
      "Epoch 46, Loss: 11.16855867092426\n",
      "Epoch 47, Loss: 11.227396524869478\n",
      "Epoch 48, Loss: 14.501386238978458\n",
      "Epoch 49, Loss: 11.414113118098332\n",
      "Epoch 50, Loss: 11.005864326770489\n",
      "Epoch 51, Loss: 10.099128338006826\n",
      "Epoch 52, Loss: 10.735136875739464\n",
      "Epoch 53, Loss: 10.642528185477623\n",
      "Epoch 54, Loss: 12.40365481376648\n",
      "Epoch 55, Loss: 10.927383056053749\n",
      "Epoch 56, Loss: 11.904664883246788\n",
      "Epoch 57, Loss: 10.424000446613018\n",
      "Epoch 58, Loss: 9.980959470455463\n",
      "Epoch 59, Loss: 8.92972414310162\n",
      "Epoch 60, Loss: 8.934745660194984\n",
      "Epoch 61, Loss: 9.845415408794697\n",
      "Epoch 62, Loss: 9.254512034929716\n",
      "Epoch 63, Loss: 9.458269816178541\n",
      "Epoch 64, Loss: 10.807523470658522\n",
      "Epoch 65, Loss: 10.665939257695126\n",
      "Epoch 66, Loss: 9.889406974499042\n",
      "Epoch 67, Loss: 9.783325048593374\n",
      "Epoch 68, Loss: 12.823865413665771\n",
      "Epoch 69, Loss: 10.078045404874361\n",
      "Epoch 70, Loss: 10.880476566461416\n",
      "Epoch 71, Loss: 10.033011289743277\n",
      "Epoch 72, Loss: 8.543298482894897\n",
      "Epoch 73, Loss: 8.668206728421724\n",
      "Epoch 74, Loss: 9.248963686136099\n",
      "Epoch 75, Loss: 8.23557213636545\n",
      "Epoch 76, Loss: 8.369305225519033\n",
      "Epoch 77, Loss: 8.383487151219295\n",
      "Epoch 78, Loss: 9.780612156941341\n",
      "Epoch 79, Loss: 15.344556423333975\n",
      "Epoch 80, Loss: 14.083382496467003\n",
      "Epoch 81, Loss: 10.664090394973755\n",
      "Epoch 82, Loss: 8.451230801068819\n",
      "Epoch 83, Loss: 8.055698064657358\n",
      "Epoch 84, Loss: 8.937640978739811\n",
      "Epoch 85, Loss: 9.378558048835167\n",
      "Epoch 86, Loss: 8.076244647686298\n",
      "Epoch 87, Loss: 8.127793312072754\n",
      "Epoch 88, Loss: 7.402657783948458\n",
      "Epoch 89, Loss: 7.146853685379028\n",
      "Epoch 90, Loss: 7.640316046201265\n",
      "Epoch 91, Loss: 7.783728746267466\n",
      "Epoch 92, Loss: 7.314935702543992\n",
      "Epoch 93, Loss: 7.390534144181472\n",
      "Epoch 94, Loss: 7.7141026350168085\n",
      "Epoch 95, Loss: 9.376568794250488\n",
      "Epoch 96, Loss: 8.632457733154297\n",
      "Epoch 97, Loss: 10.87819218635559\n",
      "Epoch 98, Loss: 9.718036835010235\n",
      "Epoch 99, Loss: 8.60208639731774\n",
      "Epoch 100, Loss: 9.23821546481206\n",
      "Epoch 101, Loss: 10.121572054349459\n",
      "Epoch 102, Loss: 8.756630714123066\n",
      "Epoch 103, Loss: 7.418802132973304\n",
      "Epoch 104, Loss: 6.945186266532311\n",
      "Epoch 105, Loss: 7.77353626031142\n",
      "Epoch 106, Loss: 7.909711104172927\n",
      "Epoch 107, Loss: 7.499843927530142\n",
      "Epoch 108, Loss: 6.809325750057514\n",
      "Epoch 109, Loss: 7.135915407767663\n",
      "Epoch 110, Loss: 7.376159484569843\n",
      "Epoch 111, Loss: 7.242189829166119\n",
      "Epoch 112, Loss: 8.324492913026075\n",
      "Epoch 113, Loss: 8.798390498528114\n",
      "Epoch 114, Loss: 7.5401411423316365\n",
      "Epoch 115, Loss: 7.661122395442082\n",
      "Epoch 116, Loss: 8.126646757125854\n",
      "Epoch 117, Loss: 8.812560283220732\n",
      "Epoch 118, Loss: 9.750004273194532\n",
      "Epoch 119, Loss: 9.9275245483105\n",
      "Epoch 120, Loss: 8.29168547116793\n",
      "Epoch 121, Loss: 8.318001838830801\n",
      "Epoch 122, Loss: 8.687752411915707\n",
      "Epoch 123, Loss: 8.313240601466251\n",
      "Epoch 124, Loss: 7.070201837099516\n",
      "Epoch 125, Loss: 6.515819127743061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:54:30,827] Trial 98 finished with value: 18.337572366233875 and parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 67, 'hidden_dim': 86, 'epochs': 127, 'causal_reg': 0.8866674602161182, 'learning_rate': 0.0037838983469805844}. Best is trial 92 with value: 16.88326444886553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126, Loss: 6.787474834001982\n",
      "Epoch 127, Loss: 7.504777083030114\n",
      "Epoch 1, Loss: 324.60590010422925\n",
      "Epoch 2, Loss: 121.544705757728\n",
      "Epoch 3, Loss: 85.5263855273907\n",
      "Epoch 4, Loss: 69.21685438889723\n",
      "Epoch 5, Loss: 62.886167819683365\n",
      "Epoch 6, Loss: 57.71723226400522\n",
      "Epoch 7, Loss: 51.66230077009935\n",
      "Epoch 8, Loss: 46.32523419306828\n",
      "Epoch 9, Loss: 42.93208617430467\n",
      "Epoch 10, Loss: 40.06256734407865\n",
      "Epoch 11, Loss: 37.329230528611404\n",
      "Epoch 12, Loss: 37.333514580359825\n",
      "Epoch 13, Loss: 34.022082768953766\n",
      "Epoch 14, Loss: 38.79668485201322\n",
      "Epoch 15, Loss: 31.238061097952034\n",
      "Epoch 16, Loss: 28.09033489227295\n",
      "Epoch 17, Loss: 27.559995284447304\n",
      "Epoch 18, Loss: 27.108303400186394\n",
      "Epoch 19, Loss: 28.059586084806003\n",
      "Epoch 20, Loss: 23.08883391893827\n",
      "Epoch 21, Loss: 20.707614715282734\n",
      "Epoch 22, Loss: 22.85562016413762\n",
      "Epoch 23, Loss: 20.1578647173368\n",
      "Epoch 24, Loss: 18.576426909520077\n",
      "Epoch 25, Loss: 18.997704615959755\n",
      "Epoch 26, Loss: 18.123489453242374\n",
      "Epoch 27, Loss: 17.0590348977309\n",
      "Epoch 28, Loss: 17.389243896190937\n",
      "Epoch 29, Loss: 18.624360157893253\n",
      "Epoch 30, Loss: 17.653069276076096\n",
      "Epoch 31, Loss: 17.57796942270719\n",
      "Epoch 32, Loss: 16.649278934185322\n",
      "Epoch 33, Loss: 14.84968719115624\n",
      "Epoch 34, Loss: 14.01696280332712\n",
      "Epoch 35, Loss: 16.663503353412334\n",
      "Epoch 36, Loss: 13.837614059448242\n",
      "Epoch 37, Loss: 13.262159934410683\n",
      "Epoch 38, Loss: 12.46683093217703\n",
      "Epoch 39, Loss: 12.627199649810791\n",
      "Epoch 40, Loss: 13.06536575464102\n",
      "Epoch 41, Loss: 12.685677748460035\n",
      "Epoch 42, Loss: 11.824052077073317\n",
      "Epoch 43, Loss: 13.691663338587833\n",
      "Epoch 44, Loss: 11.01929022715642\n",
      "Epoch 45, Loss: 11.984456043977003\n",
      "Epoch 46, Loss: 12.291442394256592\n",
      "Epoch 47, Loss: 12.544250965118408\n",
      "Epoch 48, Loss: 11.183844419626089\n",
      "Epoch 49, Loss: 11.600264329176683\n",
      "Epoch 50, Loss: 10.912100535172682\n",
      "Epoch 51, Loss: 11.14253075306232\n",
      "Epoch 52, Loss: 11.051150028522198\n",
      "Epoch 53, Loss: 11.997869124779335\n",
      "Epoch 54, Loss: 11.984512622539814\n",
      "Epoch 55, Loss: 10.376528978347778\n",
      "Epoch 56, Loss: 14.80673415844257\n",
      "Epoch 57, Loss: 15.610212729527401\n",
      "Epoch 58, Loss: 12.201836365919847\n",
      "Epoch 59, Loss: 12.227890308086689\n",
      "Epoch 60, Loss: 10.378404562289898\n",
      "Epoch 61, Loss: 9.315558140094463\n",
      "Epoch 62, Loss: 9.067190353686993\n",
      "Epoch 63, Loss: 9.615417352089516\n",
      "Epoch 64, Loss: 10.346895969831026\n",
      "Epoch 65, Loss: 10.931490953151997\n",
      "Epoch 66, Loss: 11.444145862872784\n",
      "Epoch 67, Loss: 9.354278105955858\n",
      "Epoch 68, Loss: 10.061578860649696\n",
      "Epoch 69, Loss: 9.528969416251549\n",
      "Epoch 70, Loss: 11.042771412776066\n",
      "Epoch 71, Loss: 9.86042305139395\n",
      "Epoch 72, Loss: 10.490343809127808\n",
      "Epoch 73, Loss: 8.726959998791035\n",
      "Epoch 74, Loss: 8.28483220247122\n",
      "Epoch 75, Loss: 8.143857570794912\n",
      "Epoch 76, Loss: 8.142853058301485\n",
      "Epoch 77, Loss: 9.224665751824013\n",
      "Epoch 78, Loss: 8.302317142486572\n",
      "Epoch 79, Loss: 8.200325232285719\n",
      "Epoch 80, Loss: 8.831017604241005\n",
      "Epoch 81, Loss: 7.728699262325581\n",
      "Epoch 82, Loss: 8.246133235784677\n",
      "Epoch 83, Loss: 9.120262916271503\n",
      "Epoch 84, Loss: 10.639088025459877\n",
      "Epoch 85, Loss: 13.016165586618277\n",
      "Epoch 86, Loss: 13.678583640318651\n",
      "Epoch 87, Loss: 13.953682514337393\n",
      "Epoch 88, Loss: 11.044932145338793\n",
      "Epoch 89, Loss: 9.870819458594688\n",
      "Epoch 90, Loss: 8.983756945683407\n",
      "Epoch 91, Loss: 8.093489298453697\n",
      "Epoch 92, Loss: 7.151990156907302\n",
      "Epoch 93, Loss: 7.453730968328623\n",
      "Epoch 94, Loss: 7.935175125415508\n",
      "Epoch 95, Loss: 7.788816158588116\n",
      "Epoch 96, Loss: 7.57461692736699\n",
      "Epoch 97, Loss: 7.721862316131592\n",
      "Epoch 98, Loss: 7.735370269188514\n",
      "Epoch 99, Loss: 7.7271470840160665\n",
      "Epoch 100, Loss: 7.643973350524902\n",
      "Epoch 101, Loss: 12.16235876083374\n",
      "Epoch 102, Loss: 12.078844143794132\n",
      "Epoch 103, Loss: 8.873389427478497\n",
      "Epoch 104, Loss: 8.445059519547682\n",
      "Epoch 105, Loss: 7.5654593247633715\n",
      "Epoch 106, Loss: 6.934100939677312\n",
      "Epoch 107, Loss: 6.944291866742647\n",
      "Epoch 108, Loss: 6.90625601548415\n",
      "Epoch 109, Loss: 6.60488774226262\n",
      "Epoch 110, Loss: 7.141079572530893\n",
      "Epoch 111, Loss: 7.348163182918842\n",
      "Epoch 112, Loss: 9.24108131115253\n",
      "Epoch 113, Loss: 10.321487848575298\n",
      "Epoch 114, Loss: 8.737170897997343\n",
      "Epoch 115, Loss: 8.723165292006273\n",
      "Epoch 116, Loss: 10.255524305196909\n",
      "Epoch 117, Loss: 14.54195814866286\n",
      "Epoch 118, Loss: 16.464016107412483\n",
      "Epoch 119, Loss: 13.70933565726647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 15:54:33,739] Trial 99 finished with value: 18.943947465978624 and parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 67, 'hidden_dim': 87, 'epochs': 127, 'causal_reg': 0.889392475952073, 'learning_rate': 0.0040378214022114296}. Best is trial 92 with value: 16.88326444886553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: 8.709141346124502\n",
      "Epoch 121, Loss: 7.532064584585337\n",
      "Epoch 122, Loss: 6.776101992680476\n",
      "Epoch 123, Loss: 6.871937366632315\n",
      "Epoch 124, Loss: 7.402101480043852\n",
      "Epoch 125, Loss: 7.688502568465013\n",
      "Epoch 126, Loss: 7.172717901376577\n",
      "Epoch 127, Loss: 6.92516933954679\n",
      "Best hyperparameters:  {'latent_dim_z1': 10, 'latent_dim_z2': 67, 'hidden_dim': 70, 'epochs': 121, 'causal_reg': 0.9325055584750703, 'learning_rate': 0.0020515766054521826}\n",
      "Best validation loss:  16.88326444886553\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")  # Minimize the validation loss\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Best validation loss: \", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "291b0817-2b09-49cf-9c14-8010ed621602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'latent_dim_z1': 10, 'latent_dim_z2': 67, 'hidden_dim': 70, 'epochs': 121, 'causal_reg': 0.9325055584750703, 'learning_rate': 0.0020515766054521826}\n",
      "Best validation loss:  16.88326444886553\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Best validation loss: \", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "aba1fae6-bf0a-477f-8048-649a2553834c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "cliponaxis": false,
         "hovertemplate": [
          "causal_reg (FloatDistribution): 0.002229871833740699<extra></extra>",
          "hidden_dim (IntDistribution): 0.004627393805689936<extra></extra>",
          "latent_dim_z2 (IntDistribution): 0.007593266538710536<extra></extra>",
          "epochs (IntDistribution): 0.023419153918088707<extra></extra>",
          "latent_dim_z1 (IntDistribution): 0.18087028389489967<extra></extra>",
          "learning_rate (FloatDistribution): 0.7812600300088705<extra></extra>"
         ],
         "name": "Objective Value",
         "orientation": "h",
         "text": [
          "<0.01",
          "<0.01",
          "<0.01",
          "0.02",
          "0.18",
          "0.78"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          0.002229871833740699,
          0.004627393805689936,
          0.007593266538710536,
          0.023419153918088707,
          0.18087028389489967,
          0.7812600300088705
         ],
         "y": [
          "causal_reg",
          "hidden_dim",
          "latent_dim_z2",
          "epochs",
          "latent_dim_z1",
          "learning_rate"
         ]
        }
       ],
       "layout": {
        "autosize": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Hyperparameter Importances"
        },
        "xaxis": {
         "autorange": true,
         "range": [
          0,
          0.8223789789567058
         ],
         "title": {
          "text": "Hyperparameter Importance"
         },
         "type": "linear"
        },
        "yaxis": {
         "autorange": true,
         "range": [
          -0.5,
          5.5
         ],
         "title": {
          "text": "Hyperparameter"
         },
         "type": "category"
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"43e9dbc8-7e75-4201-8b01-ec4118054507\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"43e9dbc8-7e75-4201-8b01-ec4118054507\")) {                    Plotly.newPlot(                        \"43e9dbc8-7e75-4201-8b01-ec4118054507\",                        [{\"cliponaxis\":false,\"hovertemplate\":[\"causal_reg (FloatDistribution): 0.002229871833740699\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"hidden_dim (IntDistribution): 0.004627393805689936\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"latent_dim_z2 (IntDistribution): 0.007593266538710536\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"epochs (IntDistribution): 0.023419153918088707\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"latent_dim_z1 (IntDistribution): 0.18087028389489967\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"learning_rate (FloatDistribution): 0.7812600300088705\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"],\"name\":\"Objective Value\",\"orientation\":\"h\",\"text\":[\"\\u003c0.01\",\"\\u003c0.01\",\"\\u003c0.01\",\"0.02\",\"0.18\",\"0.78\"],\"textposition\":\"outside\",\"x\":[0.002229871833740699,0.004627393805689936,0.007593266538710536,0.023419153918088707,0.18087028389489967,0.7812600300088705],\"y\":[\"causal_reg\",\"hidden_dim\",\"latent_dim_z2\",\"epochs\",\"latent_dim_z1\",\"learning_rate\"],\"type\":\"bar\"}],                        {\"title\":{\"text\":\"Hyperparameter Importances\"},\"xaxis\":{\"title\":{\"text\":\"Hyperparameter Importance\"}},\"yaxis\":{\"title\":{\"text\":\"Hyperparameter\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('43e9dbc8-7e75-4201-8b01-ec4118054507');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optuna.visualization.plot_optimization_history(study)\n",
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e17a12d-ce23-4db6-9f91-8fe49711de75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 342.8579707512489\n",
      "Epoch 2, Loss: 129.2298360971304\n",
      "Epoch 3, Loss: 88.45502031766452\n",
      "Epoch 4, Loss: 73.79098180624155\n",
      "Epoch 5, Loss: 65.06774359482985\n",
      "Epoch 6, Loss: 56.68579145578238\n",
      "Epoch 7, Loss: 46.46924899174617\n",
      "Epoch 8, Loss: 44.480110535254845\n",
      "Epoch 9, Loss: 39.200793999892014\n",
      "Epoch 10, Loss: 34.45734251462496\n",
      "Epoch 11, Loss: 32.83046472989596\n",
      "Epoch 12, Loss: 31.32878982103788\n",
      "Epoch 13, Loss: 29.982693378741924\n",
      "Epoch 14, Loss: 27.2298552439763\n",
      "Epoch 15, Loss: 25.00336764408992\n",
      "Epoch 16, Loss: 21.68213811287513\n",
      "Epoch 17, Loss: 20.155039053696854\n",
      "Epoch 18, Loss: 21.111775948451115\n",
      "Epoch 19, Loss: 19.963946855985203\n",
      "Epoch 20, Loss: 18.300419807434082\n",
      "Epoch 21, Loss: 17.75965536557711\n",
      "Epoch 22, Loss: 15.992747893700233\n",
      "Epoch 23, Loss: 13.992955354543833\n",
      "Epoch 24, Loss: 15.014331689247719\n",
      "Epoch 25, Loss: 14.862646909860464\n",
      "Epoch 26, Loss: 13.18836127794706\n",
      "Epoch 27, Loss: 12.372035796825703\n",
      "Epoch 28, Loss: 12.088921620295597\n",
      "Epoch 29, Loss: 12.672788363236647\n",
      "Epoch 30, Loss: 12.841939706068773\n",
      "Epoch 31, Loss: 11.2524699981396\n",
      "Epoch 32, Loss: 11.904292895243717\n",
      "Epoch 33, Loss: 9.904255188428438\n",
      "Epoch 34, Loss: 10.235421070685753\n",
      "Epoch 35, Loss: 9.92544308075538\n",
      "Epoch 36, Loss: 10.659690636854906\n",
      "Epoch 37, Loss: 12.015065284875723\n",
      "Epoch 38, Loss: 10.928983468275804\n",
      "Epoch 39, Loss: 8.975945839515099\n",
      "Epoch 40, Loss: 8.612168312072754\n",
      "Epoch 41, Loss: 8.240865010481615\n",
      "Epoch 42, Loss: 7.602196344962487\n",
      "Epoch 43, Loss: 7.457636466393104\n",
      "Epoch 44, Loss: 7.30478679216825\n",
      "Epoch 45, Loss: 8.439375162124634\n",
      "Epoch 46, Loss: 8.275314092636108\n",
      "Epoch 47, Loss: 9.418454922162569\n",
      "Epoch 48, Loss: 9.268373581079336\n",
      "Epoch 49, Loss: 7.746368830020611\n",
      "Epoch 50, Loss: 7.3697456029745245\n",
      "Epoch 51, Loss: 7.050983282235952\n",
      "Epoch 52, Loss: 7.262920948175283\n",
      "Epoch 53, Loss: 7.127545980306772\n",
      "Epoch 54, Loss: 7.3539874737079325\n",
      "Epoch 55, Loss: 6.900475520354051\n",
      "Epoch 56, Loss: 6.730660291818472\n",
      "Epoch 57, Loss: 6.917621227411123\n",
      "Epoch 58, Loss: 7.314575397051298\n",
      "Epoch 59, Loss: 8.366098220531757\n",
      "Epoch 60, Loss: 8.233444085487953\n",
      "Epoch 61, Loss: 8.371640828939585\n",
      "Epoch 62, Loss: 8.30500551370474\n",
      "Epoch 63, Loss: 7.769966987463144\n",
      "Epoch 64, Loss: 7.111739635467529\n",
      "Epoch 65, Loss: 6.798530596953172\n",
      "Epoch 66, Loss: 7.1173337789682245\n",
      "Epoch 67, Loss: 10.285452622633715\n",
      "Epoch 68, Loss: 10.163895020118126\n",
      "Epoch 69, Loss: 10.479265543130728\n",
      "Epoch 70, Loss: 8.159020405549269\n",
      "Epoch 71, Loss: 7.5212503763345575\n",
      "Epoch 72, Loss: 6.859062304863563\n",
      "Epoch 73, Loss: 6.6892397953913765\n",
      "Epoch 74, Loss: 6.1799113016862135\n",
      "Epoch 75, Loss: 6.065423085139348\n",
      "Epoch 76, Loss: 6.219405027536245\n",
      "Epoch 77, Loss: 6.476339982106135\n",
      "Epoch 78, Loss: 6.261607977060171\n",
      "Epoch 79, Loss: 6.34021625152001\n",
      "Epoch 80, Loss: 6.693413734436035\n",
      "Epoch 81, Loss: 6.756278093044575\n",
      "Epoch 82, Loss: 7.991652708787185\n",
      "Epoch 83, Loss: 7.889413485160241\n",
      "Epoch 84, Loss: 7.391170226610624\n",
      "Epoch 85, Loss: 7.102553495993981\n",
      "Epoch 86, Loss: 6.979230844057524\n",
      "Epoch 87, Loss: 6.681376255475557\n",
      "Epoch 88, Loss: 6.336771414830134\n",
      "Epoch 89, Loss: 6.2642837304335375\n",
      "Epoch 90, Loss: 6.389237293830285\n",
      "Epoch 91, Loss: 6.162159424561721\n",
      "Epoch 92, Loss: 6.117108290012066\n",
      "Epoch 93, Loss: 6.237667193779578\n",
      "Epoch 94, Loss: 6.709579064295842\n",
      "Epoch 95, Loss: 6.279786274983333\n",
      "Epoch 96, Loss: 6.249705498035137\n",
      "Epoch 97, Loss: 6.739457368850708\n",
      "Epoch 98, Loss: 6.505724815221933\n",
      "Epoch 99, Loss: 6.535477014688345\n",
      "Epoch 100, Loss: 6.938279812152569\n",
      "Epoch 101, Loss: 6.721527374707735\n",
      "Epoch 102, Loss: 7.051637429457444\n",
      "Epoch 103, Loss: 7.140133527609018\n",
      "Epoch 104, Loss: 6.7752913144918585\n",
      "Epoch 105, Loss: 6.765717964905959\n",
      "Epoch 106, Loss: 6.9208355866945706\n",
      "Epoch 107, Loss: 7.437359901574942\n",
      "Epoch 108, Loss: 11.345743674498339\n",
      "Epoch 109, Loss: 11.240354813062227\n",
      "Epoch 110, Loss: 9.474579077500563\n",
      "Epoch 111, Loss: 7.626253990026621\n",
      "Epoch 112, Loss: 7.0529854114239035\n",
      "Epoch 113, Loss: 6.6288902759552\n",
      "Epoch 114, Loss: 6.196014587695782\n",
      "Epoch 115, Loss: 6.049184927573571\n",
      "Epoch 116, Loss: 5.858775505652795\n",
      "Epoch 117, Loss: 5.69444201542781\n",
      "Epoch 118, Loss: 5.876308881319487\n",
      "Epoch 119, Loss: 5.870095839867225\n",
      "Epoch 120, Loss: 6.104273282564604\n",
      "Epoch 121, Loss: 6.969495754975539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.551978626905996"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CLR attempt\n",
    "best_latent_dim_z1 = 10\n",
    "best_latent_dim_z2 = 67\n",
    "best_hidden_dim = 70\n",
    "best_epochs = 121\n",
    "best_learning_rate = 0.0020515766054521826\n",
    "best_causal_reg = 0.9\n",
    "\n",
    "model_clr = ModifiedVAE(614, best_latent_dim_z1,best_latent_dim_z2,best_hidden_dim)\n",
    "optimizer = torch.optim.Adam(model_clr.parameters(), lr=best_learning_rate)\n",
    "\n",
    "train(model_clr, data_loader, optimizer, best_epochs, best_causal_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dfc89ee-d3b8-4d2b-bdae-665a60eff15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clr.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed_data = []\n",
    "    latent_z1 = []\n",
    "    latent_z2 = []\n",
    "    for x, _ in data_loader:\n",
    "        recon_x, mu_z1, logvar_z1, mu_z2, logvar_z2 = model_clr(x, _)\n",
    "        latent_z1.append(mu_z1)\n",
    "        latent_z2.append(mu_z2)\n",
    "        reconstructed_data.append(recon_x)\n",
    "        \n",
    "    reconstructed_data = torch.cat(reconstructed_data).numpy()\n",
    "    latent_z1 = torch.cat(latent_z1).numpy()\n",
    "    latent_z2 = torch.cat(latent_z2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e48a72c-4ee4-4c91-850b-dafd0c68a9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 Classifier Results:\n",
      "[[  0  87]\n",
      " [  0 157]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.64      1.00      0.78       157\n",
      "\n",
      "    accuracy                           0.64       244\n",
      "   macro avg       0.32      0.50      0.39       244\n",
      "weighted avg       0.41      0.64      0.50       244\n",
      "\n",
      "Z2 Classifier Results:\n",
      "[[  0  87]\n",
      " [  0 157]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.64      1.00      0.78       157\n",
      "\n",
      "    accuracy                           0.64       244\n",
      "   macro avg       0.32      0.50      0.39       244\n",
      "weighted avg       0.41      0.64      0.50       244\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Test with Z1\n",
    "X_train_z1, X_test_z1, y_train_z1, y_test_z1 = train_test_split(latent_z1, Y_labels, test_size=0.3, random_state=42)\n",
    "classifier_z1 = LogisticRegression(max_iter=1000)\n",
    "classifier_z1.fit(X_train_z1, y_train_z1)\n",
    "\n",
    "y_pred_z1 = classifier_z1.predict(X_test_z1)\n",
    "\n",
    "print(\"Z1 Classifier Results:\")\n",
    "print(confusion_matrix(y_test_z1, y_pred_z1))\n",
    "print(classification_report(y_test_z1, y_pred_z1))\n",
    "\n",
    "# Test with Z2\n",
    "X_train_z2, X_test_z2, y_train_z2, y_test_z2 = train_test_split(latent_z2, Y_labels, test_size=0.3, random_state=42)\n",
    "classifier_z2 = LogisticRegression(max_iter=1000)\n",
    "classifier_z2.fit(X_train_z2, y_train_z2)\n",
    "\n",
    "y_pred_z2 = classifier_z2.predict(X_test_z2)\n",
    "\n",
    "print(\"Z2 Classifier Results:\")\n",
    "print(confusion_matrix(y_test_z2, y_pred_z2))\n",
    "print(classification_report(y_test_z2, y_pred_z2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5051b7c6-ef29-4ea0-8540-bc0191fc83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ILR attempt\n",
    "X_tensor = torch.tensor(ilr_transformed_values, dtype=torch.float32) \n",
    "Y_tensor = torch.tensor(Y_labels, dtype=torch.float32)             \n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "c94411f6-bdd5-422a-b2cf-27ae8ab2f55b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:02:18,293] A new study created in memory with name: no-name-412fa711-90f5-4c50-8142-2345653b0cd5\n",
      "/var/folders/02/jj1jlsn97sj550kl_82_8lp40000gn/T/ipykernel_11945/917332750.py:9: FutureWarning:\n",
      "\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 13970.083186222957\n",
      "Epoch 2, Loss: 8178.947988656851\n",
      "Epoch 3, Loss: 5939.270362267127\n",
      "Epoch 4, Loss: 4630.290569598858\n",
      "Epoch 5, Loss: 3836.404461200421\n",
      "Epoch 6, Loss: 3316.2481595552886\n",
      "Epoch 7, Loss: 2948.6704993614785\n",
      "Epoch 8, Loss: 2677.603233924279\n",
      "Epoch 9, Loss: 2475.8347825270434\n",
      "Epoch 10, Loss: 2323.8569617638223\n",
      "Epoch 11, Loss: 2208.426783635066\n",
      "Epoch 12, Loss: 2119.4126328688403\n",
      "Epoch 13, Loss: 2050.0308462289663\n",
      "Epoch 14, Loss: 1995.1764690692607\n",
      "Epoch 15, Loss: 1951.701387845553\n",
      "Epoch 16, Loss: 1915.8888901930588\n",
      "Epoch 17, Loss: 1886.996844951923\n",
      "Epoch 18, Loss: 1863.264120248648\n",
      "Epoch 19, Loss: 1843.5788034292368\n",
      "Epoch 20, Loss: 1826.4140014648438\n",
      "Epoch 21, Loss: 1812.233602670523\n",
      "Epoch 22, Loss: 1800.1751685509314\n",
      "Epoch 23, Loss: 1789.217533991887\n",
      "Epoch 24, Loss: 1779.7351801945613\n",
      "Epoch 25, Loss: 1770.99611722506\n",
      "Epoch 26, Loss: 1763.3120563213643\n",
      "Epoch 27, Loss: 1756.0624436598557\n",
      "Epoch 28, Loss: 1749.70557814378\n",
      "Epoch 29, Loss: 1743.64744919997\n",
      "Epoch 30, Loss: 1737.5497225247896\n",
      "Epoch 31, Loss: 1732.3059809758113\n",
      "Epoch 32, Loss: 1726.8882821890024\n",
      "Epoch 33, Loss: 1721.9893470177283\n",
      "Epoch 34, Loss: 1717.0277498685396\n",
      "Epoch 35, Loss: 1712.2988492525542\n",
      "Epoch 36, Loss: 1707.6547241210938\n",
      "Epoch 37, Loss: 1703.0309377817007\n",
      "Epoch 38, Loss: 1698.057849590595\n",
      "Epoch 39, Loss: 1694.0279705341045\n",
      "Epoch 40, Loss: 1689.176060603215\n",
      "Epoch 41, Loss: 1684.47952740009\n",
      "Epoch 42, Loss: 1679.7770150991587\n",
      "Epoch 43, Loss: 1675.4530616173377\n",
      "Epoch 44, Loss: 1670.3058166503906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:02:19,573] Trial 0 finished with value: 2412.334492218801 and parameters: {'latent_dim_z1': 37, 'latent_dim_z2': 28, 'hidden_dim': 258, 'epochs': 51, 'causal_reg': 0.35234997088192144, 'learning_rate': 1.0296234334277778e-05}. Best is trial 0 with value: 2412.334492218801.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Loss: 1665.829355093149\n",
      "Epoch 46, Loss: 1660.792748084435\n",
      "Epoch 47, Loss: 1656.4595149113582\n",
      "Epoch 48, Loss: 1651.1680391751802\n",
      "Epoch 49, Loss: 1645.845196063702\n",
      "Epoch 50, Loss: 1640.4232928936299\n",
      "Epoch 51, Loss: 1635.3319467397837\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:02:21,624] Trial 1 failed with parameters: {'latent_dim_z1': 32, 'latent_dim_z2': 21, 'hidden_dim': 103, 'epochs': 102, 'causal_reg': 0.05645624940105878, 'learning_rate': 0.0775995672478991} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:02:21,624] Trial 1 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 1, Loss: 32107.019268329328\n",
      "Epoch 2, Loss: 6925.193716195913\n",
      "Epoch 3, Loss: 4626.567246657151\n",
      "Epoch 4, Loss: 3492.9190439077524\n",
      "Epoch 5, Loss: 2822.919466458834\n",
      "Epoch 6, Loss: 2382.1624990609976\n",
      "Epoch 7, Loss: 2071.1391014685996\n",
      "Epoch 8, Loss: 1845.5904869666467\n",
      "Epoch 9, Loss: 1669.535891019381\n",
      "Epoch 10, Loss: 1532.854482797476\n",
      "Epoch 11, Loss: 1422.7146582970252\n",
      "Epoch 12, Loss: 1334.9593318058894\n",
      "Epoch 13, Loss: 1260.4895535982573\n",
      "Epoch 14, Loss: 1198.0076458270732\n",
      "Epoch 15, Loss: 1146.589087853065\n",
      "Epoch 16, Loss: 1099.5193880521333\n",
      "Epoch 17, Loss: 1059.748983529898\n",
      "Epoch 18, Loss: 1024.5026468130259\n",
      "Epoch 19, Loss: 992.4295313908503\n",
      "Epoch 20, Loss: 963.4025104229266\n",
      "Epoch 21, Loss: 936.9680762657753\n",
      "Epoch 22, Loss: 912.3323375995343\n",
      "Epoch 23, Loss: 889.4528585580679\n",
      "Epoch 24, Loss: 866.7666426438552\n",
      "Epoch 25, Loss: 846.3115797776443\n",
      "Epoch 26, Loss: 824.3942554180438\n",
      "Epoch 27, Loss: 805.2462205153245\n",
      "Epoch 28, Loss: 786.3942272479718\n",
      "Epoch 29, Loss: 768.5075272780198\n",
      "Epoch 30, Loss: 751.6313699575571\n",
      "Epoch 31, Loss: 735.5273003211388\n",
      "Epoch 32, Loss: 719.1969440166766\n",
      "Epoch 33, Loss: 704.8880192683293\n",
      "Epoch 34, Loss: 691.1441380427434\n",
      "Epoch 35, Loss: 677.9171318641075\n",
      "Epoch 36, Loss: 665.6095135028546\n",
      "Epoch 37, Loss: 654.6270082913912\n",
      "Epoch 38, Loss: 643.7000814584585\n",
      "Epoch 39, Loss: 632.8447875976562\n",
      "Epoch 40, Loss: 623.0696845421425\n",
      "Epoch 41, Loss: 613.7866891714243\n",
      "Epoch 42, Loss: 605.3004467303937\n",
      "Epoch 43, Loss: 596.3352238581731\n",
      "Epoch 44, Loss: 587.9907660851112\n",
      "Epoch 45, Loss: 580.4636066143329\n",
      "Epoch 46, Loss: 572.3928915170522\n",
      "Epoch 47, Loss: 564.5155803973859\n",
      "Epoch 48, Loss: 558.1323746901292\n",
      "Epoch 49, Loss: 551.0749992957482\n",
      "Epoch 50, Loss: 543.8868314302885\n",
      "Epoch 51, Loss: 537.4668133075421\n",
      "Epoch 52, Loss: 530.8481973501353\n",
      "Epoch 53, Loss: 525.1186734713041\n",
      "Epoch 54, Loss: 519.3560873178335\n",
      "Epoch 55, Loss: 512.7943819486178\n",
      "Epoch 56, Loss: 507.17352177546576\n",
      "Epoch 57, Loss: 501.5301454984225\n",
      "Epoch 58, Loss: 496.66508014385516\n",
      "Epoch 59, Loss: 491.15238600510816\n",
      "Epoch 60, Loss: 486.2436769925631\n",
      "Epoch 61, Loss: 481.67537747896637\n",
      "Epoch 62, Loss: 476.6878392146184\n",
      "Epoch 63, Loss: 472.2873840332031\n",
      "Epoch 64, Loss: 468.26050861065204\n",
      "Epoch 65, Loss: 464.6395592322716\n",
      "Epoch 66, Loss: 460.17650310809796\n",
      "Epoch 67, Loss: 455.7596106896034\n",
      "Epoch 68, Loss: 450.74805039625903\n",
      "Epoch 69, Loss: 447.03223595252405\n",
      "Epoch 70, Loss: 444.35378910945013\n",
      "Epoch 71, Loss: 440.0704416128305\n",
      "Epoch 72, Loss: 435.8084658109225\n",
      "Epoch 73, Loss: 432.3014420729417\n",
      "Epoch 74, Loss: 428.59070763221155\n",
      "Epoch 75, Loss: 426.67755009577826\n",
      "Epoch 76, Loss: 422.05123549241284\n",
      "Epoch 77, Loss: 419.1703655536358\n",
      "Epoch 78, Loss: 416.3473686805138\n",
      "Epoch 79, Loss: 413.6038583608774\n",
      "Epoch 80, Loss: 410.5897205059345\n",
      "Epoch 81, Loss: 407.8331486628606\n",
      "Epoch 82, Loss: 405.9131892277644\n",
      "Epoch 83, Loss: 404.6990192119892\n",
      "Epoch 84, Loss: 401.3072040264423\n",
      "Epoch 85, Loss: 400.28260920597955\n",
      "Epoch 86, Loss: 396.4415600116436\n",
      "Epoch 87, Loss: 395.67708646334137\n",
      "Epoch 88, Loss: 393.12393540602466\n",
      "Epoch 89, Loss: 391.0571617713341\n",
      "Epoch 90, Loss: 389.0364555945763\n",
      "Epoch 91, Loss: 387.79276569073016\n",
      "Epoch 92, Loss: 386.58411583533655\n",
      "Epoch 93, Loss: 384.93919372558594\n",
      "Epoch 94, Loss: 383.62228041428784\n",
      "Epoch 95, Loss: 381.5323462853065\n",
      "Epoch 96, Loss: 381.26129737267127\n",
      "Epoch 97, Loss: 379.0651385967548\n",
      "Epoch 98, Loss: 377.8604736328125\n",
      "Epoch 99, Loss: 376.12084139310394\n",
      "Epoch 100, Loss: 374.9710458608774\n",
      "Epoch 101, Loss: 373.60069392277643\n",
      "Epoch 102, Loss: 373.5309741680439\n",
      "Epoch 103, Loss: 375.9581803541917\n",
      "Epoch 104, Loss: 370.27229074331433\n",
      "Epoch 105, Loss: 369.20677067683295\n",
      "Epoch 106, Loss: 367.6712892972506\n",
      "Epoch 107, Loss: 367.16111637995795\n",
      "Epoch 108, Loss: 365.80031057504505\n",
      "Epoch 109, Loss: 364.503655066857\n",
      "Epoch 110, Loss: 364.0442082331731\n",
      "Epoch 111, Loss: 362.30015329214245\n",
      "Epoch 112, Loss: 361.9922403188852\n",
      "Epoch 113, Loss: 360.72325134277344\n",
      "Epoch 114, Loss: 359.936274601863\n",
      "Epoch 115, Loss: 358.2936336810772\n",
      "Epoch 116, Loss: 357.0148691030649\n",
      "Epoch 117, Loss: 356.10760732797473\n",
      "Epoch 118, Loss: 354.8057133601262\n",
      "Epoch 119, Loss: 354.8568866436298\n",
      "Epoch 120, Loss: 353.17623549241284\n",
      "Epoch 121, Loss: 353.271971482497\n",
      "Epoch 122, Loss: 350.6898322472206\n",
      "Epoch 123, Loss: 350.67387038010816\n",
      "Epoch 124, Loss: 348.4411938007061\n",
      "Epoch 125, Loss: 347.6977996826172\n",
      "Epoch 126, Loss: 346.222901564378\n",
      "Epoch 127, Loss: 345.7419938307542\n",
      "Epoch 128, Loss: 345.11632948655347\n",
      "Epoch 129, Loss: 344.0674861027644\n",
      "Epoch 130, Loss: 342.33603022648737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:02:24,870] Trial 2 finished with value: 934.1151132697736 and parameters: {'latent_dim_z1': 12, 'latent_dim_z2': 58, 'hidden_dim': 236, 'epochs': 135, 'causal_reg': 0.9397297330112627, 'learning_rate': 7.850897155005972e-05}. Best is trial 2 with value: 934.1151132697736.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131, Loss: 340.65255033052887\n",
      "Epoch 132, Loss: 341.4173091008113\n",
      "Epoch 133, Loss: 339.7347875741812\n",
      "Epoch 134, Loss: 338.06314556415265\n",
      "Epoch 135, Loss: 337.48123521071216\n",
      "Epoch 1, Loss: 22347.803100585938\n",
      "Epoch 2, Loss: 5856.173227163462\n",
      "Epoch 3, Loss: 4170.595247708834\n",
      "Epoch 4, Loss: 3561.7598313551684\n",
      "Epoch 5, Loss: 3204.8809908353364\n",
      "Epoch 6, Loss: 2915.1500056340146\n",
      "Epoch 7, Loss: 2660.462897667518\n",
      "Epoch 8, Loss: 2431.739952674279\n",
      "Epoch 9, Loss: 2235.151564378005\n",
      "Epoch 10, Loss: 2074.738337590144\n",
      "Epoch 11, Loss: 1935.7040311373198\n",
      "Epoch 12, Loss: 1829.58248431866\n",
      "Epoch 13, Loss: 1733.159909761869\n",
      "Epoch 14, Loss: 1655.8079223632812\n",
      "Epoch 15, Loss: 1588.6153987004207\n",
      "Epoch 16, Loss: 1533.5462599534255\n",
      "Epoch 17, Loss: 1517.9248492901143\n",
      "Epoch 18, Loss: 1444.35745708759\n",
      "Epoch 19, Loss: 1384.352806678185\n",
      "Epoch 20, Loss: 1355.423083965595\n",
      "Epoch 21, Loss: 1316.1601092998799\n",
      "Epoch 22, Loss: 1289.5918273925781\n",
      "Epoch 23, Loss: 1255.818110539363\n",
      "Epoch 24, Loss: 1247.7235647348257\n",
      "Epoch 25, Loss: 1215.8948645958533\n",
      "Epoch 26, Loss: 1189.145980834961\n",
      "Epoch 27, Loss: 1177.5677971473108\n",
      "Epoch 28, Loss: 1163.977759728065\n",
      "Epoch 29, Loss: 1142.6777907151443\n",
      "Epoch 30, Loss: 1139.5995366023137\n",
      "Epoch 31, Loss: 1116.4525193434495\n",
      "Epoch 32, Loss: 1102.5275491567759\n",
      "Epoch 33, Loss: 1083.3772113506611\n",
      "Epoch 34, Loss: 1058.5169055645283\n",
      "Epoch 35, Loss: 1044.869863656851\n",
      "Epoch 36, Loss: 1052.9350257286658\n",
      "Epoch 37, Loss: 1046.4774240347056\n",
      "Epoch 38, Loss: 1016.8557856633113\n",
      "Epoch 39, Loss: 1001.62451171875\n",
      "Epoch 40, Loss: 989.9213538536659\n",
      "Epoch 41, Loss: 982.5806004450872\n",
      "Epoch 42, Loss: 992.3288198617788\n",
      "Epoch 43, Loss: 972.9629141000601\n",
      "Epoch 44, Loss: 941.3235966609075\n",
      "Epoch 45, Loss: 948.3930499737079\n",
      "Epoch 46, Loss: 964.4786811241737\n",
      "Epoch 47, Loss: 925.6825937124399\n",
      "Epoch 48, Loss: 904.0894376314603\n",
      "Epoch 49, Loss: 895.4420506403997\n",
      "Epoch 50, Loss: 893.5194314809946\n",
      "Epoch 51, Loss: 911.6631141075721\n",
      "Epoch 52, Loss: 894.5959144005409\n",
      "Epoch 53, Loss: 883.5480311467097\n",
      "Epoch 54, Loss: 872.4863656850962\n",
      "Epoch 55, Loss: 876.5107973538912\n",
      "Epoch 56, Loss: 885.8570850078876\n",
      "Epoch 57, Loss: 882.0933297964243\n",
      "Epoch 58, Loss: 869.348636333759\n",
      "Epoch 59, Loss: 852.5599482609675\n",
      "Epoch 60, Loss: 837.5427797757662\n",
      "Epoch 61, Loss: 834.2433753380409\n",
      "Epoch 62, Loss: 849.9156365027794\n",
      "Epoch 63, Loss: 831.9386432354266\n",
      "Epoch 64, Loss: 814.6272606482872\n",
      "Epoch 65, Loss: 810.4979341947115\n",
      "Epoch 66, Loss: 815.7626295823318\n",
      "Epoch 67, Loss: 800.6292302058293\n",
      "Epoch 68, Loss: 801.4939246544471\n",
      "Epoch 69, Loss: 800.9870218130259\n",
      "Epoch 70, Loss: 808.215078500601\n",
      "Epoch 71, Loss: 789.9480590820312\n",
      "Epoch 72, Loss: 794.1644157996544\n",
      "Epoch 73, Loss: 795.85328909067\n",
      "Epoch 74, Loss: 784.8977696345403\n",
      "Epoch 75, Loss: 775.312509390024\n",
      "Epoch 76, Loss: 771.6879695012019\n",
      "Epoch 77, Loss: 777.1929520827073\n",
      "Epoch 78, Loss: 767.4528327355018\n",
      "Epoch 79, Loss: 751.0404252272385\n",
      "Epoch 80, Loss: 769.0079721304087\n",
      "Epoch 81, Loss: 763.4044400728666\n",
      "Epoch 82, Loss: 759.9855734018179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:02:26,752] Trial 3 finished with value: 1497.068525597533 and parameters: {'latent_dim_z1': 77, 'latent_dim_z2': 28, 'hidden_dim': 62, 'epochs': 86, 'causal_reg': 0.8758725709397865, 'learning_rate': 9.77771731589198e-05}. Best is trial 2 with value: 934.1151132697736.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83, Loss: 747.311775794396\n",
      "Epoch 84, Loss: 754.1910388653095\n",
      "Epoch 85, Loss: 739.9575969989484\n",
      "Epoch 86, Loss: 762.5218188946063\n",
      "Epoch 1, Loss: 24994.458233173078\n",
      "Epoch 2, Loss: 5634.996253380408\n",
      "Epoch 3, Loss: 3771.053701547476\n",
      "Epoch 4, Loss: 3144.641277606671\n",
      "Epoch 5, Loss: 2804.2404902531553\n",
      "Epoch 6, Loss: 2555.35112116887\n",
      "Epoch 7, Loss: 2349.5496544471152\n",
      "Epoch 8, Loss: 2171.79157433143\n",
      "Epoch 9, Loss: 2000.2652986966646\n",
      "Epoch 10, Loss: 1843.9087923490083\n",
      "Epoch 11, Loss: 1701.596475454477\n",
      "Epoch 12, Loss: 1584.4523245004507\n",
      "Epoch 13, Loss: 1489.0997079702524\n",
      "Epoch 14, Loss: 1386.7742943396936\n",
      "Epoch 15, Loss: 1315.1390286959136\n",
      "Epoch 16, Loss: 1252.6811899038462\n",
      "Epoch 17, Loss: 1188.4898282564604\n",
      "Epoch 18, Loss: 1126.5224938025842\n",
      "Epoch 19, Loss: 1088.3548537034255\n",
      "Epoch 20, Loss: 1051.6265822190505\n",
      "Epoch 21, Loss: 1014.2078599196213\n",
      "Epoch 22, Loss: 992.2427896352915\n",
      "Epoch 23, Loss: 967.9272930438702\n",
      "Epoch 24, Loss: 952.9507868840144\n",
      "Epoch 25, Loss: 934.3972731370193\n",
      "Epoch 26, Loss: 905.2489624023438\n",
      "Epoch 27, Loss: 908.5338017390325\n",
      "Epoch 28, Loss: 906.0677337646484\n",
      "Epoch 29, Loss: 868.1337010310247\n",
      "Epoch 30, Loss: 864.2459951547476\n",
      "Epoch 31, Loss: 852.6395674485427\n",
      "Epoch 32, Loss: 839.4620549128606\n",
      "Epoch 33, Loss: 834.4941875751202\n",
      "Epoch 34, Loss: 824.198238666241\n",
      "Epoch 35, Loss: 813.3067826491135\n",
      "Epoch 36, Loss: 807.9264373779297\n",
      "Epoch 37, Loss: 798.3700796274038\n",
      "Epoch 38, Loss: 793.5499056302584\n",
      "Epoch 39, Loss: 787.5027113694412\n",
      "Epoch 40, Loss: 781.1167579064003\n",
      "Epoch 41, Loss: 775.8849193866437\n",
      "Epoch 42, Loss: 776.592060969426\n",
      "Epoch 43, Loss: 774.6830796461838\n",
      "Epoch 44, Loss: 760.0691868708684\n",
      "Epoch 45, Loss: 756.9345104510968\n",
      "Epoch 46, Loss: 744.5306255634015\n",
      "Epoch 47, Loss: 748.2769658015325\n",
      "Epoch 48, Loss: 748.7013491117037\n",
      "Epoch 49, Loss: 735.9510251558744\n",
      "Epoch 50, Loss: 728.6705287053035\n",
      "Epoch 51, Loss: 725.2665757399338\n",
      "Epoch 52, Loss: 732.2719245323768\n",
      "Epoch 53, Loss: 719.0951538085938\n",
      "Epoch 54, Loss: 710.1040837214543\n",
      "Epoch 55, Loss: 707.5855971116287\n",
      "Epoch 56, Loss: 701.2400665283203\n",
      "Epoch 57, Loss: 701.1409559983474\n",
      "Epoch 58, Loss: 694.572267972506\n",
      "Epoch 59, Loss: 695.7264404296875\n",
      "Epoch 60, Loss: 685.1969287578876\n",
      "Epoch 61, Loss: 688.9009739802434\n",
      "Epoch 62, Loss: 681.06494140625\n",
      "Epoch 63, Loss: 685.1041529728816\n",
      "Epoch 64, Loss: 675.376710158128\n",
      "Epoch 65, Loss: 673.5524397629958\n",
      "Epoch 66, Loss: 665.9179452749399\n",
      "Epoch 67, Loss: 670.2624910794772\n",
      "Epoch 68, Loss: 668.9563504732572\n",
      "Epoch 69, Loss: 658.378665630634\n",
      "Epoch 70, Loss: 656.82494530311\n",
      "Epoch 71, Loss: 648.4776587853065\n",
      "Epoch 72, Loss: 655.0845383864182\n",
      "Epoch 73, Loss: 651.2099315936749\n",
      "Epoch 74, Loss: 650.1356670673077\n",
      "Epoch 75, Loss: 639.5948533278245\n",
      "Epoch 76, Loss: 641.3719435471755\n",
      "Epoch 77, Loss: 647.4736762413612\n",
      "Epoch 78, Loss: 639.7235694298378\n",
      "Epoch 79, Loss: 637.0198610745944\n",
      "Epoch 80, Loss: 635.4842341496394\n",
      "Epoch 81, Loss: 627.5929858867938\n",
      "Epoch 82, Loss: 629.5012805645282\n",
      "Epoch 83, Loss: 627.1730381892278\n",
      "Epoch 84, Loss: 617.6111238919772\n",
      "Epoch 85, Loss: 614.9439169076772\n",
      "Epoch 86, Loss: 621.543939443735\n",
      "Epoch 87, Loss: 612.2305368276743\n",
      "Epoch 88, Loss: 625.8912365253155\n",
      "Epoch 89, Loss: 647.4917309100812\n",
      "Epoch 90, Loss: 603.7066333477313\n",
      "Epoch 91, Loss: 613.4688075138972\n",
      "Epoch 92, Loss: 602.7834789569562\n",
      "Epoch 93, Loss: 602.121826171875\n",
      "Epoch 94, Loss: 609.6261972280649\n",
      "Epoch 95, Loss: 597.871097271259\n",
      "Epoch 96, Loss: 596.8213606614333\n",
      "Epoch 97, Loss: 594.9553926908053\n",
      "Epoch 98, Loss: 596.9709625244141\n",
      "Epoch 99, Loss: 603.79563669058\n",
      "Epoch 100, Loss: 611.7933701735276\n",
      "Epoch 101, Loss: 583.8539651724009\n",
      "Epoch 102, Loss: 593.9367335392878\n",
      "Epoch 103, Loss: 581.2228557880109\n",
      "Epoch 104, Loss: 583.1737858698918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:02:29,350] Trial 4 finished with value: 1156.225409113197 and parameters: {'latent_dim_z1': 56, 'latent_dim_z2': 71, 'hidden_dim': 71, 'epochs': 111, 'causal_reg': 0.3098806544313528, 'learning_rate': 9.301597480749521e-05}. Best is trial 2 with value: 934.1151132697736.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105, Loss: 588.4723780705378\n",
      "Epoch 106, Loss: 576.2335885854868\n",
      "Epoch 107, Loss: 573.9068357027494\n",
      "Epoch 108, Loss: 581.6471651517428\n",
      "Epoch 109, Loss: 572.9821906456581\n",
      "Epoch 110, Loss: 572.1701918381912\n",
      "Epoch 111, Loss: 581.6002772404597\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:02:33,969] Trial 5 failed with parameters: {'latent_dim_z1': 77, 'latent_dim_z2': 45, 'hidden_dim': 225, 'epochs': 176, 'causal_reg': 0.7958113419633508, 'learning_rate': 0.06813656642076502} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:02:33,970] Trial 5 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:02:36,190] Trial 6 failed with parameters: {'latent_dim_z1': 61, 'latent_dim_z2': 71, 'hidden_dim': 22, 'epochs': 102, 'causal_reg': 0.6241634576899339, 'learning_rate': 0.026147673623714982} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:02:36,190] Trial 6 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 1, Loss: 11386.23854651818\n",
      "Epoch 2, Loss: 2598.936762883113\n",
      "Epoch 3, Loss: 2362.5748924842246\n",
      "Epoch 4, Loss: 2070.955284705529\n",
      "Epoch 5, Loss: 1796.965592604417\n",
      "Epoch 6, Loss: 1536.641366811899\n",
      "Epoch 7, Loss: 1354.7335345928486\n",
      "Epoch 8, Loss: 1255.5974461482122\n",
      "Epoch 9, Loss: 1095.1637291541467\n",
      "Epoch 10, Loss: 1004.2425818810096\n",
      "Epoch 11, Loss: 970.875737116887\n",
      "Epoch 12, Loss: 942.3088684082031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:02:36,543] Trial 7 finished with value: 2063.8611065204327 and parameters: {'latent_dim_z1': 61, 'latent_dim_z2': 27, 'hidden_dim': 206, 'epochs': 15, 'causal_reg': 0.5662306622742451, 'learning_rate': 0.0003224753169565187}. Best is trial 2 with value: 934.1151132697736.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 889.995851956881\n",
      "Epoch 14, Loss: 855.2879368708684\n",
      "Epoch 15, Loss: 837.3984656700721\n",
      "Epoch 1, Loss: 32516.24335186298\n",
      "Epoch 2, Loss: 7717.7942457932695\n",
      "Epoch 3, Loss: 5240.958749624399\n",
      "Epoch 4, Loss: 4039.8910193810098\n",
      "Epoch 5, Loss: 3332.316927396334\n",
      "Epoch 6, Loss: 2875.106668325571\n",
      "Epoch 7, Loss: 2563.246063232422\n",
      "Epoch 8, Loss: 2341.6793189415566\n",
      "Epoch 9, Loss: 2175.1092599722056\n",
      "Epoch 10, Loss: 2048.504617544321\n",
      "Epoch 11, Loss: 1946.369844876803\n",
      "Epoch 12, Loss: 1863.0336256760818\n",
      "Epoch 13, Loss: 1791.5783386230469\n",
      "Epoch 14, Loss: 1729.1471674992488\n",
      "Epoch 15, Loss: 1672.8645958533655\n",
      "Epoch 16, Loss: 1621.0919752854568\n",
      "Epoch 17, Loss: 1573.427983210637\n",
      "Epoch 18, Loss: 1528.3207561786357\n",
      "Epoch 19, Loss: 1485.705073429988\n",
      "Epoch 20, Loss: 1444.5286137507512\n",
      "Epoch 21, Loss: 1405.4966994065505\n",
      "Epoch 22, Loss: 1367.3903667743389\n",
      "Epoch 23, Loss: 1330.3920288085938\n",
      "Epoch 24, Loss: 1295.5226276104268\n",
      "Epoch 25, Loss: 1261.5282358022837\n",
      "Epoch 26, Loss: 1230.7437791090745\n",
      "Epoch 27, Loss: 1199.1154151329627\n",
      "Epoch 28, Loss: 1168.9187340369592\n",
      "Epoch 29, Loss: 1140.8765493539663\n",
      "Epoch 30, Loss: 1114.8345735990083\n",
      "Epoch 31, Loss: 1091.1392681415264\n",
      "Epoch 32, Loss: 1067.1016164926382\n",
      "Epoch 33, Loss: 1045.4474346454326\n",
      "Epoch 34, Loss: 1024.0667372483474\n",
      "Epoch 35, Loss: 1005.4745905949519\n",
      "Epoch 36, Loss: 987.0444852388822\n",
      "Epoch 37, Loss: 968.2444234994741\n",
      "Epoch 38, Loss: 953.6470970740685\n",
      "Epoch 39, Loss: 937.7384068415715\n",
      "Epoch 40, Loss: 921.8460165170522\n",
      "Epoch 41, Loss: 907.1664346548228\n",
      "Epoch 42, Loss: 893.1472766582782\n",
      "Epoch 43, Loss: 880.7191279484675\n",
      "Epoch 44, Loss: 869.3998283973107\n",
      "Epoch 45, Loss: 856.3528383695162\n",
      "Epoch 46, Loss: 844.050044133113\n",
      "Epoch 47, Loss: 834.3023493840144\n",
      "Epoch 48, Loss: 823.8660900409405\n",
      "Epoch 49, Loss: 811.8797478309044\n",
      "Epoch 50, Loss: 802.8923586331881\n",
      "Epoch 51, Loss: 793.9653578538162\n",
      "Epoch 52, Loss: 784.7305180476262\n",
      "Epoch 53, Loss: 778.0076880821815\n",
      "Epoch 54, Loss: 768.6878802959735\n",
      "Epoch 55, Loss: 759.3831388033353\n",
      "Epoch 56, Loss: 752.9266897348257\n",
      "Epoch 57, Loss: 746.4710646409255\n",
      "Epoch 58, Loss: 741.1760500394381\n",
      "Epoch 59, Loss: 731.6670567439153\n",
      "Epoch 60, Loss: 726.098624596229\n",
      "Epoch 61, Loss: 723.592549250676\n",
      "Epoch 62, Loss: 716.3172701322115\n",
      "Epoch 63, Loss: 711.2724832388071\n",
      "Epoch 64, Loss: 705.5769747220553\n",
      "Epoch 65, Loss: 700.3799884502704\n",
      "Epoch 66, Loss: 695.8909548245944\n",
      "Epoch 67, Loss: 691.7036531888522\n",
      "Epoch 68, Loss: 685.0271735558143\n",
      "Epoch 69, Loss: 682.8255603496845\n",
      "Epoch 70, Loss: 677.3756244365985\n",
      "Epoch 71, Loss: 674.490480863131\n",
      "Epoch 72, Loss: 668.996828519381\n",
      "Epoch 73, Loss: 665.1118903526893\n",
      "Epoch 74, Loss: 659.5846768892728\n",
      "Epoch 75, Loss: 657.9252647986779\n",
      "Epoch 76, Loss: 653.4298706054688\n",
      "Epoch 77, Loss: 649.5852309006912\n",
      "Epoch 78, Loss: 646.1826547475962\n",
      "Epoch 79, Loss: 642.4539689284104\n",
      "Epoch 80, Loss: 640.9432995135968\n",
      "Epoch 81, Loss: 636.4657850999099\n",
      "Epoch 82, Loss: 634.1383936955378\n",
      "Epoch 83, Loss: 629.52465233436\n",
      "Epoch 84, Loss: 628.4434391902043\n",
      "Epoch 85, Loss: 624.3055584247296\n",
      "Epoch 86, Loss: 620.9596804105319\n",
      "Epoch 87, Loss: 618.0484595665565\n",
      "Epoch 88, Loss: 613.7459599421575\n",
      "Epoch 89, Loss: 611.9231673020583\n",
      "Epoch 90, Loss: 609.3703296367938\n",
      "Epoch 91, Loss: 609.7068035419171\n",
      "Epoch 92, Loss: 603.7689431997446\n",
      "Epoch 93, Loss: 599.5407644418569\n",
      "Epoch 94, Loss: 595.2227747990535\n",
      "Epoch 95, Loss: 591.2243206317609\n",
      "Epoch 96, Loss: 589.1666036752554\n",
      "Epoch 97, Loss: 586.800290621244\n",
      "Epoch 98, Loss: 584.8280064509465\n",
      "Epoch 99, Loss: 579.8951052152194\n",
      "Epoch 100, Loss: 579.5775580772987\n",
      "Epoch 101, Loss: 580.2505939190204\n",
      "Epoch 102, Loss: 579.6142014723557\n",
      "Epoch 103, Loss: 571.1700075589694\n",
      "Epoch 104, Loss: 569.5576629638672\n",
      "Epoch 105, Loss: 566.8580557016226\n",
      "Epoch 106, Loss: 563.6691471980168\n",
      "Epoch 107, Loss: 559.6371119572566\n",
      "Epoch 108, Loss: 561.6535961444562\n",
      "Epoch 109, Loss: 559.3909747783954\n",
      "Epoch 110, Loss: 557.6381143423228\n",
      "Epoch 111, Loss: 556.8958669809194\n",
      "Epoch 112, Loss: 549.3043541541466\n",
      "Epoch 113, Loss: 547.8923797607422\n",
      "Epoch 114, Loss: 547.2676227276141\n",
      "Epoch 115, Loss: 546.0483903151292\n",
      "Epoch 116, Loss: 542.1014005220853\n",
      "Epoch 117, Loss: 540.91749220628\n",
      "Epoch 118, Loss: 539.1104971078726\n",
      "Epoch 119, Loss: 536.7552208533654\n",
      "Epoch 120, Loss: 537.45360154372\n",
      "Epoch 121, Loss: 532.4445472130409\n",
      "Epoch 122, Loss: 530.1910165640024\n",
      "Epoch 123, Loss: 531.8332801231971\n",
      "Epoch 124, Loss: 527.1323981651893\n",
      "Epoch 125, Loss: 525.6405956561749\n",
      "Epoch 126, Loss: 523.6951716496394\n",
      "Epoch 127, Loss: 523.3807689960187\n",
      "Epoch 128, Loss: 522.7913278432993\n",
      "Epoch 129, Loss: 517.6419501671425\n",
      "Epoch 130, Loss: 518.1935319166917\n",
      "Epoch 131, Loss: 514.9504464956431\n",
      "Epoch 132, Loss: 514.9607532207782\n",
      "Epoch 133, Loss: 524.3234405517578\n",
      "Epoch 134, Loss: 513.0457141582782\n",
      "Epoch 135, Loss: 513.2429621769832\n",
      "Epoch 136, Loss: 508.80128361628607\n",
      "Epoch 137, Loss: 508.5752422626202\n",
      "Epoch 138, Loss: 508.52062166654144\n",
      "Epoch 139, Loss: 506.93631333571216\n",
      "Epoch 140, Loss: 507.51494774451623\n",
      "Epoch 141, Loss: 502.95134793795074\n",
      "Epoch 142, Loss: 502.07735032301684\n",
      "Epoch 143, Loss: 499.18702815129205\n",
      "Epoch 144, Loss: 501.2934805063101\n",
      "Epoch 145, Loss: 501.1686882605919\n",
      "Epoch 146, Loss: 497.2545224703275\n",
      "Epoch 147, Loss: 497.86609708345856\n",
      "Epoch 148, Loss: 495.81209740271936\n",
      "Epoch 149, Loss: 496.16832322340747\n",
      "Epoch 150, Loss: 491.7545694204477\n",
      "Epoch 151, Loss: 492.6376424936148\n",
      "Epoch 152, Loss: 491.88314349834735\n",
      "Epoch 153, Loss: 493.744874220628\n",
      "Epoch 154, Loss: 493.13206364558295\n",
      "Epoch 155, Loss: 489.4393111008864\n",
      "Epoch 156, Loss: 488.74357135479266\n",
      "Epoch 157, Loss: 485.79978708120495\n",
      "Epoch 158, Loss: 484.73611332820013\n",
      "Epoch 159, Loss: 485.3520754300631\n",
      "Epoch 160, Loss: 483.66922701322113\n",
      "Epoch 161, Loss: 483.69746751051684\n",
      "Epoch 162, Loss: 481.34801893967847\n",
      "Epoch 163, Loss: 481.7174001840445\n",
      "Epoch 164, Loss: 479.7640146108774\n",
      "Epoch 165, Loss: 483.4305220383864\n",
      "Epoch 166, Loss: 481.6203402005709\n",
      "Epoch 167, Loss: 477.16413057767426\n",
      "Epoch 168, Loss: 476.6127272385817\n",
      "Epoch 169, Loss: 475.31490501990686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:02:40,610] Trial 8 finished with value: 1063.5924816029235 and parameters: {'latent_dim_z1': 33, 'latent_dim_z2': 44, 'hidden_dim': 229, 'epochs': 172, 'causal_reg': 0.5599635293999022, 'learning_rate': 4.8258603192869735e-05}. Best is trial 2 with value: 934.1151132697736.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170, Loss: 475.1673607459435\n",
      "Epoch 171, Loss: 478.8286003699669\n",
      "Epoch 172, Loss: 475.8669867882362\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:02:43,305] Trial 9 failed with parameters: {'latent_dim_z1': 40, 'latent_dim_z2': 44, 'hidden_dim': 173, 'epochs': 102, 'causal_reg': 0.47358882984873174, 'learning_rate': 0.01681873397901389} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:02:43,306] Trial 9 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 1, Loss: 783273.1186899039\n",
      "Epoch 2, Loss: 97740.75518329327\n",
      "Epoch 3, Loss: 57456.78478064904\n",
      "Epoch 4, Loss: 41258.87417367788\n",
      "Epoch 5, Loss: 32009.69921875\n",
      "Epoch 6, Loss: 25774.79187950721\n",
      "Epoch 7, Loss: 21307.736234224758\n",
      "Epoch 8, Loss: 18004.788555438703\n",
      "Epoch 9, Loss: 15479.167949969951\n",
      "Epoch 10, Loss: 13517.09200345553\n",
      "Epoch 11, Loss: 11941.717604417066\n",
      "Epoch 12, Loss: 10670.319983849158\n",
      "Epoch 13, Loss: 9613.745774489184\n",
      "Epoch 14, Loss: 8736.995060847355\n",
      "Epoch 15, Loss: 7998.36874624399\n",
      "Epoch 16, Loss: 7362.678842397837\n",
      "Epoch 17, Loss: 6776.5819749098555\n",
      "Epoch 18, Loss: 6253.981661283053\n",
      "Epoch 19, Loss: 5817.61172250601\n",
      "Epoch 20, Loss: 5452.431133563702\n",
      "Epoch 21, Loss: 5147.0526780348555\n",
      "Epoch 22, Loss: 4884.5079345703125\n",
      "Epoch 23, Loss: 4666.8760986328125\n",
      "Epoch 24, Loss: 4481.4578200120195\n",
      "Epoch 25, Loss: 4327.6752037635215\n",
      "Epoch 26, Loss: 4193.118732158954\n",
      "Epoch 27, Loss: 4079.3429236778848\n",
      "Epoch 28, Loss: 3980.561053936298\n",
      "Epoch 29, Loss: 3897.1431837815503\n",
      "Epoch 30, Loss: 3820.808293269231\n",
      "Epoch 31, Loss: 3754.367267315204\n",
      "Epoch 32, Loss: 3695.3752957857573\n",
      "Epoch 33, Loss: 3641.5885056715747\n",
      "Epoch 34, Loss: 3593.722341684195\n",
      "Epoch 35, Loss: 3550.755352313702\n",
      "Epoch 36, Loss: 3510.401601938101\n",
      "Epoch 37, Loss: 3472.534005972055\n",
      "Epoch 38, Loss: 3438.0472740760215\n",
      "Epoch 39, Loss: 3405.97512113131\n",
      "Epoch 40, Loss: 3376.076359675481\n",
      "Epoch 41, Loss: 3347.89555476262\n",
      "Epoch 42, Loss: 3319.5980928861177\n",
      "Epoch 43, Loss: 3292.664095365084\n",
      "Epoch 44, Loss: 3267.0035963792066\n",
      "Epoch 45, Loss: 3241.907029371995\n",
      "Epoch 46, Loss: 3218.630652794471\n",
      "Epoch 47, Loss: 3195.636545034555\n",
      "Epoch 48, Loss: 3173.619591346154\n",
      "Epoch 49, Loss: 3152.4871215820312\n",
      "Epoch 50, Loss: 3131.850421612079\n",
      "Epoch 51, Loss: 3112.12009371244\n",
      "Epoch 52, Loss: 3091.624779334435\n",
      "Epoch 53, Loss: 3071.94731257512\n",
      "Epoch 54, Loss: 3052.6146240234375\n",
      "Epoch 55, Loss: 3033.7951049804688\n",
      "Epoch 56, Loss: 3016.1224083533652\n",
      "Epoch 57, Loss: 2997.8257446289062\n",
      "Epoch 58, Loss: 2979.726247934195\n",
      "Epoch 59, Loss: 2962.5772705078125\n",
      "Epoch 60, Loss: 2944.945368840144\n",
      "Epoch 61, Loss: 2928.552940955529\n",
      "Epoch 62, Loss: 2913.7799260066104\n",
      "Epoch 63, Loss: 2896.1737342247598\n",
      "Epoch 64, Loss: 2878.2163931039663\n",
      "Epoch 65, Loss: 2862.459392841046\n",
      "Epoch 66, Loss: 2845.946007361779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:02:44,904] Trial 10 finished with value: 19619.518456228336 and parameters: {'latent_dim_z1': 66, 'latent_dim_z2': 48, 'hidden_dim': 34, 'epochs': 69, 'causal_reg': 0.6904129012588232, 'learning_rate': 3.802423580549028e-05}. Best is trial 2 with value: 934.1151132697736.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, Loss: 2831.9723088191104\n",
      "Epoch 68, Loss: 2819.043142465445\n",
      "Epoch 69, Loss: 2801.4097571739785\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:02:47,143] Trial 11 failed with parameters: {'latent_dim_z1': 23, 'latent_dim_z2': 14, 'hidden_dim': 200, 'epochs': 103, 'causal_reg': 0.6995626233264317, 'learning_rate': 0.015487878661096703} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:02:47,144] Trial 11 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 1, Loss: 34960.959327110875\n",
      "Epoch 2, Loss: 2420.4199336125303\n",
      "Epoch 3, Loss: 1316.4000056340144\n",
      "Epoch 4, Loss: 1151.1788799579326\n",
      "Epoch 5, Loss: 1062.8964116023137\n",
      "Epoch 6, Loss: 1002.7103060208834\n",
      "Epoch 7, Loss: 958.1645730825571\n",
      "Epoch 8, Loss: 925.9645268366887\n",
      "Epoch 9, Loss: 900.4196542593149\n",
      "Epoch 10, Loss: 877.6334944504958\n",
      "Epoch 11, Loss: 854.9593681922325\n",
      "Epoch 12, Loss: 833.6766404371995\n",
      "Epoch 13, Loss: 813.3058682955228\n",
      "Epoch 14, Loss: 793.622798039363\n",
      "Epoch 15, Loss: 776.7805809607872\n",
      "Epoch 16, Loss: 756.7921694242037\n",
      "Epoch 17, Loss: 739.4628272423378\n",
      "Epoch 18, Loss: 724.2629981407753\n",
      "Epoch 19, Loss: 709.7965416541466\n",
      "Epoch 20, Loss: 695.7864415095403\n",
      "Epoch 21, Loss: 684.067623431866\n",
      "Epoch 22, Loss: 673.7088282658503\n",
      "Epoch 23, Loss: 664.4465648944562\n",
      "Epoch 24, Loss: 656.1120382455679\n",
      "Epoch 25, Loss: 648.0114382230319\n",
      "Epoch 26, Loss: 640.3901085486779\n",
      "Epoch 27, Loss: 633.2960216815655\n",
      "Epoch 28, Loss: 626.6509516789363\n",
      "Epoch 29, Loss: 619.8372767521785\n",
      "Epoch 30, Loss: 613.369132408729\n",
      "Epoch 31, Loss: 607.2457017164963\n",
      "Epoch 32, Loss: 601.0808809720553\n",
      "Epoch 33, Loss: 595.0604424109825\n",
      "Epoch 34, Loss: 588.9486377422626\n",
      "Epoch 35, Loss: 582.7904944786659\n",
      "Epoch 36, Loss: 576.6689640925481\n",
      "Epoch 37, Loss: 570.8957683856671\n",
      "Epoch 38, Loss: 564.6958324725812\n",
      "Epoch 39, Loss: 559.0298767089844\n",
      "Epoch 40, Loss: 553.2218557504507\n",
      "Epoch 41, Loss: 548.6885094275841\n",
      "Epoch 42, Loss: 544.5853259746845\n",
      "Epoch 43, Loss: 540.1670590914213\n",
      "Epoch 44, Loss: 536.2787745549128\n",
      "Epoch 45, Loss: 532.4858797513522\n",
      "Epoch 46, Loss: 529.2741088867188\n",
      "Epoch 47, Loss: 525.142836350661\n",
      "Epoch 48, Loss: 520.7225963885968\n",
      "Epoch 49, Loss: 517.5984626183143\n",
      "Epoch 50, Loss: 514.1309744027944\n",
      "Epoch 51, Loss: 511.64971219576324\n",
      "Epoch 52, Loss: 507.3677462064303\n",
      "Epoch 53, Loss: 504.2015709510216\n",
      "Epoch 54, Loss: 500.41421978290265\n",
      "Epoch 55, Loss: 497.35223506047174\n",
      "Epoch 56, Loss: 494.37914452186\n",
      "Epoch 57, Loss: 491.9917907714844\n",
      "Epoch 58, Loss: 487.66732083834137\n",
      "Epoch 59, Loss: 487.2425302358774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:02:48,558] Trial 12 finished with value: 1238.1734037564945 and parameters: {'latent_dim_z1': 20, 'latent_dim_z2': 35, 'hidden_dim': 215, 'epochs': 62, 'causal_reg': 0.33314935026991344, 'learning_rate': 0.0010141936237030923}. Best is trial 2 with value: 934.1151132697736.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Loss: 483.60164701021637\n",
      "Epoch 61, Loss: 479.6448481633113\n",
      "Epoch 62, Loss: 477.44195439265326\n",
      "Epoch 1, Loss: 39828.01188777043\n",
      "Epoch 2, Loss: 4920.357637845553\n",
      "Epoch 3, Loss: 3167.8576941856973\n",
      "Epoch 4, Loss: 2672.613539475661\n",
      "Epoch 5, Loss: 2402.2579345703125\n",
      "Epoch 6, Loss: 2220.2952740009014\n",
      "Epoch 7, Loss: 2081.3543982872598\n",
      "Epoch 8, Loss: 1967.239741398738\n",
      "Epoch 9, Loss: 1866.0196814903845\n",
      "Epoch 10, Loss: 1775.6665931114783\n",
      "Epoch 11, Loss: 1690.7402977576623\n",
      "Epoch 12, Loss: 1616.1166334885818\n",
      "Epoch 13, Loss: 1547.0074392465444\n",
      "Epoch 14, Loss: 1490.9238868126502\n",
      "Epoch 15, Loss: 1438.9260699932393\n",
      "Epoch 16, Loss: 1391.0535653921274\n",
      "Epoch 17, Loss: 1350.546846829928\n",
      "Epoch 18, Loss: 1315.6116732083833\n",
      "Epoch 19, Loss: 1282.353067251352\n",
      "Epoch 20, Loss: 1255.1902113694412\n",
      "Epoch 21, Loss: 1228.8519146259014\n",
      "Epoch 22, Loss: 1201.5306419959436\n",
      "Epoch 23, Loss: 1174.6896221454326\n",
      "Epoch 24, Loss: 1153.1568861741287\n",
      "Epoch 25, Loss: 1130.2972271259014\n",
      "Epoch 26, Loss: 1104.9605525090144\n",
      "Epoch 27, Loss: 1082.0723806527944\n",
      "Epoch 28, Loss: 1063.6606222299429\n",
      "Epoch 29, Loss: 1042.2074584960938\n",
      "Epoch 30, Loss: 1029.0123737041768\n",
      "Epoch 31, Loss: 1025.0073981651892\n",
      "Epoch 32, Loss: 993.5602593055138\n",
      "Epoch 33, Loss: 979.24559373122\n",
      "Epoch 34, Loss: 962.1526207557091\n",
      "Epoch 35, Loss: 946.1133915827825\n",
      "Epoch 36, Loss: 936.9857036884015\n",
      "Epoch 37, Loss: 929.0004542424128\n",
      "Epoch 38, Loss: 909.2799494816707\n",
      "Epoch 39, Loss: 906.03392967811\n",
      "Epoch 40, Loss: 882.705086341271\n",
      "Epoch 41, Loss: 878.3822537935697\n",
      "Epoch 42, Loss: 860.8589383638822\n",
      "Epoch 43, Loss: 848.4625737116887\n",
      "Epoch 44, Loss: 843.8123262845553\n",
      "Epoch 45, Loss: 832.1129760742188\n",
      "Epoch 46, Loss: 817.7314605712891\n",
      "Epoch 47, Loss: 805.687246469351\n",
      "Epoch 48, Loss: 802.4938061053937\n",
      "Epoch 49, Loss: 801.5087585449219\n",
      "Epoch 50, Loss: 825.7586129995493\n",
      "Epoch 51, Loss: 795.9352745643029\n",
      "Epoch 52, Loss: 764.3256284273588\n",
      "Epoch 53, Loss: 759.7090266301082\n",
      "Epoch 54, Loss: 762.7806126521184\n",
      "Epoch 55, Loss: 746.1558967003456\n",
      "Epoch 56, Loss: 725.5087033785306\n",
      "Epoch 57, Loss: 727.2173086313101\n",
      "Epoch 58, Loss: 724.8442840576172\n",
      "Epoch 59, Loss: 721.9849548339844\n",
      "Epoch 60, Loss: 701.228755070613\n",
      "Epoch 61, Loss: 705.4072113037109\n",
      "Epoch 62, Loss: 720.4440354567307\n",
      "Epoch 63, Loss: 685.6575305645282\n",
      "Epoch 64, Loss: 685.4949141282302\n",
      "Epoch 65, Loss: 674.4963684082031\n",
      "Epoch 66, Loss: 661.3029890794021\n",
      "Epoch 67, Loss: 663.5330423208384\n",
      "Epoch 68, Loss: 655.9340209960938\n",
      "Epoch 69, Loss: 645.8250908484825\n",
      "Epoch 70, Loss: 647.1829822246845\n",
      "Epoch 71, Loss: 642.8223595252404\n",
      "Epoch 72, Loss: 683.7030627910907\n",
      "Epoch 73, Loss: 630.9723604642428\n",
      "Epoch 74, Loss: 624.746103140024\n",
      "Epoch 75, Loss: 616.9912438025841\n",
      "Epoch 76, Loss: 626.7258453369141\n",
      "Epoch 77, Loss: 621.0420872614934\n",
      "Epoch 78, Loss: 604.1141451322115\n",
      "Epoch 79, Loss: 625.7455350435697\n",
      "Epoch 80, Loss: 598.8387169471154\n",
      "Epoch 81, Loss: 602.9917684701772\n",
      "Epoch 82, Loss: 597.8813781738281\n",
      "Epoch 83, Loss: 593.7174530029297\n",
      "Epoch 84, Loss: 578.926028911884\n",
      "Epoch 85, Loss: 590.002934382512\n",
      "Epoch 86, Loss: 577.6965907170222\n",
      "Epoch 87, Loss: 578.639155461238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:02:50,569] Trial 13 finished with value: 1424.5774201427873 and parameters: {'latent_dim_z1': 57, 'latent_dim_z2': 40, 'hidden_dim': 44, 'epochs': 95, 'causal_reg': 0.22763939375309583, 'learning_rate': 0.00031857987449812164}. Best is trial 2 with value: 934.1151132697736.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88, Loss: 567.3798933762771\n",
      "Epoch 89, Loss: 571.2288067157452\n",
      "Epoch 90, Loss: 566.1986037034255\n",
      "Epoch 91, Loss: 560.489980844351\n",
      "Epoch 92, Loss: 558.0595574012169\n",
      "Epoch 93, Loss: 564.3316251314603\n",
      "Epoch 94, Loss: 557.1221489539513\n",
      "Epoch 95, Loss: 542.0418055607722\n",
      "Epoch 1, Loss: 15862.313983623799\n",
      "Epoch 2, Loss: 1975.4664048414963\n",
      "Epoch 3, Loss: 1671.6133587176982\n",
      "Epoch 4, Loss: 1554.539311335637\n",
      "Epoch 5, Loss: 1432.8015794020432\n",
      "Epoch 6, Loss: 1310.0017794095552\n",
      "Epoch 7, Loss: 1202.747767521785\n",
      "Epoch 8, Loss: 1117.8725961538462\n",
      "Epoch 9, Loss: 1050.123006967398\n",
      "Epoch 10, Loss: 1002.492433988131\n",
      "Epoch 11, Loss: 945.8993119459885\n",
      "Epoch 12, Loss: 899.2024407019982\n",
      "Epoch 13, Loss: 864.7148073636569\n",
      "Epoch 14, Loss: 833.9801729642428\n",
      "Epoch 15, Loss: 809.4399249737079\n",
      "Epoch 16, Loss: 785.7596928523137\n",
      "Epoch 17, Loss: 761.897230881911\n",
      "Epoch 18, Loss: 743.9535264235276\n",
      "Epoch 19, Loss: 723.5332559438853\n",
      "Epoch 20, Loss: 703.9102231539213\n",
      "Epoch 21, Loss: 694.9943014291616\n",
      "Epoch 22, Loss: 689.0919165978065\n",
      "Epoch 23, Loss: 674.4537107027494\n",
      "Epoch 24, Loss: 658.3143251859225\n",
      "Epoch 25, Loss: 645.4052124023438\n",
      "Epoch 26, Loss: 649.4136822040265\n",
      "Epoch 27, Loss: 632.3022672213041\n",
      "Epoch 28, Loss: 623.396501981295\n",
      "Epoch 29, Loss: 627.135988675631\n",
      "Epoch 30, Loss: 628.5447117732122\n",
      "Epoch 31, Loss: 608.6056495079628\n",
      "Epoch 32, Loss: 609.9037099984976\n",
      "Epoch 33, Loss: 591.9448653001052\n",
      "Epoch 34, Loss: 586.3691746638372\n",
      "Epoch 35, Loss: 591.9551602877103\n",
      "Epoch 36, Loss: 577.8906602125901\n",
      "Epoch 37, Loss: 569.5888918363131\n",
      "Epoch 38, Loss: 557.4678779015175\n",
      "Epoch 39, Loss: 552.2119821401743\n",
      "Epoch 40, Loss: 556.783212515024\n",
      "Epoch 41, Loss: 552.3157935509315\n",
      "Epoch 42, Loss: 538.5052607609675\n",
      "Epoch 43, Loss: 534.4278822678787\n",
      "Epoch 44, Loss: 527.4643061711238\n",
      "Epoch 45, Loss: 526.9508385291466\n",
      "Epoch 46, Loss: 531.5727186936599\n",
      "Epoch 47, Loss: 524.3836411696213\n",
      "Epoch 48, Loss: 516.0509913517878\n",
      "Epoch 49, Loss: 513.3103942871094\n",
      "Epoch 50, Loss: 523.7908724271334\n",
      "Epoch 51, Loss: 502.06723139836237\n",
      "Epoch 52, Loss: 506.71449514535755\n",
      "Epoch 53, Loss: 509.08888127253607\n",
      "Epoch 54, Loss: 497.69125483586237\n",
      "Epoch 55, Loss: 524.1509575477013\n",
      "Epoch 56, Loss: 499.0753185565655\n",
      "Epoch 57, Loss: 523.4755554199219\n",
      "Epoch 58, Loss: 478.5849045973558\n",
      "Epoch 59, Loss: 482.94981971153845\n",
      "Epoch 60, Loss: 487.7054173396184\n",
      "Epoch 61, Loss: 491.35132892315204\n",
      "Epoch 62, Loss: 548.1539353590745\n",
      "Epoch 63, Loss: 482.4435143103966\n",
      "Epoch 64, Loss: 467.20208622859076\n",
      "Epoch 65, Loss: 488.3314701960637\n",
      "Epoch 66, Loss: 465.78602130596454\n",
      "Epoch 67, Loss: 464.1883145845853\n",
      "Epoch 68, Loss: 456.77581904484674\n",
      "Epoch 69, Loss: 477.2290743314303\n",
      "Epoch 70, Loss: 460.70835993840143\n",
      "Epoch 71, Loss: 458.323488675631\n",
      "Epoch 72, Loss: 456.90013592059796\n",
      "Epoch 73, Loss: 462.0990940974309\n",
      "Epoch 74, Loss: 466.8803440974309\n",
      "Epoch 75, Loss: 451.94272672213043\n",
      "Epoch 76, Loss: 442.84007145808295\n",
      "Epoch 77, Loss: 450.83314631535455\n",
      "Epoch 78, Loss: 460.62433448204627\n",
      "Epoch 79, Loss: 465.88047086275543\n",
      "Epoch 80, Loss: 458.9646770770733\n",
      "Epoch 81, Loss: 433.0957336425781\n",
      "Epoch 82, Loss: 434.3166809082031\n",
      "Epoch 83, Loss: 461.485602745643\n",
      "Epoch 84, Loss: 433.43011474609375\n",
      "Epoch 85, Loss: 427.6859788161058\n",
      "Epoch 86, Loss: 431.33661592923676\n",
      "Epoch 87, Loss: 426.1086296668419\n",
      "Epoch 88, Loss: 431.0151578463041\n",
      "Epoch 89, Loss: 431.6890916090745\n",
      "Epoch 90, Loss: 442.2538358248197\n",
      "Epoch 91, Loss: 413.6877711369441\n",
      "Epoch 92, Loss: 410.8920405461238\n",
      "Epoch 93, Loss: 414.62547419621393\n",
      "Epoch 94, Loss: 412.4525316678561\n",
      "Epoch 95, Loss: 409.78978553185095\n",
      "Epoch 96, Loss: 409.2457087590144\n",
      "Epoch 97, Loss: 409.8567540095403\n",
      "Epoch 98, Loss: 425.9397700383113\n",
      "Epoch 99, Loss: 409.3062356802133\n",
      "Epoch 100, Loss: 400.61027174729566\n",
      "Epoch 101, Loss: 456.0713712252103\n",
      "Epoch 102, Loss: 421.09290607158954\n",
      "Epoch 103, Loss: 427.0225055401142\n",
      "Epoch 104, Loss: 396.1969205416166\n",
      "Epoch 105, Loss: 400.6315483680138\n",
      "Epoch 106, Loss: 397.31035672701324\n",
      "Epoch 107, Loss: 406.74861497145434\n",
      "Epoch 108, Loss: 409.09744145320013\n",
      "Epoch 109, Loss: 408.8974339411809\n",
      "Epoch 110, Loss: 409.5453866811899\n",
      "Epoch 111, Loss: 411.11719102125903\n",
      "Epoch 112, Loss: 384.1832768366887\n",
      "Epoch 113, Loss: 391.0200958251953\n",
      "Epoch 114, Loss: 408.2817065899189\n",
      "Epoch 115, Loss: 395.5367889404297\n",
      "Epoch 116, Loss: 384.89739990234375\n",
      "Epoch 117, Loss: 396.4369788536659\n",
      "Epoch 118, Loss: 434.44122314453125\n",
      "Epoch 119, Loss: 390.85620469313403\n",
      "Epoch 120, Loss: 399.1463094858023\n",
      "Epoch 121, Loss: 374.95936701847955\n",
      "Epoch 122, Loss: 373.64065434382513\n",
      "Epoch 123, Loss: 416.32177734375\n",
      "Epoch 124, Loss: 397.3273479755108\n",
      "Epoch 125, Loss: 385.8330571101262\n",
      "Epoch 126, Loss: 374.24320162259613\n",
      "Epoch 127, Loss: 377.3108602670523\n",
      "Epoch 128, Loss: 371.96104900653546\n",
      "Epoch 129, Loss: 371.35676926832934\n",
      "Epoch 130, Loss: 391.0882321871244\n",
      "Epoch 131, Loss: 368.2011260986328\n",
      "Epoch 132, Loss: 376.1069852388822\n",
      "Epoch 133, Loss: 364.87896024263824\n",
      "Epoch 134, Loss: 381.9652756911058\n",
      "Epoch 135, Loss: 364.38535837026745\n",
      "Epoch 136, Loss: 356.1031728891226\n",
      "Epoch 137, Loss: 362.2256516676683\n",
      "Epoch 138, Loss: 363.70721552922174\n",
      "Epoch 139, Loss: 384.41513296274036\n",
      "Epoch 140, Loss: 378.9828444260817\n",
      "Epoch 141, Loss: 373.9051384559044\n",
      "Epoch 142, Loss: 348.1938147911659\n",
      "Epoch 143, Loss: 352.13716184175934\n",
      "Epoch 144, Loss: 359.445803128756\n",
      "Epoch 145, Loss: 376.6483987661508\n",
      "Epoch 146, Loss: 358.95375765286957\n",
      "Epoch 147, Loss: 349.8739201472356\n",
      "Epoch 148, Loss: 353.37550354003906\n",
      "Epoch 149, Loss: 354.46295283390924\n",
      "Epoch 150, Loss: 353.87070523775543\n",
      "Epoch 151, Loss: 361.8104975773738\n",
      "Epoch 152, Loss: 352.03142723670373\n",
      "Epoch 153, Loss: 367.646483201247\n",
      "Epoch 154, Loss: 361.5409252460186\n",
      "Epoch 155, Loss: 348.7727801983173\n",
      "Epoch 156, Loss: 359.28856600247894\n",
      "Epoch 157, Loss: 345.35405203012317\n",
      "Epoch 158, Loss: 363.77391639122595\n",
      "Epoch 159, Loss: 366.77334477351263\n",
      "Epoch 160, Loss: 348.4898892916166\n",
      "Epoch 161, Loss: 423.8883620042067\n",
      "Epoch 162, Loss: 353.0222813532903\n",
      "Epoch 163, Loss: 344.95948791503906\n",
      "Epoch 164, Loss: 350.6954322228065\n",
      "Epoch 165, Loss: 344.85890197753906\n",
      "Epoch 166, Loss: 346.00485464242786\n",
      "Epoch 167, Loss: 349.5315070519081\n",
      "Epoch 168, Loss: 339.9726351224459\n",
      "Epoch 169, Loss: 338.29474053016077\n",
      "Epoch 170, Loss: 336.989508995643\n",
      "Epoch 171, Loss: 335.47527841421277\n",
      "Epoch 172, Loss: 347.8470470721905\n",
      "Epoch 173, Loss: 331.51841853215143\n",
      "Epoch 174, Loss: 336.5068793663612\n",
      "Epoch 175, Loss: 345.1888744647686\n",
      "Epoch 176, Loss: 340.5780305128831\n",
      "Epoch 177, Loss: 345.3705303485577\n",
      "Epoch 178, Loss: 348.02791302020734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:02:55,628] Trial 14 finished with value: 588.743737582725 and parameters: {'latent_dim_z1': 39, 'latent_dim_z2': 77, 'hidden_dim': 183, 'epochs': 182, 'causal_reg': 0.8215347973650419, 'learning_rate': 0.0004262208166382702}. Best is trial 14 with value: 588.743737582725.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179, Loss: 337.1398092416617\n",
      "Epoch 180, Loss: 331.21472989595856\n",
      "Epoch 181, Loss: 331.2406029334435\n",
      "Epoch 182, Loss: 348.5443303034856\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 190, Loss: nan\n",
      "Epoch 191, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:03:00,676] Trial 15 failed with parameters: {'latent_dim_z1': 42, 'latent_dim_z2': 80, 'hidden_dim': 150, 'epochs': 198, 'causal_reg': 0.005437787624002988, 'learning_rate': 0.020232721678387352} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:03:00,676] Trial 15 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192, Loss: nan\n",
      "Epoch 193, Loss: nan\n",
      "Epoch 194, Loss: nan\n",
      "Epoch 195, Loss: nan\n",
      "Epoch 196, Loss: nan\n",
      "Epoch 197, Loss: nan\n",
      "Epoch 198, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:03:05,346] Trial 16 failed with parameters: {'latent_dim_z1': 43, 'latent_dim_z2': 80, 'hidden_dim': 158, 'epochs': 186, 'causal_reg': 0.042060335413080074, 'learning_rate': 0.0619146763481772} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:03:05,347] Trial 16 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 1, Loss: 1.166783836980358e+31\n",
      "Epoch 2, Loss: 1.1937608053736473e+31\n",
      "Epoch 3, Loss: 1.1937607889543422e+31\n",
      "Epoch 4, Loss: 1.19376080304879e+31\n",
      "Epoch 5, Loss: 1.1937608297846495e+31\n",
      "Epoch 6, Loss: 1.193760798834986e+31\n",
      "Epoch 7, Loss: 1.193760832690721e+31\n",
      "Epoch 8, Loss: 1.1937608629138665e+31\n",
      "Epoch 9, Loss: 1.1937608475116866e+31\n",
      "Epoch 10, Loss: 1.1937608036300044e+31\n",
      "Epoch 11, Loss: 1.1937607966554323e+31\n",
      "Epoch 12, Loss: 1.1937608727945103e+31\n",
      "Epoch 13, Loss: 1.1937608106045764e+31\n",
      "Epoch 14, Loss: 1.1937609076673705e+31\n",
      "Epoch 15, Loss: 1.1937608035028637e+31\n",
      "Epoch 16, Loss: 1.1937607524831427e+31\n",
      "Epoch 17, Loss: 1.1937607780565735e+31\n",
      "Epoch 18, Loss: 1.1937607705007871e+31\n",
      "Epoch 19, Loss: 1.1937608350155785e+31\n",
      "Epoch 20, Loss: 1.1937607873560028e+31\n",
      "Epoch 21, Loss: 1.1937608332719356e+31\n",
      "Epoch 22, Loss: 1.193760816997934e+31\n",
      "Epoch 23, Loss: 1.1937608129294336e+31\n",
      "Epoch 24, Loss: 1.1937608088609333e+31\n",
      "Epoch 25, Loss: 1.1937608181603628e+31\n",
      "Epoch 26, Loss: 1.1937608637856882e+31\n",
      "Epoch 27, Loss: 1.1937608111857907e+31\n",
      "Epoch 28, Loss: 1.1937608402465074e+31\n",
      "Epoch 29, Loss: 1.1937608280410064e+31\n",
      "Epoch 30, Loss: 1.1937609099922279e+31\n",
      "Epoch 31, Loss: 1.1937608094421476e+31\n",
      "Epoch 32, Loss: 1.1937608140918622e+31\n",
      "Epoch 33, Loss: 1.193760797817861e+31\n",
      "Epoch 34, Loss: 1.193760788009869e+31\n",
      "Epoch 35, Loss: 1.193760775731716e+31\n",
      "Epoch 36, Loss: 1.193760872213296e+31\n",
      "Epoch 37, Loss: 1.1937608200493094e+31\n",
      "Epoch 38, Loss: 1.1937609913622349e+31\n",
      "Epoch 39, Loss: 1.193760815254291e+31\n",
      "Epoch 40, Loss: 1.193760796074218e+31\n",
      "Epoch 41, Loss: 1.1937607879372173e+31\n",
      "Epoch 42, Loss: 1.1937607966554323e+31\n",
      "Epoch 43, Loss: 1.1937607943305749e+31\n",
      "Epoch 44, Loss: 1.193760849545937e+31\n",
      "Epoch 45, Loss: 1.1937608297846495e+31\n",
      "Epoch 46, Loss: 1.1937607739880732e+31\n",
      "Epoch 47, Loss: 1.193760744346142e+31\n",
      "Epoch 48, Loss: 1.1937607832875025e+31\n",
      "Epoch 49, Loss: 1.193760823972506e+31\n",
      "Epoch 50, Loss: 1.1937608146730768e+31\n",
      "Epoch 51, Loss: 1.1937608018863612e+31\n",
      "Epoch 52, Loss: 1.1937608088609333e+31\n",
      "Epoch 53, Loss: 1.1937608262973635e+31\n",
      "Epoch 54, Loss: 1.1937608065360759e+31\n",
      "Epoch 55, Loss: 1.193760810023362e+31\n",
      "Epoch 56, Loss: 1.1937608193227914e+31\n",
      "Epoch 57, Loss: 1.1937606763440646e+31\n",
      "Epoch 58, Loss: 1.1937608257161492e+31\n",
      "Epoch 59, Loss: 1.1937608065360759e+31\n",
      "Epoch 60, Loss: 1.1937607821250739e+31\n",
      "Epoch 61, Loss: 1.1937607867747885e+31\n",
      "Epoch 62, Loss: 1.1937608042112187e+31\n",
      "Epoch 63, Loss: 1.1937608065360759e+31\n",
      "Epoch 64, Loss: 1.1937607983990752e+31\n",
      "Epoch 65, Loss: 1.1937608181603628e+31\n",
      "Epoch 66, Loss: 1.1937609035988701e+31\n",
      "Epoch 67, Loss: 1.193760827459792e+31\n",
      "Epoch 68, Loss: 1.19376078561236e+31\n",
      "Epoch 69, Loss: 1.1937608309470781e+31\n",
      "Epoch 70, Loss: 1.1937607885184316e+31\n",
      "Epoch 71, Loss: 1.1937607949117892e+31\n",
      "Epoch 72, Loss: 1.1937608335625427e+31\n",
      "Epoch 73, Loss: 1.1937607983990752e+31\n",
      "Epoch 74, Loss: 1.193760827459792e+31\n",
      "Epoch 75, Loss: 1.1937608265879706e+31\n",
      "Epoch 76, Loss: 1.193760814963684e+31\n",
      "Epoch 77, Loss: 1.1937608524520086e+31\n",
      "Epoch 78, Loss: 1.193760870469653e+31\n",
      "Epoch 79, Loss: 1.1937607998521112e+31\n",
      "Epoch 80, Loss: 1.193760823972506e+31\n",
      "Epoch 81, Loss: 1.193760767013501e+31\n",
      "Epoch 82, Loss: 1.1937608348702748e+31\n",
      "Epoch 83, Loss: 1.193760799561504e+31\n",
      "Epoch 84, Loss: 1.193760799561504e+31\n",
      "Epoch 85, Loss: 1.1937608140918622e+31\n",
      "Epoch 86, Loss: 1.193760836178007e+31\n",
      "Epoch 87, Loss: 1.1937607449273563e+31\n",
      "Epoch 88, Loss: 1.1937609146419425e+31\n",
      "Epoch 89, Loss: 1.193760810023362e+31\n",
      "Epoch 90, Loss: 1.1937608190321843e+31\n",
      "Epoch 91, Loss: 1.1937608309470781e+31\n",
      "Epoch 92, Loss: 1.1937608158355054e+31\n",
      "Epoch 93, Loss: 1.1937608262973635e+31\n",
      "Epoch 94, Loss: 1.1937607937493606e+31\n",
      "Epoch 95, Loss: 1.1937608018863612e+31\n",
      "Epoch 96, Loss: 1.1937608367592214e+31\n",
      "Epoch 97, Loss: 1.1937608108951836e+31\n",
      "Epoch 98, Loss: 1.193760896624298e+31\n",
      "Epoch 99, Loss: 1.1937608158355054e+31\n",
      "Epoch 100, Loss: 1.193760837340436e+31\n",
      "Epoch 101, Loss: 1.193760813510648e+31\n",
      "Epoch 102, Loss: 1.193760813510648e+31\n",
      "Epoch 103, Loss: 1.193760815254291e+31\n",
      "Epoch 104, Loss: 1.1937607693383585e+31\n",
      "Epoch 105, Loss: 1.193760865238724e+31\n",
      "Epoch 106, Loss: 1.1937607629450008e+31\n",
      "Epoch 107, Loss: 1.1937607914245031e+31\n",
      "Epoch 108, Loss: 1.1937608210664343e+31\n",
      "Epoch 109, Loss: 1.1937608309470781e+31\n",
      "Epoch 110, Loss: 1.1937607972366466e+31\n",
      "Epoch 111, Loss: 1.193760814963684e+31\n",
      "Epoch 112, Loss: 1.1937608111857907e+31\n",
      "Epoch 113, Loss: 1.1937607699195728e+31\n",
      "Epoch 114, Loss: 1.1937607931681463e+31\n",
      "Epoch 115, Loss: 1.193760860007795e+31\n",
      "Epoch 116, Loss: 1.1937608010145398e+31\n",
      "Epoch 117, Loss: 1.193760837340436e+31\n",
      "Epoch 118, Loss: 1.1937608332719356e+31\n",
      "Epoch 119, Loss: 1.1937608065360759e+31\n",
      "Epoch 120, Loss: 1.1937607969460394e+31\n",
      "Epoch 121, Loss: 1.193760810023362e+31\n",
      "Epoch 122, Loss: 1.193760873956939e+31\n",
      "Epoch 123, Loss: 1.1937608140918622e+31\n",
      "Epoch 124, Loss: 1.1937608042112187e+31\n",
      "Epoch 125, Loss: 1.193760877444225e+31\n",
      "Epoch 126, Loss: 1.193760853033223e+31\n",
      "Epoch 127, Loss: 1.1937609065049418e+31\n",
      "Epoch 128, Loss: 1.193760813510648e+31\n",
      "Epoch 129, Loss: 1.193760816997934e+31\n",
      "Epoch 130, Loss: 1.1937608037753078e+31\n",
      "Epoch 131, Loss: 1.1937608309470781e+31\n",
      "Epoch 132, Loss: 1.1937608228100774e+31\n",
      "Epoch 133, Loss: 1.1937607809626452e+31\n",
      "Epoch 134, Loss: 1.1937608332719356e+31\n",
      "Epoch 135, Loss: 1.1937608582641519e+31\n",
      "Epoch 136, Loss: 1.193760811767005e+31\n",
      "Epoch 137, Loss: 1.1937607902620745e+31\n",
      "Epoch 138, Loss: 1.1937607931681463e+31\n",
      "Epoch 139, Loss: 1.1937608129294336e+31\n",
      "Epoch 140, Loss: 1.1937608007239326e+31\n",
      "Epoch 141, Loss: 1.1937608106045764e+31\n",
      "Epoch 142, Loss: 1.193760842861972e+31\n",
      "Epoch 143, Loss: 1.1937608106045764e+31\n",
      "Epoch 144, Loss: 1.1937608172885411e+31\n",
      "Epoch 145, Loss: 1.1937608004333255e+31\n",
      "Epoch 146, Loss: 1.1937608158355054e+31\n",
      "Epoch 147, Loss: 1.1937607957836108e+31\n",
      "Epoch 148, Loss: 1.1937608425713648e+31\n",
      "Epoch 149, Loss: 1.1937608255708455e+31\n",
      "Epoch 150, Loss: 1.1937608425713648e+31\n",
      "Epoch 151, Loss: 1.1937608018863612e+31\n",
      "Epoch 152, Loss: 1.193760813510648e+31\n",
      "Epoch 153, Loss: 1.1937609588142322e+31\n",
      "Epoch 154, Loss: 1.193760799161919e+31\n",
      "Epoch 155, Loss: 1.1937607739880732e+31\n",
      "Epoch 156, Loss: 1.1937607989802895e+31\n",
      "Epoch 157, Loss: 1.1937608257161492e+31\n",
      "Epoch 158, Loss: 1.1937608021769684e+31\n",
      "Epoch 159, Loss: 1.1937608164167197e+31\n",
      "Epoch 160, Loss: 1.1937607821250739e+31\n",
      "Epoch 161, Loss: 1.1937608094421476e+31\n",
      "Epoch 162, Loss: 1.193760751320714e+31\n",
      "Epoch 163, Loss: 1.1937610831941002e+31\n",
      "Epoch 164, Loss: 1.193760809006237e+31\n",
      "Epoch 165, Loss: 1.1937608199040057e+31\n",
      "Epoch 166, Loss: 1.193760801595754e+31\n",
      "Epoch 167, Loss: 1.193760797454602e+31\n",
      "Epoch 168, Loss: 1.1937608039206115e+31\n",
      "Epoch 169, Loss: 1.1937608454774366e+31\n",
      "Epoch 170, Loss: 1.1937608507083655e+31\n",
      "Epoch 171, Loss: 1.1937607902620745e+31\n",
      "Epoch 172, Loss: 1.1937608297846495e+31\n",
      "Epoch 173, Loss: 1.193760796074218e+31\n",
      "Epoch 174, Loss: 1.1937608344343642e+31\n",
      "Epoch 175, Loss: 1.1937607652698582e+31\n",
      "Epoch 176, Loss: 1.1937608460586509e+31\n",
      "Epoch 177, Loss: 1.193760780381431e+31\n",
      "Epoch 178, Loss: 1.193760796074218e+31\n",
      "Epoch 179, Loss: 1.193760823972506e+31\n",
      "Epoch 180, Loss: 1.1937608056642544e+31\n",
      "Epoch 181, Loss: 1.1937607899714674e+31\n",
      "Epoch 182, Loss: 1.1937608068266833e+31\n",
      "Epoch 183, Loss: 1.1937607902620745e+31\n",
      "Epoch 184, Loss: 1.1937608245537204e+31\n",
      "Epoch 185, Loss: 1.19376078561236e+31\n",
      "Epoch 186, Loss: 1.1937608414089362e+31\n",
      "Epoch 187, Loss: 1.193760760038929e+31\n",
      "Epoch 188, Loss: 1.1937608042112187e+31\n",
      "Epoch 189, Loss: 1.193760822955381e+31\n",
      "Epoch 190, Loss: 1.19376081845097e+31\n",
      "Epoch 191, Loss: 1.1937607902620745e+31\n",
      "Epoch 192, Loss: 1.1937608367592214e+31\n",
      "Epoch 193, Loss: 1.1937608576829376e+31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:03:09,572] Trial 17 finished with value: 1.193625254725574e+31 and parameters: {'latent_dim_z1': 41, 'latent_dim_z2': 10, 'hidden_dim': 140, 'epochs': 199, 'causal_reg': 0.7468659407237397, 'learning_rate': 0.01080295522910722}. Best is trial 14 with value: 588.743737582725.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, Loss: 1.1937608181603628e+31\n",
      "Epoch 195, Loss: 1.1937608937182265e+31\n",
      "Epoch 196, Loss: 1.19376078561236e+31\n",
      "Epoch 197, Loss: 1.1937608175791483e+31\n",
      "Epoch 198, Loss: 1.1937608001427183e+31\n",
      "Epoch 199, Loss: 1.1937608068266833e+31\n",
      "Epoch 1, Loss: 1968043800.3082933\n",
      "Epoch 2, Loss: 13316.07998422476\n",
      "Epoch 3, Loss: 4933.836965707632\n",
      "Epoch 4, Loss: 3562.037616436298\n",
      "Epoch 5, Loss: 3185.818091759315\n",
      "Epoch 6, Loss: 3007.7520047701323\n",
      "Epoch 7, Loss: 2834.977830153245\n",
      "Epoch 8, Loss: 2722.1923405573916\n",
      "Epoch 9, Loss: 2661.051025390625\n",
      "Epoch 10, Loss: 2602.0553025465747\n",
      "Epoch 11, Loss: 2553.7135385366587\n",
      "Epoch 12, Loss: 2505.845435509315\n",
      "Epoch 13, Loss: 2469.9821049616885\n",
      "Epoch 14, Loss: 2443.6798870380107\n",
      "Epoch 15, Loss: 2444.343043400691\n",
      "Epoch 16, Loss: 2412.0216440054087\n",
      "Epoch 17, Loss: 2389.6585857684795\n",
      "Epoch 18, Loss: 2372.0623638446514\n",
      "Epoch 19, Loss: 2349.409125694862\n",
      "Epoch 20, Loss: 2326.319786658654\n",
      "Epoch 21, Loss: 2313.87504695012\n",
      "Epoch 22, Loss: 2289.0042959359976\n",
      "Epoch 23, Loss: 2260.2047565166768\n",
      "Epoch 24, Loss: 2239.1371459960938\n",
      "Epoch 25, Loss: 2210.5508493276743\n",
      "Epoch 26, Loss: 2196.138911320613\n",
      "Epoch 27, Loss: 2178.2193439190205\n",
      "Epoch 28, Loss: 2178.8849369929385\n",
      "Epoch 29, Loss: 2138.9824899526743\n",
      "Epoch 30, Loss: 2101.6716918945312\n",
      "Epoch 31, Loss: 2080.167461688702\n",
      "Epoch 32, Loss: 2045.7505258413462\n",
      "Epoch 33, Loss: 2021.1692270132212\n",
      "Epoch 34, Loss: 2005.730694110577\n",
      "Epoch 35, Loss: 1978.880119910607\n",
      "Epoch 36, Loss: 1943.3145751953125\n",
      "Epoch 37, Loss: 1910.698244535006\n",
      "Epoch 38, Loss: 1891.806868333083\n",
      "Epoch 39, Loss: 1849.0692091721755\n",
      "Epoch 40, Loss: 1813.4733417217549\n",
      "Epoch 41, Loss: 1785.4392911470854\n",
      "Epoch 42, Loss: 1768.3074880746694\n",
      "Epoch 43, Loss: 1737.5452340932993\n",
      "Epoch 44, Loss: 1698.5394991361177\n",
      "Epoch 45, Loss: 1679.892585167518\n",
      "Epoch 46, Loss: 1645.2144446739783\n",
      "Epoch 47, Loss: 1624.3555415226863\n",
      "Epoch 48, Loss: 1640.0142986591045\n",
      "Epoch 49, Loss: 1586.8445598895732\n",
      "Epoch 50, Loss: 1564.8617530235877\n",
      "Epoch 51, Loss: 1572.9371807391826\n",
      "Epoch 52, Loss: 1557.2679701585037\n",
      "Epoch 53, Loss: 1506.283210167518\n",
      "Epoch 54, Loss: 1490.411872276893\n",
      "Epoch 55, Loss: 1478.1682504507212\n",
      "Epoch 56, Loss: 1460.950467623197\n",
      "Epoch 57, Loss: 1440.1228614220252\n",
      "Epoch 58, Loss: 1421.2557819073018\n",
      "Epoch 59, Loss: 1408.6875892052283\n",
      "Epoch 60, Loss: 1422.6868990384614\n",
      "Epoch 61, Loss: 1395.2386709359976\n",
      "Epoch 62, Loss: 1375.968470646785\n",
      "Epoch 63, Loss: 1360.8678565392127\n",
      "Epoch 64, Loss: 1350.3388531024639\n",
      "Epoch 65, Loss: 1341.9404555100662\n",
      "Epoch 66, Loss: 1342.4649681678186\n",
      "Epoch 67, Loss: 1329.5640822190505\n",
      "Epoch 68, Loss: 1300.7359431340144\n",
      "Epoch 69, Loss: 1287.5570256159856\n",
      "Epoch 70, Loss: 1284.1739267202524\n",
      "Epoch 71, Loss: 1270.1392517089844\n",
      "Epoch 72, Loss: 1259.1761991060698\n",
      "Epoch 73, Loss: 1232.4761915940505\n",
      "Epoch 74, Loss: 1223.2779869666467\n",
      "Epoch 75, Loss: 1238.6112154447114\n",
      "Epoch 76, Loss: 1231.3289888822114\n",
      "Epoch 77, Loss: 1194.0607229379507\n",
      "Epoch 78, Loss: 1180.788081242488\n",
      "Epoch 79, Loss: 1186.3812842735877\n",
      "Epoch 80, Loss: 1171.2498309795674\n",
      "Epoch 81, Loss: 1166.6025930551382\n",
      "Epoch 82, Loss: 1151.0941138634314\n",
      "Epoch 83, Loss: 1141.9437561035156\n",
      "Epoch 84, Loss: 1247.3382192758413\n",
      "Epoch 85, Loss: 1142.93607036884\n",
      "Epoch 86, Loss: 1121.080071082482\n",
      "Epoch 87, Loss: 1113.6368971604568\n",
      "Epoch 88, Loss: 1088.0062185434194\n",
      "Epoch 89, Loss: 1074.9781681941106\n",
      "Epoch 90, Loss: 1078.7801349346455\n",
      "Epoch 91, Loss: 1063.1551044170674\n",
      "Epoch 92, Loss: 1088.0697561410757\n",
      "Epoch 93, Loss: 1047.324009821965\n",
      "Epoch 94, Loss: 1030.422109750601\n",
      "Epoch 95, Loss: 1022.3285768949069\n",
      "Epoch 96, Loss: 1017.0169889009916\n",
      "Epoch 97, Loss: 1007.7469282883865\n",
      "Epoch 98, Loss: 1016.6054851825421\n",
      "Epoch 99, Loss: 997.9029165414663\n",
      "Epoch 100, Loss: 1003.3706876314603\n",
      "Epoch 101, Loss: 1001.6341494046725\n",
      "Epoch 102, Loss: 1024.203892634465\n",
      "Epoch 103, Loss: 1434.4630643404448\n",
      "Epoch 104, Loss: 1312.2129540076623\n",
      "Epoch 105, Loss: 1155.4254854642427\n",
      "Epoch 106, Loss: 1123.676526583158\n",
      "Epoch 107, Loss: 966.6415581336388\n",
      "Epoch 108, Loss: 957.4768711970403\n",
      "Epoch 109, Loss: 954.4365539550781\n",
      "Epoch 110, Loss: 1438.418475811298\n",
      "Epoch 111, Loss: 1228.522176889273\n",
      "Epoch 112, Loss: 968.8660383958083\n",
      "Epoch 113, Loss: 923.1070204514724\n",
      "Epoch 114, Loss: 918.7085277850812\n",
      "Epoch 115, Loss: 909.2422720102163\n",
      "Epoch 116, Loss: 901.5870420015775\n",
      "Epoch 117, Loss: 899.6080334003155\n",
      "Epoch 118, Loss: 921.1474503737229\n",
      "Epoch 119, Loss: 886.3707604041466\n",
      "Epoch 120, Loss: 926.7578840989333\n",
      "Epoch 121, Loss: 1075.1272348257212\n",
      "Epoch 122, Loss: 934.5793210543119\n",
      "Epoch 123, Loss: 880.50610938439\n",
      "Epoch 124, Loss: 862.9192763108474\n",
      "Epoch 125, Loss: 914.5961362398588\n",
      "Epoch 126, Loss: 1208.9368755634014\n",
      "Epoch 127, Loss: 866.1557922363281\n",
      "Epoch 128, Loss: 817.2851069523738\n",
      "Epoch 129, Loss: 807.5205007699819\n",
      "Epoch 130, Loss: 799.5420849139874\n",
      "Epoch 131, Loss: 804.7204777644231\n",
      "Epoch 132, Loss: 791.5518505389874\n",
      "Epoch 133, Loss: 779.5243166410006\n",
      "Epoch 134, Loss: 782.9755272498497\n",
      "Epoch 135, Loss: 776.8224322979266\n",
      "Epoch 136, Loss: 928.8952002892128\n",
      "Epoch 137, Loss: 1032.5687467134917\n",
      "Epoch 138, Loss: 830.8949937086838\n",
      "Epoch 139, Loss: 745.2909651536208\n",
      "Epoch 140, Loss: 747.2408588115985\n",
      "Epoch 141, Loss: 751.7874673696665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:03:13,050] Trial 18 finished with value: 13861034.767308975 and parameters: {'latent_dim_z1': 11, 'latent_dim_z2': 80, 'hidden_dim': 162, 'epochs': 142, 'causal_reg': 0.9373340869901359, 'learning_rate': 0.0030469086436128584}. Best is trial 14 with value: 588.743737582725.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142, Loss: 738.6695650540865\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:03:16,196] Trial 19 failed with parameters: {'latent_dim_z1': 23, 'latent_dim_z2': 62, 'hidden_dim': 176, 'epochs': 139, 'causal_reg': 0.04339982182898883, 'learning_rate': 0.07075888973894649} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:03:16,196] Trial 19 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:03:19,659] Trial 20 failed with parameters: {'latent_dim_z1': 24, 'latent_dim_z2': 62, 'hidden_dim': 176, 'epochs': 148, 'causal_reg': 0.9916134021837629, 'learning_rate': 0.08573528387489388} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:03:19,660] Trial 20 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:03:23,070] Trial 21 failed with parameters: {'latent_dim_z1': 24, 'latent_dim_z2': 61, 'hidden_dim': 174, 'epochs': 143, 'causal_reg': 0.8106571951036947, 'learning_rate': 0.07946494167683354} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:03:23,071] Trial 21 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:03:26,454] Trial 22 failed with parameters: {'latent_dim_z1': 24, 'latent_dim_z2': 62, 'hidden_dim': 181, 'epochs': 142, 'causal_reg': 0.020053486780782892, 'learning_rate': 0.030223101377494947} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:03:26,454] Trial 22 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:03:29,970] Trial 23 failed with parameters: {'latent_dim_z1': 27, 'latent_dim_z2': 63, 'hidden_dim': 185, 'epochs': 146, 'causal_reg': 0.0590318132138713, 'learning_rate': 0.05913255783239292} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:03:29,971] Trial 23 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 1, Loss: 669097.8211763822\n",
      "Epoch 2, Loss: 7973.499539888822\n",
      "Epoch 3, Loss: 4182.665517953726\n",
      "Epoch 4, Loss: 3287.4156165489785\n",
      "Epoch 5, Loss: 2876.7345017653247\n",
      "Epoch 6, Loss: 2613.783517690805\n",
      "Epoch 7, Loss: 2407.5496427095854\n",
      "Epoch 8, Loss: 2232.2781841571514\n",
      "Epoch 9, Loss: 2072.6230750450723\n",
      "Epoch 10, Loss: 1950.7826397235576\n",
      "Epoch 11, Loss: 1843.1265024038462\n",
      "Epoch 12, Loss: 1751.2873159555288\n",
      "Epoch 13, Loss: 1663.0396024263823\n",
      "Epoch 14, Loss: 1604.1461580716646\n",
      "Epoch 15, Loss: 1563.0304518479568\n",
      "Epoch 16, Loss: 1533.701167179988\n",
      "Epoch 17, Loss: 1502.7581857534556\n",
      "Epoch 18, Loss: 1472.7295344426082\n",
      "Epoch 19, Loss: 1451.7757333608774\n",
      "Epoch 20, Loss: 1440.3688823993389\n",
      "Epoch 21, Loss: 1411.2820504995493\n",
      "Epoch 22, Loss: 1391.5052678034856\n",
      "Epoch 23, Loss: 1375.388200026292\n",
      "Epoch 24, Loss: 1358.4842505821814\n",
      "Epoch 25, Loss: 1360.6271291879507\n",
      "Epoch 26, Loss: 1336.0004225510818\n",
      "Epoch 27, Loss: 1323.9892038198618\n",
      "Epoch 28, Loss: 1315.5993182842549\n",
      "Epoch 29, Loss: 1317.6042691744292\n",
      "Epoch 30, Loss: 1294.9458805964543\n",
      "Epoch 31, Loss: 1290.1441040039062\n",
      "Epoch 32, Loss: 1278.631105863131\n",
      "Epoch 33, Loss: 1274.61667574369\n",
      "Epoch 34, Loss: 1265.324991079477\n",
      "Epoch 35, Loss: 1262.8645324707031\n",
      "Epoch 36, Loss: 1239.6726989746094\n",
      "Epoch 37, Loss: 1235.7217994103064\n",
      "Epoch 38, Loss: 1228.388655442458\n",
      "Epoch 39, Loss: 1215.0733173076924\n",
      "Epoch 40, Loss: 1212.2039466271033\n",
      "Epoch 41, Loss: 1200.1858121431792\n",
      "Epoch 42, Loss: 1195.442141019381\n",
      "Epoch 43, Loss: 1185.6709195650542\n",
      "Epoch 44, Loss: 1182.4090529221755\n",
      "Epoch 45, Loss: 1175.8986957256611\n",
      "Epoch 46, Loss: 1173.8917611929087\n",
      "Epoch 47, Loss: 1191.7909639798677\n",
      "Epoch 48, Loss: 1162.4747267503005\n",
      "Epoch 49, Loss: 1148.724888728215\n",
      "Epoch 50, Loss: 1148.8679058368389\n",
      "Epoch 51, Loss: 1133.300501896785\n",
      "Epoch 52, Loss: 1132.5057185246394\n",
      "Epoch 53, Loss: 1127.8688659667969\n",
      "Epoch 54, Loss: 1127.4475520207332\n",
      "Epoch 55, Loss: 1112.6901714618389\n",
      "Epoch 56, Loss: 1106.3209298940806\n",
      "Epoch 57, Loss: 1110.562748835637\n",
      "Epoch 58, Loss: 1099.960195688101\n",
      "Epoch 59, Loss: 1092.2468590369592\n",
      "Epoch 60, Loss: 1089.367715688852\n",
      "Epoch 61, Loss: 1079.7006119948167\n",
      "Epoch 62, Loss: 1072.9359788161057\n",
      "Epoch 63, Loss: 1075.6794950045073\n",
      "Epoch 64, Loss: 1077.0594376784104\n",
      "Epoch 65, Loss: 1064.2376967210037\n",
      "Epoch 66, Loss: 1058.9256873497595\n",
      "Epoch 67, Loss: 1054.7970017653245\n",
      "Epoch 68, Loss: 1065.1222158578726\n",
      "Epoch 69, Loss: 1090.5244891826924\n",
      "Epoch 70, Loss: 1048.861079289363\n",
      "Epoch 71, Loss: 1049.6719642052283\n",
      "Epoch 72, Loss: 1050.8714294433594\n",
      "Epoch 73, Loss: 1026.7223581167368\n",
      "Epoch 74, Loss: 1022.0180992713341\n",
      "Epoch 75, Loss: 1012.998046875\n",
      "Epoch 76, Loss: 1013.2784846379207\n",
      "Epoch 77, Loss: 1020.2389420729417\n",
      "Epoch 78, Loss: 994.5389791635366\n",
      "Epoch 79, Loss: 994.9601264366737\n",
      "Epoch 80, Loss: 1004.1916316105769\n",
      "Epoch 81, Loss: 980.162349994366\n",
      "Epoch 82, Loss: 976.4589186448318\n",
      "Epoch 83, Loss: 976.9747584416316\n",
      "Epoch 84, Loss: 975.3274254432091\n",
      "Epoch 85, Loss: 964.5381587101863\n",
      "Epoch 86, Loss: 958.6474186823918\n",
      "Epoch 87, Loss: 946.5312805175781\n",
      "Epoch 88, Loss: 951.2273888221154\n",
      "Epoch 89, Loss: 968.0638815072866\n",
      "Epoch 90, Loss: 945.1809011606069\n",
      "Epoch 91, Loss: 931.0202190692609\n",
      "Epoch 92, Loss: 920.4900982196515\n",
      "Epoch 93, Loss: 908.5404076209435\n",
      "Epoch 94, Loss: 900.9931828425481\n",
      "Epoch 95, Loss: 903.9432748647837\n",
      "Epoch 96, Loss: 892.3312483567458\n",
      "Epoch 97, Loss: 886.4811166616587\n",
      "Epoch 98, Loss: 883.4906522310697\n",
      "Epoch 99, Loss: 874.3301403339093\n",
      "Epoch 100, Loss: 866.1859389085037\n",
      "Epoch 101, Loss: 863.3059575007512\n",
      "Epoch 102, Loss: 859.6328664926382\n",
      "Epoch 103, Loss: 857.2789952204778\n",
      "Epoch 104, Loss: 842.4719754732572\n",
      "Epoch 105, Loss: 830.2700711763822\n",
      "Epoch 106, Loss: 824.0326373760516\n",
      "Epoch 107, Loss: 810.6904590313251\n",
      "Epoch 108, Loss: 821.1204446645884\n",
      "Epoch 109, Loss: 804.8002190223107\n",
      "Epoch 110, Loss: 787.3230672983022\n",
      "Epoch 111, Loss: 782.8687990628756\n",
      "Epoch 112, Loss: 766.0504819429838\n",
      "Epoch 113, Loss: 755.5286043607272\n",
      "Epoch 114, Loss: 751.995112492488\n",
      "Epoch 115, Loss: 739.3716864952675\n",
      "Epoch 116, Loss: 721.1607501690204\n",
      "Epoch 117, Loss: 709.2595966045673\n",
      "Epoch 118, Loss: 704.4181002103365\n",
      "Epoch 119, Loss: 687.298583984375\n",
      "Epoch 120, Loss: 676.8838207538312\n",
      "Epoch 121, Loss: 664.6153235802284\n",
      "Epoch 122, Loss: 659.6607536902794\n",
      "Epoch 123, Loss: 629.8375384990985\n",
      "Epoch 124, Loss: 617.441641000601\n",
      "Epoch 125, Loss: 598.5398618257963\n",
      "Epoch 126, Loss: 589.2453918457031\n",
      "Epoch 127, Loss: 584.123541025015\n",
      "Epoch 128, Loss: 574.3104564960187\n",
      "Epoch 129, Loss: 577.6865563025841\n",
      "Epoch 130, Loss: 576.0482541597806\n",
      "Epoch 131, Loss: 569.1067141019381\n",
      "Epoch 132, Loss: 561.7058234581581\n",
      "Epoch 133, Loss: 567.58448439378\n",
      "Epoch 134, Loss: 572.1859224759615\n",
      "Epoch 135, Loss: 565.2289088322566\n",
      "Epoch 136, Loss: 568.7417579064003\n",
      "Epoch 137, Loss: 573.5845501239484\n",
      "Epoch 138, Loss: 558.1603616567759\n",
      "Epoch 139, Loss: 547.184587918795\n",
      "Epoch 140, Loss: 544.8851118821365\n",
      "Epoch 141, Loss: 557.0055988018329\n",
      "Epoch 142, Loss: 555.9507821890024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:03:33,385] Trial 24 finished with value: 5780.630861494276 and parameters: {'latent_dim_z1': 25, 'latent_dim_z2': 58, 'hidden_dim': 170, 'epochs': 144, 'causal_reg': 0.9984969518640051, 'learning_rate': 0.0017803324039332195}. Best is trial 14 with value: 588.743737582725.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143, Loss: 544.8175929142878\n",
      "Epoch 144, Loss: 562.3649022028997\n",
      "Epoch 1, Loss: 5981.802608783429\n",
      "Epoch 2, Loss: 790.4989224947416\n",
      "Epoch 3, Loss: 764.3441772460938\n",
      "Epoch 4, Loss: 751.0680119441106\n",
      "Epoch 5, Loss: 733.7494002122146\n",
      "Epoch 6, Loss: 711.5722891000601\n",
      "Epoch 7, Loss: 680.6839822622446\n",
      "Epoch 8, Loss: 648.736818753756\n",
      "Epoch 9, Loss: 610.8481938288762\n",
      "Epoch 10, Loss: 574.3831165020282\n",
      "Epoch 11, Loss: 538.8016944298378\n",
      "Epoch 12, Loss: 506.69769991361176\n",
      "Epoch 13, Loss: 483.0282475398137\n",
      "Epoch 14, Loss: 461.6667269193209\n",
      "Epoch 15, Loss: 445.8706442025992\n",
      "Epoch 16, Loss: 431.16104595477765\n",
      "Epoch 17, Loss: 420.4732795128456\n",
      "Epoch 18, Loss: 414.43382615309497\n",
      "Epoch 19, Loss: 408.40517953725964\n",
      "Epoch 20, Loss: 403.451173048753\n",
      "Epoch 21, Loss: 397.95785052959735\n",
      "Epoch 22, Loss: 389.63379258375903\n",
      "Epoch 23, Loss: 388.01331974909857\n",
      "Epoch 24, Loss: 381.83572152944714\n",
      "Epoch 25, Loss: 377.29618483323316\n",
      "Epoch 26, Loss: 372.9981701190655\n",
      "Epoch 27, Loss: 373.05995178222656\n",
      "Epoch 28, Loss: 373.36441744290863\n",
      "Epoch 29, Loss: 371.6555703970102\n",
      "Epoch 30, Loss: 378.37005732609674\n",
      "Epoch 31, Loss: 360.49661724384015\n",
      "Epoch 32, Loss: 359.11131403996393\n",
      "Epoch 33, Loss: 358.54530569223255\n",
      "Epoch 34, Loss: 360.4462456336388\n",
      "Epoch 35, Loss: 352.4825697678786\n",
      "Epoch 36, Loss: 361.01337374173676\n",
      "Epoch 37, Loss: 353.94582308255707\n",
      "Epoch 38, Loss: 354.5459993802584\n",
      "Epoch 39, Loss: 354.0340705284706\n",
      "Epoch 40, Loss: 354.18948951134314\n",
      "Epoch 41, Loss: 345.77380723219653\n",
      "Epoch 42, Loss: 349.73850778432995\n",
      "Epoch 43, Loss: 345.6073444073017\n",
      "Epoch 44, Loss: 341.27340228740985\n",
      "Epoch 45, Loss: 339.22964242788464\n",
      "Epoch 46, Loss: 346.6361107459435\n",
      "Epoch 47, Loss: 342.66477966308594\n",
      "Epoch 48, Loss: 337.59356924203723\n",
      "Epoch 49, Loss: 336.00800147423377\n",
      "Epoch 50, Loss: 337.97192734938403\n",
      "Epoch 51, Loss: 334.17442673903247\n",
      "Epoch 52, Loss: 349.47374666654144\n",
      "Epoch 53, Loss: 335.20521428034857\n",
      "Epoch 54, Loss: 329.80731905423676\n",
      "Epoch 55, Loss: 331.0129194993239\n",
      "Epoch 56, Loss: 328.6305330716647\n",
      "Epoch 57, Loss: 328.6418069692758\n",
      "Epoch 58, Loss: 326.3896742600661\n",
      "Epoch 59, Loss: 324.1421426626352\n",
      "Epoch 60, Loss: 328.12767322246845\n",
      "Epoch 61, Loss: 327.4801459679237\n",
      "Epoch 62, Loss: 324.6129537729117\n",
      "Epoch 63, Loss: 326.5331872793344\n",
      "Epoch 64, Loss: 321.45073758638824\n",
      "Epoch 65, Loss: 323.75649789663464\n",
      "Epoch 66, Loss: 320.3301573533278\n",
      "Epoch 67, Loss: 318.53573138897235\n",
      "Epoch 68, Loss: 318.2837160550631\n",
      "Epoch 69, Loss: 316.4483208289513\n",
      "Epoch 70, Loss: 318.3268737792969\n",
      "Epoch 71, Loss: 317.4268071101262\n",
      "Epoch 72, Loss: 316.08448439378003\n",
      "Epoch 73, Loss: 333.30612417367786\n",
      "Epoch 74, Loss: 346.98387498121997\n",
      "Epoch 75, Loss: 323.1771310659555\n",
      "Epoch 76, Loss: 312.62987342247595\n",
      "Epoch 77, Loss: 313.5572063739483\n",
      "Epoch 78, Loss: 311.90022629957934\n",
      "Epoch 79, Loss: 317.5065671480619\n",
      "Epoch 80, Loss: 317.35927229661206\n",
      "Epoch 81, Loss: 319.7039348895733\n",
      "Epoch 82, Loss: 312.18883925217847\n",
      "Epoch 83, Loss: 311.66244448148285\n",
      "Epoch 84, Loss: 311.34286733774036\n",
      "Epoch 85, Loss: 308.697512113131\n",
      "Epoch 86, Loss: 309.112798837515\n",
      "Epoch 87, Loss: 308.66951340895434\n",
      "Epoch 88, Loss: 311.5736841055063\n",
      "Epoch 89, Loss: 313.426758986253\n",
      "Epoch 90, Loss: 308.92552537184497\n",
      "Epoch 91, Loss: 308.2965017465445\n",
      "Epoch 92, Loss: 306.8677062988281\n",
      "Epoch 93, Loss: 307.87459622896637\n",
      "Epoch 94, Loss: 306.7510798527644\n",
      "Epoch 95, Loss: 313.0724769005409\n",
      "Epoch 96, Loss: 306.00726905235877\n",
      "Epoch 97, Loss: 302.90447411170373\n",
      "Epoch 98, Loss: 302.2913472102239\n",
      "Epoch 99, Loss: 301.6342386099008\n",
      "Epoch 100, Loss: 301.52818885216345\n",
      "Epoch 101, Loss: 303.51784456693207\n",
      "Epoch 102, Loss: 304.6055086576022\n",
      "Epoch 103, Loss: 300.13707557091345\n",
      "Epoch 104, Loss: 298.9184312086839\n",
      "Epoch 105, Loss: 303.92601130558893\n",
      "Epoch 106, Loss: 298.9252037635216\n",
      "Epoch 107, Loss: 298.34534336970404\n",
      "Epoch 108, Loss: 299.00262685922473\n",
      "Epoch 109, Loss: 298.6439983661358\n",
      "Epoch 110, Loss: 299.984616793119\n",
      "Epoch 111, Loss: 300.77088341346155\n",
      "Epoch 112, Loss: 298.68204909104566\n",
      "Epoch 113, Loss: 303.0579047569862\n",
      "Epoch 114, Loss: 296.42095947265625\n",
      "Epoch 115, Loss: 297.85880103478064\n",
      "Epoch 116, Loss: 295.07348045936\n",
      "Epoch 117, Loss: 295.4627973116361\n",
      "Epoch 118, Loss: 303.80082350510816\n",
      "Epoch 119, Loss: 295.0182096041166\n",
      "Epoch 120, Loss: 296.23049692007214\n",
      "Epoch 121, Loss: 299.16978571965143\n",
      "Epoch 122, Loss: 293.02853334867035\n",
      "Epoch 123, Loss: 293.9075211745042\n",
      "Epoch 124, Loss: 297.3750528188852\n",
      "Epoch 125, Loss: 293.4637979360727\n",
      "Epoch 126, Loss: 291.23614384577826\n",
      "Epoch 127, Loss: 291.3737476055439\n",
      "Epoch 128, Loss: 291.8089816753681\n",
      "Epoch 129, Loss: 291.8637390136719\n",
      "Epoch 130, Loss: 289.9510544996995\n",
      "Epoch 131, Loss: 289.7571012056791\n",
      "Epoch 132, Loss: 289.4106950026292\n",
      "Epoch 133, Loss: 289.1744126539964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:03:36,484] Trial 25 finished with value: 394.9877769973252 and parameters: {'latent_dim_z1': 13, 'latent_dim_z2': 63, 'hidden_dim': 109, 'epochs': 140, 'causal_reg': 0.7994006096451421, 'learning_rate': 0.00036373702905184827}. Best is trial 25 with value: 394.9877769973252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134, Loss: 292.25028933011566\n",
      "Epoch 135, Loss: 301.76279918964093\n",
      "Epoch 136, Loss: 289.08527198204627\n",
      "Epoch 137, Loss: 288.0461255587064\n",
      "Epoch 138, Loss: 286.52419222318207\n",
      "Epoch 139, Loss: 286.6800325833834\n",
      "Epoch 140, Loss: 293.0393571120042\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:03:41,229] Trial 26 failed with parameters: {'latent_dim_z1': 48, 'latent_dim_z2': 67, 'hidden_dim': 113, 'epochs': 197, 'causal_reg': 0.7784857018038409, 'learning_rate': 0.07805754908882372} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:03:41,229] Trial 26 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190, Loss: nan\n",
      "Epoch 191, Loss: nan\n",
      "Epoch 192, Loss: nan\n",
      "Epoch 193, Loss: nan\n",
      "Epoch 194, Loss: nan\n",
      "Epoch 195, Loss: nan\n",
      "Epoch 196, Loss: nan\n",
      "Epoch 197, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 190, Loss: nan\n",
      "Epoch 191, Loss: nan\n",
      "Epoch 192, Loss: nan\n",
      "Epoch 193, Loss: nan\n",
      "Epoch 194, Loss: nan\n",
      "Epoch 195, Loss: nan\n",
      "Epoch 196, Loss: nan\n",
      "Epoch 197, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:03:45,766] Trial 27 failed with parameters: {'latent_dim_z1': 23, 'latent_dim_z2': 70, 'hidden_dim': 98, 'epochs': 200, 'causal_reg': 0.7612100324829553, 'learning_rate': 0.050785371205727625} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:03:45,766] Trial 27 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198, Loss: nan\n",
      "Epoch 199, Loss: nan\n",
      "Epoch 200, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:03:50,076] Trial 28 failed with parameters: {'latent_dim_z1': 48, 'latent_dim_z2': 68, 'hidden_dim': 100, 'epochs': 181, 'causal_reg': 0.7499455475638165, 'learning_rate': 0.02470863648876703} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:03:50,076] Trial 28 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:03:54,676] Trial 29 failed with parameters: {'latent_dim_z1': 48, 'latent_dim_z2': 69, 'hidden_dim': 98, 'epochs': 182, 'causal_reg': 0.02340926529345061, 'learning_rate': 0.0937615586954773} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:03:54,676] Trial 29 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:04:00,219] Trial 30 failed with parameters: {'latent_dim_z1': 25, 'latent_dim_z2': 67, 'hidden_dim': 101, 'epochs': 179, 'causal_reg': 0.01562500714579157, 'learning_rate': 0.02066550473976} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:04:00,219] Trial 30 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:04:05,888] Trial 31 failed with parameters: {'latent_dim_z1': 27, 'latent_dim_z2': 72, 'hidden_dim': 102, 'epochs': 183, 'causal_reg': 0.7487052024286004, 'learning_rate': 0.026965750113030332} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:04:05,889] Trial 31 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:04:10,737] Trial 32 failed with parameters: {'latent_dim_z1': 26, 'latent_dim_z2': 70, 'hidden_dim': 89, 'epochs': 183, 'causal_reg': 0.06003201597860558, 'learning_rate': 0.06505881631781141} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:04:10,737] Trial 32 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 190, Loss: nan\n",
      "Epoch 191, Loss: nan\n",
      "Epoch 192, Loss: nan\n",
      "Epoch 193, Loss: nan\n",
      "Epoch 194, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:04:15,739] Trial 33 failed with parameters: {'latent_dim_z1': 49, 'latent_dim_z2': 73, 'hidden_dim': 100, 'epochs': 199, 'causal_reg': 0.745942833515063, 'learning_rate': 0.08638280163227896} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:04:15,739] Trial 33 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195, Loss: nan\n",
      "Epoch 196, Loss: nan\n",
      "Epoch 197, Loss: nan\n",
      "Epoch 198, Loss: nan\n",
      "Epoch 199, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 190, Loss: nan\n",
      "Epoch 191, Loss: nan\n",
      "Epoch 192, Loss: nan\n",
      "Epoch 193, Loss: nan\n",
      "Epoch 194, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:04:20,438] Trial 34 failed with parameters: {'latent_dim_z1': 49, 'latent_dim_z2': 67, 'hidden_dim': 103, 'epochs': 200, 'causal_reg': 0.7458948241002586, 'learning_rate': 0.07775031203632021} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:04:20,438] Trial 34 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195, Loss: nan\n",
      "Epoch 196, Loss: nan\n",
      "Epoch 197, Loss: nan\n",
      "Epoch 198, Loss: nan\n",
      "Epoch 199, Loss: nan\n",
      "Epoch 200, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 190, Loss: nan\n",
      "Epoch 191, Loss: nan\n",
      "Epoch 192, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:04:25,197] Trial 35 failed with parameters: {'latent_dim_z1': 46, 'latent_dim_z2': 70, 'hidden_dim': 94, 'epochs': 199, 'causal_reg': 0.7439446810508499, 'learning_rate': 0.018904712967494783} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:04:25,198] Trial 35 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193, Loss: nan\n",
      "Epoch 194, Loss: nan\n",
      "Epoch 195, Loss: nan\n",
      "Epoch 196, Loss: nan\n",
      "Epoch 197, Loss: nan\n",
      "Epoch 198, Loss: nan\n",
      "Epoch 199, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 190, Loss: nan\n",
      "Epoch 191, Loss: nan\n",
      "Epoch 192, Loss: nan\n",
      "Epoch 193, Loss: nan\n",
      "Epoch 194, Loss: nan\n",
      "Epoch 195, Loss: nan\n",
      "Epoch 196, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:04:31,131] Trial 36 failed with parameters: {'latent_dim_z1': 27, 'latent_dim_z2': 70, 'hidden_dim': 101, 'epochs': 199, 'causal_reg': 0.7532543018082969, 'learning_rate': 0.07377354268962633} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:04:31,131] Trial 36 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197, Loss: nan\n",
      "Epoch 198, Loss: nan\n",
      "Epoch 199, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:04:36,042] Trial 37 failed with parameters: {'latent_dim_z1': 48, 'latent_dim_z2': 69, 'hidden_dim': 102, 'epochs': 181, 'causal_reg': 0.7621235453844887, 'learning_rate': 0.060960832931035436} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:04:36,043] Trial 37 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:04:41,887] Trial 38 failed with parameters: {'latent_dim_z1': 48, 'latent_dim_z2': 70, 'hidden_dim': 94, 'epochs': 187, 'causal_reg': 0.7902119001817788, 'learning_rate': 0.011302840449358165} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:04:41,887] Trial 38 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:04:46,491] Trial 39 failed with parameters: {'latent_dim_z1': 48, 'latent_dim_z2': 69, 'hidden_dim': 105, 'epochs': 184, 'causal_reg': 0.7634827140931757, 'learning_rate': 0.09237279570163569} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:04:46,491] Trial 39 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:04:50,855] Trial 40 failed with parameters: {'latent_dim_z1': 47, 'latent_dim_z2': 70, 'hidden_dim': 105, 'epochs': 182, 'causal_reg': 0.07937727483587315, 'learning_rate': 0.06389072844533551} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:04:50,855] Trial 40 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 190, Loss: nan\n",
      "Epoch 191, Loss: nan\n",
      "Epoch 192, Loss: nan\n",
      "Epoch 193, Loss: nan\n",
      "Epoch 194, Loss: nan\n",
      "Epoch 195, Loss: nan\n",
      "Epoch 196, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:04:55,758] Trial 41 failed with parameters: {'latent_dim_z1': 46, 'latent_dim_z2': 68, 'hidden_dim': 101, 'epochs': 199, 'causal_reg': 0.05712766298411348, 'learning_rate': 0.04211667123820591} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:04:55,758] Trial 41 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197, Loss: nan\n",
      "Epoch 198, Loss: nan\n",
      "Epoch 199, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:05:00,231] Trial 42 failed with parameters: {'latent_dim_z1': 47, 'latent_dim_z2': 73, 'hidden_dim': 95, 'epochs': 185, 'causal_reg': 0.762200085984847, 'learning_rate': 0.04010654194372205} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:05:00,232] Trial 42 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 1, Loss: 4.871755637577473e+30\n",
      "Epoch 2, Loss: 5.011334888164174e+30\n",
      "Epoch 3, Loss: 5.0113349782523965e+30\n",
      "Epoch 4, Loss: 5.011334917224891e+30\n",
      "Epoch 5, Loss: 5.011335045455304e+30\n",
      "Epoch 6, Loss: 5.011334995688826e+30\n",
      "Epoch 7, Loss: 5.011335129368124e+30\n",
      "Epoch 8, Loss: 5.011335130276271e+30\n",
      "Epoch 9, Loss: 5.0113348765398875e+30\n",
      "Epoch 10, Loss: 5.0113350494511524e+30\n",
      "Epoch 11, Loss: 5.0113351744122346e+30\n",
      "Epoch 12, Loss: 5.0113351497106257e+30\n",
      "Epoch 13, Loss: 5.01133500150097e+30\n",
      "Epoch 14, Loss: 5.011334792263809e+30\n",
      "Epoch 15, Loss: 5.011335083597495e+30\n",
      "Epoch 16, Loss: 5.0113350944952636e+30\n",
      "Epoch 17, Loss: 5.011334899788461e+30\n",
      "Epoch 18, Loss: 5.0113348939763177e+30\n",
      "Epoch 19, Loss: 5.0113349608159664e+30\n",
      "Epoch 20, Loss: 5.01133503637383e+30\n",
      "Epoch 21, Loss: 5.011335148984108e+30\n",
      "Epoch 22, Loss: 5.01133507124669e+30\n",
      "Epoch 23, Loss: 5.0113350349207943e+30\n",
      "Epoch 24, Loss: 5.01133507124669e+30\n",
      "Epoch 25, Loss: 5.011335050904188e+30\n",
      "Epoch 26, Loss: 5.01133494555909e+30\n",
      "Epoch 27, Loss: 5.0113350160313284e+30\n",
      "Epoch 28, Loss: 5.011334930302213e+30\n",
      "Epoch 29, Loss: 5.0113349186779267e+30\n",
      "Epoch 30, Loss: 5.0113350203904357e+30\n",
      "Epoch 31, Loss: 5.011335080691423e+30\n",
      "Epoch 32, Loss: 5.011335056716332e+30\n",
      "Epoch 33, Loss: 5.011335111931694e+30\n",
      "Epoch 34, Loss: 5.011334981158468e+30\n",
      "Epoch 35, Loss: 5.011335027655615e+30\n",
      "Epoch 36, Loss: 5.011335017484364e+30\n",
      "Epoch 37, Loss: 5.011335050904188e+30\n",
      "Epoch 38, Loss: 5.011334800982024e+30\n",
      "Epoch 39, Loss: 5.011335072699726e+30\n",
      "Epoch 40, Loss: 5.011335033467758e+30\n",
      "Epoch 41, Loss: 5.011335033467758e+30\n",
      "Epoch 42, Loss: 5.011335074152762e+30\n",
      "Epoch 43, Loss: 5.011334982611504e+30\n",
      "Epoch 44, Loss: 5.0113350850505307e+30\n",
      "Epoch 45, Loss: 5.011334902694533e+30\n",
      "Epoch 46, Loss: 5.0113349579098943e+30\n",
      "Epoch 47, Loss: 5.01133503637383e+30\n",
      "Epoch 48, Loss: 5.011334944832572e+30\n",
      "Epoch 49, Loss: 5.011334792263809e+30\n",
      "Epoch 50, Loss: 5.0113350479981167e+30\n",
      "Epoch 51, Loss: 5.011334995688826e+30\n",
      "Epoch 52, Loss: 5.011334939746946e+30\n",
      "Epoch 53, Loss: 5.0113349608159664e+30\n",
      "Epoch 54, Loss: 5.011335050904188e+30\n",
      "Epoch 55, Loss: 5.011335054536778e+30\n",
      "Epoch 56, Loss: 5.0113351235559804e+30\n",
      "Epoch 57, Loss: 5.011334970987217e+30\n",
      "Epoch 58, Loss: 5.011335032014722e+30\n",
      "Epoch 59, Loss: 5.011335024749543e+30\n",
      "Epoch 60, Loss: 5.011334860556493e+30\n",
      "Epoch 61, Loss: 5.0113349695341814e+30\n",
      "Epoch 62, Loss: 5.0113350116722206e+30\n",
      "Epoch 63, Loss: 5.0113349927827546e+30\n",
      "Epoch 64, Loss: 5.011334982611504e+30\n",
      "Epoch 65, Loss: 5.011334934661321e+30\n",
      "Epoch 66, Loss: 5.0113349753463245e+30\n",
      "Epoch 67, Loss: 5.0113350189374e+30\n",
      "Epoch 68, Loss: 5.011335074152762e+30\n",
      "Epoch 69, Loss: 5.011334955003823e+30\n",
      "Epoch 70, Loss: 5.0113349404734647e+30\n",
      "Epoch 71, Loss: 5.0113349608159664e+30\n",
      "Epoch 72, Loss: 5.0113350305616865e+30\n",
      "Epoch 73, Loss: 5.0113350392799016e+30\n",
      "Epoch 74, Loss: 5.011334972440253e+30\n",
      "Epoch 75, Loss: 5.011334955003823e+30\n",
      "Epoch 76, Loss: 5.01133510611955e+30\n",
      "Epoch 77, Loss: 5.011335033467758e+30\n",
      "Epoch 78, Loss: 5.01133503637383e+30\n",
      "Epoch 79, Loss: 5.011334955003823e+30\n",
      "Epoch 80, Loss: 5.011335029108651e+30\n",
      "Epoch 81, Loss: 5.0113348736338155e+30\n",
      "Epoch 82, Loss: 5.0113348591034574e+30\n",
      "Epoch 83, Loss: 5.01133500150097e+30\n",
      "Epoch 84, Loss: 5.0113350392799016e+30\n",
      "Epoch 85, Loss: 5.011335059622403e+30\n",
      "Epoch 86, Loss: 5.011335050904188e+30\n",
      "Epoch 87, Loss: 5.0113351380863385e+30\n",
      "Epoch 88, Loss: 5.0113350189374e+30\n",
      "Epoch 89, Loss: 5.0113349201309624e+30\n",
      "Epoch 90, Loss: 5.011335050904188e+30\n",
      "Epoch 91, Loss: 5.0113350131252563e+30\n",
      "Epoch 92, Loss: 5.011334917224891e+30\n",
      "Epoch 93, Loss: 5.011335074152762e+30\n",
      "Epoch 94, Loss: 5.0113352027464335e+30\n",
      "Epoch 95, Loss: 5.011334986970611e+30\n",
      "Epoch 96, Loss: 5.011335056716332e+30\n",
      "Epoch 97, Loss: 5.01133505381026e+30\n",
      "Epoch 98, Loss: 5.011334991329719e+30\n",
      "Epoch 99, Loss: 5.0113349128657836e+30\n",
      "Epoch 100, Loss: 5.011335043639009e+30\n",
      "Epoch 101, Loss: 5.01133497679936e+30\n",
      "Epoch 102, Loss: 5.0113349695341814e+30\n",
      "Epoch 103, Loss: 5.011335037826866e+30\n",
      "Epoch 104, Loss: 5.0113350607121804e+30\n",
      "Epoch 105, Loss: 5.0113350679773597e+30\n",
      "Epoch 106, Loss: 5.0113350218434714e+30\n",
      "Epoch 107, Loss: 5.0113349927827546e+30\n",
      "Epoch 108, Loss: 5.011334902694533e+30\n",
      "Epoch 109, Loss: 5.011334880898995e+30\n",
      "Epoch 110, Loss: 5.011335065434547e+30\n",
      "Epoch 111, Loss: 5.0113350160313284e+30\n",
      "Epoch 112, Loss: 5.0113349666281094e+30\n",
      "Epoch 113, Loss: 5.011334899788461e+30\n",
      "Epoch 114, Loss: 5.011334888164174e+30\n",
      "Epoch 115, Loss: 5.011335029108651e+30\n",
      "Epoch 116, Loss: 5.0113350392799016e+30\n",
      "Epoch 117, Loss: 5.011335027655615e+30\n",
      "Epoch 118, Loss: 5.0113349695341814e+30\n",
      "Epoch 119, Loss: 5.0113350349207943e+30\n",
      "Epoch 120, Loss: 5.011334934661321e+30\n",
      "Epoch 121, Loss: 5.0113350073131133e+30\n",
      "Epoch 122, Loss: 5.0113349579098943e+30\n",
      "Epoch 123, Loss: 5.0113349288491775e+30\n",
      "Epoch 124, Loss: 5.011335061075439e+30\n",
      "Epoch 125, Loss: 5.011335074152762e+30\n",
      "Epoch 126, Loss: 5.011335056716332e+30\n",
      "Epoch 127, Loss: 5.011334896882389e+30\n",
      "Epoch 128, Loss: 5.011334869274708e+30\n",
      "Epoch 129, Loss: 5.0113351380863385e+30\n",
      "Epoch 130, Loss: 5.0113349041475686e+30\n",
      "Epoch 131, Loss: 5.0113349695341814e+30\n",
      "Epoch 132, Loss: 5.0113350683406184e+30\n",
      "Epoch 133, Loss: 5.011335059622403e+30\n",
      "Epoch 134, Loss: 5.0113348126063104e+30\n",
      "Epoch 135, Loss: 5.011335024749543e+30\n",
      "Epoch 136, Loss: 5.011335082870977e+30\n",
      "Epoch 137, Loss: 5.011334963722038e+30\n",
      "Epoch 138, Loss: 5.011334850385242e+30\n",
      "Epoch 139, Loss: 5.011334841667027e+30\n",
      "Epoch 140, Loss: 5.011335024749543e+30\n",
      "Epoch 141, Loss: 5.011335017484364e+30\n",
      "Epoch 142, Loss: 5.011334989876683e+30\n",
      "Epoch 143, Loss: 5.01133501530481e+30\n",
      "Epoch 144, Loss: 5.0113349194044445e+30\n",
      "Epoch 145, Loss: 5.0113350683406184e+30\n",
      "Epoch 146, Loss: 5.01133489651913e+30\n",
      "Epoch 147, Loss: 5.011335143898482e+30\n",
      "Epoch 148, Loss: 5.01133510611955e+30\n",
      "Epoch 149, Loss: 5.0113350407329373e+30\n",
      "Epoch 150, Loss: 5.011334995688826e+30\n",
      "Epoch 151, Loss: 5.0113349404734647e+30\n",
      "Epoch 152, Loss: 5.011335059622403e+30\n",
      "Epoch 153, Loss: 5.0113349855175753e+30\n",
      "Epoch 154, Loss: 5.011334926669624e+30\n",
      "Epoch 155, Loss: 5.0113350588958854e+30\n",
      "Epoch 156, Loss: 5.011334943379536e+30\n",
      "Epoch 157, Loss: 5.011334989876683e+30\n",
      "Epoch 158, Loss: 5.011335033467758e+30\n",
      "Epoch 159, Loss: 5.0113349230370345e+30\n",
      "Epoch 160, Loss: 5.011334963722038e+30\n",
      "Epoch 161, Loss: 5.0113350770588335e+30\n",
      "Epoch 162, Loss: 5.011335019663918e+30\n",
      "Epoch 163, Loss: 5.011335024749543e+30\n",
      "Epoch 164, Loss: 5.011335079964905e+30\n",
      "Epoch 165, Loss: 5.011334930302213e+30\n",
      "Epoch 166, Loss: 5.0113351380863385e+30\n",
      "Epoch 167, Loss: 5.011334927396142e+30\n",
      "Epoch 168, Loss: 5.011335033467758e+30\n",
      "Epoch 169, Loss: 5.0113350160313284e+30\n",
      "Epoch 170, Loss: 5.011334792263809e+30\n",
      "Epoch 171, Loss: 5.011335052357224e+30\n",
      "Epoch 172, Loss: 5.0113349593629306e+30\n",
      "Epoch 173, Loss: 5.011335167147056e+30\n",
      "Epoch 174, Loss: 5.011335033467758e+30\n",
      "Epoch 175, Loss: 5.011335065434547e+30\n",
      "Epoch 176, Loss: 5.011335014578292e+30\n",
      "Epoch 177, Loss: 5.011334924126811e+30\n",
      "Epoch 178, Loss: 5.011335090136156e+30\n",
      "Epoch 179, Loss: 5.011335046545081e+30\n",
      "Epoch 180, Loss: 5.0113350392799016e+30\n",
      "Epoch 181, Loss: 5.0113349666281094e+30\n",
      "Epoch 182, Loss: 5.011334995688826e+30\n",
      "Epoch 183, Loss: 5.011334896882389e+30\n",
      "Epoch 184, Loss: 5.0113349579098943e+30\n",
      "Epoch 185, Loss: 5.011334963722038e+30\n",
      "Epoch 186, Loss: 5.0113349840645395e+30\n",
      "Epoch 187, Loss: 5.0113350392799016e+30\n",
      "Epoch 188, Loss: 5.011335004407041e+30\n",
      "Epoch 189, Loss: 5.0113348852581026e+30\n",
      "Epoch 190, Loss: 5.0113349855175753e+30\n",
      "Epoch 191, Loss: 5.0113350865035664e+30\n",
      "Epoch 192, Loss: 5.011335000047934e+30\n",
      "Epoch 193, Loss: 5.011335050904188e+30\n",
      "Epoch 194, Loss: 5.011335052357224e+30\n",
      "Epoch 195, Loss: 5.0113348910702456e+30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:05:04,988] Trial 43 finished with value: 5.010637104724031e+30 and parameters: {'latent_dim_z1': 48, 'latent_dim_z2': 66, 'hidden_dim': 104, 'epochs': 200, 'causal_reg': 0.0018573084992757338, 'learning_rate': 0.010518853315477495}. Best is trial 25 with value: 394.9877769973252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196, Loss: 5.0113349855175753e+30\n",
      "Epoch 197, Loss: 5.011335010219185e+30\n",
      "Epoch 198, Loss: 5.0113349579098943e+30\n",
      "Epoch 199, Loss: 5.011335043639009e+30\n",
      "Epoch 200, Loss: 5.0113350770588335e+30\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:05:09,001] Trial 44 failed with parameters: {'latent_dim_z1': 27, 'latent_dim_z2': 80, 'hidden_dim': 90, 'epochs': 168, 'causal_reg': 0.767458397133884, 'learning_rate': 0.08265315709814397} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:05:09,002] Trial 44 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:05:13,101] Trial 45 failed with parameters: {'latent_dim_z1': 27, 'latent_dim_z2': 78, 'hidden_dim': 107, 'epochs': 169, 'causal_reg': 0.7612796346792543, 'learning_rate': 0.034302779254904485} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:05:13,101] Trial 45 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:05:17,150] Trial 46 failed with parameters: {'latent_dim_z1': 28, 'latent_dim_z2': 79, 'hidden_dim': 105, 'epochs': 169, 'causal_reg': 0.774185290485533, 'learning_rate': 0.09860480383780239} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:05:17,150] Trial 46 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 1, Loss: 17618.136709359977\n",
      "Epoch 2, Loss: 2933.2983844463643\n",
      "Epoch 3, Loss: 1735.6744619516226\n",
      "Epoch 4, Loss: 1442.3659034142127\n",
      "Epoch 5, Loss: 1309.7067425067607\n",
      "Epoch 6, Loss: 1220.384763277494\n",
      "Epoch 7, Loss: 1142.3282306377705\n",
      "Epoch 8, Loss: 1065.5278930664062\n",
      "Epoch 9, Loss: 987.1923522949219\n",
      "Epoch 10, Loss: 911.9671337421124\n",
      "Epoch 11, Loss: 844.3709681584285\n",
      "Epoch 12, Loss: 785.7015486497146\n",
      "Epoch 13, Loss: 741.7923231858474\n",
      "Epoch 14, Loss: 703.4593071570763\n",
      "Epoch 15, Loss: 681.7984313964844\n",
      "Epoch 16, Loss: 654.3442089374249\n",
      "Epoch 17, Loss: 629.562744140625\n",
      "Epoch 18, Loss: 612.1134760929988\n",
      "Epoch 19, Loss: 594.8410022442157\n",
      "Epoch 20, Loss: 583.932619535006\n",
      "Epoch 21, Loss: 571.8142218956581\n",
      "Epoch 22, Loss: 559.9197998046875\n",
      "Epoch 23, Loss: 554.4791471041166\n",
      "Epoch 24, Loss: 541.806408221905\n",
      "Epoch 25, Loss: 536.7112778883713\n",
      "Epoch 26, Loss: 530.6668572059044\n",
      "Epoch 27, Loss: 521.339840228741\n",
      "Epoch 28, Loss: 514.9402982271635\n",
      "Epoch 29, Loss: 509.64131516676684\n",
      "Epoch 30, Loss: 502.4968731219952\n",
      "Epoch 31, Loss: 497.95814162034253\n",
      "Epoch 32, Loss: 493.796380849985\n",
      "Epoch 33, Loss: 488.8531623253456\n",
      "Epoch 34, Loss: 485.6602454552284\n",
      "Epoch 35, Loss: 485.16168212890625\n",
      "Epoch 36, Loss: 477.85997713529144\n",
      "Epoch 37, Loss: 476.61305941068207\n",
      "Epoch 38, Loss: 472.48175635704627\n",
      "Epoch 39, Loss: 468.74444697453424\n",
      "Epoch 40, Loss: 465.58690584622894\n",
      "Epoch 41, Loss: 464.47833369328424\n",
      "Epoch 42, Loss: 463.95849257249097\n",
      "Epoch 43, Loss: 461.6880657489483\n",
      "Epoch 44, Loss: 456.7552032470703\n",
      "Epoch 45, Loss: 458.01243708683893\n",
      "Epoch 46, Loss: 455.20327641413763\n",
      "Epoch 47, Loss: 447.8154801588792\n",
      "Epoch 48, Loss: 449.64421316293567\n",
      "Epoch 49, Loss: 456.17616976224457\n",
      "Epoch 50, Loss: 441.65914447490985\n",
      "Epoch 51, Loss: 441.240721482497\n",
      "Epoch 52, Loss: 437.48020935058594\n",
      "Epoch 53, Loss: 441.770756648137\n",
      "Epoch 54, Loss: 433.8721172626202\n",
      "Epoch 55, Loss: 434.90335552509015\n",
      "Epoch 56, Loss: 431.93823594313403\n",
      "Epoch 57, Loss: 433.07740666316107\n",
      "Epoch 58, Loss: 425.58844111515924\n",
      "Epoch 59, Loss: 423.9409954364483\n",
      "Epoch 60, Loss: 422.3221705510066\n",
      "Epoch 61, Loss: 427.9170227050781\n",
      "Epoch 62, Loss: 423.8321274977464\n",
      "Epoch 63, Loss: 422.6844769991361\n",
      "Epoch 64, Loss: 417.22567279522235\n",
      "Epoch 65, Loss: 418.68043165940503\n",
      "Epoch 66, Loss: 413.22481126051684\n",
      "Epoch 67, Loss: 419.431159386268\n",
      "Epoch 68, Loss: 414.9554701585036\n",
      "Epoch 69, Loss: 409.4450366680439\n",
      "Epoch 70, Loss: 408.72822922926684\n",
      "Epoch 71, Loss: 406.78417147122894\n",
      "Epoch 72, Loss: 405.6921703632061\n",
      "Epoch 73, Loss: 405.3661358173077\n",
      "Epoch 74, Loss: 402.2034665621244\n",
      "Epoch 75, Loss: 403.02660780686597\n",
      "Epoch 76, Loss: 405.57850646972656\n",
      "Epoch 77, Loss: 405.28318551870495\n",
      "Epoch 78, Loss: 402.53118779109076\n",
      "Epoch 79, Loss: 406.4142279991737\n",
      "Epoch 80, Loss: 421.2212864802434\n",
      "Epoch 81, Loss: 401.1453282282903\n",
      "Epoch 82, Loss: 399.5746060884916\n",
      "Epoch 83, Loss: 396.3564065786508\n",
      "Epoch 84, Loss: 393.8439483642578\n",
      "Epoch 85, Loss: 396.24188349797174\n",
      "Epoch 86, Loss: 396.8910158597506\n",
      "Epoch 87, Loss: 388.30850806603064\n",
      "Epoch 88, Loss: 387.47501784104566\n",
      "Epoch 89, Loss: 392.19911193847656\n",
      "Epoch 90, Loss: 387.2011906550481\n",
      "Epoch 91, Loss: 396.8956627478966\n",
      "Epoch 92, Loss: 385.0092022235577\n",
      "Epoch 93, Loss: 384.22354830228363\n",
      "Epoch 94, Loss: 383.21362891564\n",
      "Epoch 95, Loss: 382.59215252216046\n",
      "Epoch 96, Loss: 380.58914654071515\n",
      "Epoch 97, Loss: 380.54917203463043\n",
      "Epoch 98, Loss: 380.3246495173528\n",
      "Epoch 99, Loss: 380.1044921875\n",
      "Epoch 100, Loss: 385.5970224233774\n",
      "Epoch 101, Loss: 383.777106651893\n",
      "Epoch 102, Loss: 374.82697120079627\n",
      "Epoch 103, Loss: 374.2478332519531\n",
      "Epoch 104, Loss: 385.42690805288464\n",
      "Epoch 105, Loss: 381.48489966759314\n",
      "Epoch 106, Loss: 374.58937777005707\n",
      "Epoch 107, Loss: 370.8427006648137\n",
      "Epoch 108, Loss: 371.4974940373347\n",
      "Epoch 109, Loss: 368.3133216271034\n",
      "Epoch 110, Loss: 367.3474895770733\n",
      "Epoch 111, Loss: 369.58267564039966\n",
      "Epoch 112, Loss: 364.4701115534856\n",
      "Epoch 113, Loss: 370.9674001840445\n",
      "Epoch 114, Loss: 371.0108842116136\n",
      "Epoch 115, Loss: 363.8492631178636\n",
      "Epoch 116, Loss: 367.31947209284857\n",
      "Epoch 117, Loss: 360.9447080172025\n",
      "Epoch 118, Loss: 362.0002957857572\n",
      "Epoch 119, Loss: 364.56309626652643\n",
      "Epoch 120, Loss: 359.1426555926983\n",
      "Epoch 121, Loss: 368.6017362154447\n",
      "Epoch 122, Loss: 361.9916276198167\n",
      "Epoch 123, Loss: 358.5596172626202\n",
      "Epoch 124, Loss: 359.7706486628606\n",
      "Epoch 125, Loss: 359.70057326096753\n",
      "Epoch 126, Loss: 358.77533076359674\n",
      "Epoch 127, Loss: 355.48750070425183\n",
      "Epoch 128, Loss: 361.49659024752106\n",
      "Epoch 129, Loss: 356.27759845440204\n",
      "Epoch 130, Loss: 363.0579041701097\n",
      "Epoch 131, Loss: 363.03856600247894\n",
      "Epoch 132, Loss: 359.14321077786957\n",
      "Epoch 133, Loss: 358.6747659536508\n",
      "Epoch 134, Loss: 356.75193082369293\n",
      "Epoch 135, Loss: 355.83331885704627\n",
      "Epoch 136, Loss: 359.6583756666917\n",
      "Epoch 137, Loss: 354.5419358473558\n",
      "Epoch 138, Loss: 349.9148665208083\n",
      "Epoch 139, Loss: 354.7979219876803\n",
      "Epoch 140, Loss: 361.0620574951172\n",
      "Epoch 141, Loss: 348.6978196364183\n",
      "Epoch 142, Loss: 347.53777958796576\n",
      "Epoch 143, Loss: 360.5314284104567\n",
      "Epoch 144, Loss: 360.75806837815503\n",
      "Epoch 145, Loss: 348.1642045241136\n",
      "Epoch 146, Loss: 354.9571321927584\n",
      "Epoch 147, Loss: 346.87244473970856\n",
      "Epoch 148, Loss: 343.46351858285755\n",
      "Epoch 149, Loss: 342.47032282902643\n",
      "Epoch 150, Loss: 346.60702162522534\n",
      "Epoch 151, Loss: 367.57625462458685\n",
      "Epoch 152, Loss: 345.5096705510066\n",
      "Epoch 153, Loss: 340.6758375901442\n",
      "Epoch 154, Loss: 341.15496708796576\n",
      "Epoch 155, Loss: 349.2491689828726\n",
      "Epoch 156, Loss: 342.67936060978815\n",
      "Epoch 157, Loss: 339.04838209885816\n",
      "Epoch 158, Loss: 339.3797994760367\n",
      "Epoch 159, Loss: 344.3933621920072\n",
      "Epoch 160, Loss: 338.9793501633864\n",
      "Epoch 161, Loss: 342.44127772404596\n",
      "Epoch 162, Loss: 339.57080078125\n",
      "Epoch 163, Loss: 345.54124450683594\n",
      "Epoch 164, Loss: 338.8868173452524\n",
      "Epoch 165, Loss: 338.4135137704703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:05:21,248] Trial 47 finished with value: 574.4424598610008 and parameters: {'latent_dim_z1': 26, 'latent_dim_z2': 80, 'hidden_dim': 94, 'epochs': 168, 'causal_reg': 0.7593738346682153, 'learning_rate': 0.00028596767778192914}. Best is trial 25 with value: 394.9877769973252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166, Loss: 340.8893855168269\n",
      "Epoch 167, Loss: 340.1281092717097\n",
      "Epoch 168, Loss: 333.2981649545523\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:05:24,934] Trial 48 failed with parameters: {'latent_dim_z1': 25, 'latent_dim_z2': 67, 'hidden_dim': 101, 'epochs': 160, 'causal_reg': 0.6673977802528568, 'learning_rate': 0.03421595469086769} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:05:24,934] Trial 48 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 1, Loss: 3620523884.1960635\n",
      "Epoch 2, Loss: 28583.674898587742\n",
      "Epoch 3, Loss: 11421.071843073918\n",
      "Epoch 4, Loss: 7563.01381272536\n",
      "Epoch 5, Loss: 6397.240638146033\n",
      "Epoch 6, Loss: 5981.999868539663\n",
      "Epoch 7, Loss: 5635.6535081129805\n",
      "Epoch 8, Loss: 5496.075570913462\n",
      "Epoch 9, Loss: 5180.80747633714\n",
      "Epoch 10, Loss: 4931.1627854567305\n",
      "Epoch 11, Loss: 4860.503000112681\n",
      "Epoch 12, Loss: 4784.223463792067\n",
      "Epoch 13, Loss: 4628.736966646635\n",
      "Epoch 14, Loss: 4499.200702373798\n",
      "Epoch 15, Loss: 4818.959050105168\n",
      "Epoch 16, Loss: 4977.552706204928\n",
      "Epoch 17, Loss: 4370.80287522536\n",
      "Epoch 18, Loss: 4304.803560697115\n",
      "Epoch 19, Loss: 4621.5723876953125\n",
      "Epoch 20, Loss: 5666.494469275842\n",
      "Epoch 21, Loss: 6272.9069260817305\n",
      "Epoch 22, Loss: 6807.905677208533\n",
      "Epoch 23, Loss: 5906.12109375\n",
      "Epoch 24, Loss: 4790.939326359676\n",
      "Epoch 25, Loss: 4256.929668719952\n",
      "Epoch 26, Loss: 5689.471435546875\n",
      "Epoch 27, Loss: 6717.065504807692\n",
      "Epoch 28, Loss: 7004.315147986779\n",
      "Epoch 29, Loss: 6898.977332481971\n",
      "Epoch 30, Loss: 6905.1350661057695\n",
      "Epoch 31, Loss: 5685.888437124399\n",
      "Epoch 32, Loss: 5653.449415940505\n",
      "Epoch 33, Loss: 5792.313021146334\n",
      "Epoch 34, Loss: 5367.714524489183\n",
      "Epoch 35, Loss: 6749.641179011418\n",
      "Epoch 36, Loss: 8983.915283203125\n",
      "Epoch 37, Loss: 7963.807147686298\n",
      "Epoch 38, Loss: 7997.120567908654\n",
      "Epoch 39, Loss: 8796.685049203727\n",
      "Epoch 40, Loss: 8130.223247821515\n",
      "Epoch 41, Loss: 7112.3104905348555\n",
      "Epoch 42, Loss: 6137.155470628005\n",
      "Epoch 43, Loss: 4949.790316068209\n",
      "Epoch 44, Loss: 4209.942317082332\n",
      "Epoch 45, Loss: 3701.766812838041\n",
      "Epoch 46, Loss: 3265.0516592172476\n",
      "Epoch 47, Loss: 3213.3963059645434\n",
      "Epoch 48, Loss: 3804.112825833834\n",
      "Epoch 49, Loss: 4829.950134277344\n",
      "Epoch 50, Loss: 3591.0764582707334\n",
      "Epoch 51, Loss: 2814.8599571814902\n",
      "Epoch 52, Loss: 2459.417710524339\n",
      "Epoch 53, Loss: 2737.8338435246396\n",
      "Epoch 54, Loss: 4264.459547776442\n",
      "Epoch 55, Loss: 3853.562767615685\n",
      "Epoch 56, Loss: 3120.86030461238\n",
      "Epoch 57, Loss: 3741.2461453951323\n",
      "Epoch 58, Loss: 3000.4017052283652\n",
      "Epoch 59, Loss: 4010.3124389648438\n",
      "Epoch 60, Loss: 4612.608896108774\n",
      "Epoch 61, Loss: 4142.332050030048\n",
      "Epoch 62, Loss: 5189.862835223858\n",
      "Epoch 63, Loss: 5138.386798565204\n",
      "Epoch 64, Loss: 4705.435819185697\n",
      "Epoch 65, Loss: 4449.555208646334\n",
      "Epoch 66, Loss: 4152.181692270132\n",
      "Epoch 67, Loss: 3889.0799842247598\n",
      "Epoch 68, Loss: 3646.5660353440503\n",
      "Epoch 69, Loss: 3492.2593947190503\n",
      "Epoch 70, Loss: 3379.00627723107\n",
      "Epoch 71, Loss: 2901.675030048077\n",
      "Epoch 72, Loss: 2562.4217764047476\n",
      "Epoch 73, Loss: 2228.060596172626\n",
      "Epoch 74, Loss: 2079.148207444411\n",
      "Epoch 75, Loss: 1932.5604060246394\n",
      "Epoch 76, Loss: 1720.1021517240083\n",
      "Epoch 77, Loss: 1730.1727834848257\n",
      "Epoch 78, Loss: 2940.5853083683896\n",
      "Epoch 79, Loss: 6740.742384690505\n",
      "Epoch 80, Loss: 7486.4211989182695\n",
      "Epoch 81, Loss: 8902.837853064904\n",
      "Epoch 82, Loss: 10680.050274188701\n",
      "Epoch 83, Loss: 9428.742591271033\n",
      "Epoch 84, Loss: 8293.264685997596\n",
      "Epoch 85, Loss: 7111.680429311899\n",
      "Epoch 86, Loss: 6039.718674879808\n",
      "Epoch 87, Loss: 5321.757192758413\n",
      "Epoch 88, Loss: 5274.809133676382\n",
      "Epoch 89, Loss: 6461.241323617788\n",
      "Epoch 90, Loss: 6903.034433218149\n",
      "Epoch 91, Loss: 6228.911959134615\n",
      "Epoch 92, Loss: 5619.28139085036\n",
      "Epoch 93, Loss: 5177.785419170673\n",
      "Epoch 94, Loss: 4679.200584998498\n",
      "Epoch 95, Loss: 4328.847759540265\n",
      "Epoch 96, Loss: 4197.324664776142\n",
      "Epoch 97, Loss: 4083.204777644231\n",
      "Epoch 98, Loss: 4069.0153057391826\n",
      "Epoch 99, Loss: 3690.579580453726\n",
      "Epoch 100, Loss: 3418.2654325045073\n",
      "Epoch 101, Loss: 3382.6505314753604\n",
      "Epoch 102, Loss: 3072.995812049279\n",
      "Epoch 103, Loss: 2872.937481219952\n",
      "Epoch 104, Loss: 2746.176508976863\n",
      "Epoch 105, Loss: 4332.5341233473555\n",
      "Epoch 106, Loss: 9500.892033503605\n",
      "Epoch 107, Loss: 11507.391498272236\n",
      "Epoch 108, Loss: 12351.91006234976\n",
      "Epoch 109, Loss: 11126.206702599158\n",
      "Epoch 110, Loss: 8893.29764498197\n",
      "Epoch 111, Loss: 8349.15067232572\n",
      "Epoch 112, Loss: 9349.01943734976\n",
      "Epoch 113, Loss: 8273.24100435697\n",
      "Epoch 114, Loss: 6358.9299879807695\n",
      "Epoch 115, Loss: 6508.5064039963945\n",
      "Epoch 116, Loss: 6459.037663386418\n",
      "Epoch 117, Loss: 7772.468421349158\n",
      "Epoch 118, Loss: 8194.301720252404\n",
      "Epoch 119, Loss: 8074.229717548077\n",
      "Epoch 120, Loss: 7707.08940241887\n",
      "Epoch 121, Loss: 6293.803119365985\n",
      "Epoch 122, Loss: 4925.740713266226\n",
      "Epoch 123, Loss: 4221.031268780048\n",
      "Epoch 124, Loss: 3738.0911395733174\n",
      "Epoch 125, Loss: 3465.815396822416\n",
      "Epoch 126, Loss: 3181.6991107647236\n",
      "Epoch 127, Loss: 4794.464815579928\n",
      "Epoch 128, Loss: 7511.773277869592\n",
      "Epoch 129, Loss: 8535.139385516826\n",
      "Epoch 130, Loss: 8554.18905874399\n",
      "Epoch 131, Loss: 7793.23246882512\n",
      "Epoch 132, Loss: 6263.648700420673\n",
      "Epoch 133, Loss: 5281.853773850661\n",
      "Epoch 134, Loss: 5066.594876802885\n",
      "Epoch 135, Loss: 6025.552912785457\n",
      "Epoch 136, Loss: 5385.297654371995\n",
      "Epoch 137, Loss: 5518.835965670072\n",
      "Epoch 138, Loss: 6434.702430138221\n",
      "Epoch 139, Loss: 5951.607290414663\n",
      "Epoch 140, Loss: 7290.303673377404\n",
      "Epoch 141, Loss: 8208.366408128004\n",
      "Epoch 142, Loss: 7709.940194936899\n",
      "Epoch 143, Loss: 7302.585345928485\n",
      "Epoch 144, Loss: 8815.507934570312\n",
      "Epoch 145, Loss: 8559.336538461539\n",
      "Epoch 146, Loss: 7121.083082932692\n",
      "Epoch 147, Loss: 5734.057157076322\n",
      "Epoch 148, Loss: 5791.279710036058\n",
      "Epoch 149, Loss: 7528.389169546274\n",
      "Epoch 150, Loss: 7601.503943810096\n",
      "Epoch 151, Loss: 8212.337824894832\n",
      "Epoch 152, Loss: 9787.10760967548\n",
      "Epoch 153, Loss: 9895.607177734375\n",
      "Epoch 154, Loss: 8466.247962364783\n",
      "Epoch 155, Loss: 7734.601149338942\n",
      "Epoch 156, Loss: 6784.989098182092\n",
      "Epoch 157, Loss: 6325.315927358774\n",
      "Epoch 158, Loss: 8230.164992112379\n",
      "Epoch 159, Loss: 6864.132831280048\n",
      "Epoch 160, Loss: 6272.644784780649\n",
      "Epoch 161, Loss: 4705.915071927584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:05:28,790] Trial 49 finished with value: 21816243.81066472 and parameters: {'latent_dim_z1': 25, 'latent_dim_z2': 62, 'hidden_dim': 102, 'epochs': 166, 'causal_reg': 0.6922405932189291, 'learning_rate': 0.004451382555444349}. Best is trial 25 with value: 394.9877769973252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162, Loss: 3957.418212890625\n",
      "Epoch 163, Loss: 3557.2242102989785\n",
      "Epoch 164, Loss: 3314.050560584435\n",
      "Epoch 165, Loss: 3108.7557607797476\n",
      "Epoch 166, Loss: 2993.1676072340747\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:05:31,247] Trial 50 failed with parameters: {'latent_dim_z1': 20, 'latent_dim_z2': 72, 'hidden_dim': 11, 'epochs': 119, 'causal_reg': 0.766427357112803, 'learning_rate': 0.028220421941603246} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:05:31,247] Trial 50 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:05:33,965] Trial 51 failed with parameters: {'latent_dim_z1': 19, 'latent_dim_z2': 72, 'hidden_dim': 109, 'epochs': 116, 'causal_reg': 0.47574654552349604, 'learning_rate': 0.06240791638001314} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:05:33,965] Trial 51 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:05:36,830] Trial 52 failed with parameters: {'latent_dim_z1': 21, 'latent_dim_z2': 72, 'hidden_dim': 102, 'epochs': 119, 'causal_reg': 0.7782357876765997, 'learning_rate': 0.09444233083879655} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:05:36,830] Trial 52 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:05:39,403] Trial 53 failed with parameters: {'latent_dim_z1': 28, 'latent_dim_z2': 71, 'hidden_dim': 10, 'epochs': 122, 'causal_reg': 0.7927273729183255, 'learning_rate': 0.07957254667777673} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:05:39,404] Trial 53 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 1, Loss: 34472.20705472506\n",
      "Epoch 2, Loss: 2814.3186950683594\n",
      "Epoch 3, Loss: 1585.0448091947114\n",
      "Epoch 4, Loss: 1297.1637338491587\n",
      "Epoch 5, Loss: 1168.548548771785\n",
      "Epoch 6, Loss: 1098.7616506723257\n",
      "Epoch 7, Loss: 1040.3747887244592\n",
      "Epoch 8, Loss: 979.2005239633413\n",
      "Epoch 9, Loss: 924.7872032752404\n",
      "Epoch 10, Loss: 891.841566819411\n",
      "Epoch 11, Loss: 873.022705078125\n",
      "Epoch 12, Loss: 859.5193399282603\n",
      "Epoch 13, Loss: 839.5827965369591\n",
      "Epoch 14, Loss: 825.3791316105769\n",
      "Epoch 15, Loss: 810.478030865009\n",
      "Epoch 16, Loss: 794.4586205115685\n",
      "Epoch 17, Loss: 786.2783355712891\n",
      "Epoch 18, Loss: 772.1478388859675\n",
      "Epoch 19, Loss: 757.6761568509615\n",
      "Epoch 20, Loss: 750.3890322171725\n",
      "Epoch 21, Loss: 737.2077448918269\n",
      "Epoch 22, Loss: 728.6407811091497\n",
      "Epoch 23, Loss: 716.9614105224609\n",
      "Epoch 24, Loss: 698.3733943058894\n",
      "Epoch 25, Loss: 688.2238546518179\n",
      "Epoch 26, Loss: 677.035652747521\n",
      "Epoch 27, Loss: 666.3898198054387\n",
      "Epoch 28, Loss: 657.1933816763071\n",
      "Epoch 29, Loss: 647.4533327542819\n",
      "Epoch 30, Loss: 638.5058100773738\n",
      "Epoch 31, Loss: 623.656480055589\n",
      "Epoch 32, Loss: 618.0849222036509\n",
      "Epoch 33, Loss: 600.7856914813702\n",
      "Epoch 34, Loss: 590.7031437800481\n",
      "Epoch 35, Loss: 581.1375309870793\n",
      "Epoch 36, Loss: 574.7757885272687\n",
      "Epoch 37, Loss: 568.0266934908353\n",
      "Epoch 38, Loss: 551.9356548602765\n",
      "Epoch 39, Loss: 541.1966705322266\n",
      "Epoch 40, Loss: 529.7114586463341\n",
      "Epoch 41, Loss: 520.2993269700271\n",
      "Epoch 42, Loss: 511.74039048414966\n",
      "Epoch 43, Loss: 510.15147752028247\n",
      "Epoch 44, Loss: 500.897712120643\n",
      "Epoch 45, Loss: 489.91597806490387\n",
      "Epoch 46, Loss: 483.60735497107873\n",
      "Epoch 47, Loss: 473.79491013746997\n",
      "Epoch 48, Loss: 475.94447326660156\n",
      "Epoch 49, Loss: 476.40686387282153\n",
      "Epoch 50, Loss: 456.7387296236478\n",
      "Epoch 51, Loss: 451.42724609375\n",
      "Epoch 52, Loss: 448.0109417255108\n",
      "Epoch 53, Loss: 452.29584327110877\n",
      "Epoch 54, Loss: 435.0374239408053\n",
      "Epoch 55, Loss: 431.74020033616284\n",
      "Epoch 56, Loss: 427.21351623535156\n",
      "Epoch 57, Loss: 426.8869182880108\n",
      "Epoch 58, Loss: 421.94009986290564\n",
      "Epoch 59, Loss: 417.5020059438852\n",
      "Epoch 60, Loss: 425.18201035719653\n",
      "Epoch 61, Loss: 408.51559565617487\n",
      "Epoch 62, Loss: 418.7909663273738\n",
      "Epoch 63, Loss: 409.05475557767426\n",
      "Epoch 64, Loss: 415.0281266432542\n",
      "Epoch 65, Loss: 412.75070425180286\n",
      "Epoch 66, Loss: 408.79325514573316\n",
      "Epoch 67, Loss: 392.4368074857272\n",
      "Epoch 68, Loss: 388.750248835637\n",
      "Epoch 69, Loss: 392.3508112980769\n",
      "Epoch 70, Loss: 387.5207273043119\n",
      "Epoch 71, Loss: 398.2632023737981\n",
      "Epoch 72, Loss: 379.4687042236328\n",
      "Epoch 73, Loss: 374.5313028188852\n",
      "Epoch 74, Loss: 381.83726384089545\n",
      "Epoch 75, Loss: 379.11368149977466\n",
      "Epoch 76, Loss: 374.98931767390326\n",
      "Epoch 77, Loss: 372.1479985163762\n",
      "Epoch 78, Loss: 373.1004697359525\n",
      "Epoch 79, Loss: 381.6887007493239\n",
      "Epoch 80, Loss: 367.9577061579778\n",
      "Epoch 81, Loss: 362.2982647235577\n",
      "Epoch 82, Loss: 367.3489426832933\n",
      "Epoch 83, Loss: 362.12855529785156\n",
      "Epoch 84, Loss: 360.09244478665863\n",
      "Epoch 85, Loss: 356.4054741492638\n",
      "Epoch 86, Loss: 373.25900385929987\n",
      "Epoch 87, Loss: 352.12013244628906\n",
      "Epoch 88, Loss: 351.67593266413763\n",
      "Epoch 89, Loss: 358.48370478703424\n",
      "Epoch 90, Loss: 346.7161994347206\n",
      "Epoch 91, Loss: 346.90753878079926\n",
      "Epoch 92, Loss: 342.8955970177284\n",
      "Epoch 93, Loss: 361.83287400465747\n",
      "Epoch 94, Loss: 365.4977135291466\n",
      "Epoch 95, Loss: 347.96312654935394\n",
      "Epoch 96, Loss: 341.8068331204928\n",
      "Epoch 97, Loss: 340.1065345177284\n",
      "Epoch 98, Loss: 337.3247469388522\n",
      "Epoch 99, Loss: 338.88956392728363\n",
      "Epoch 100, Loss: 334.62037541316107\n",
      "Epoch 101, Loss: 332.5041832557091\n",
      "Epoch 102, Loss: 333.7157440185547\n",
      "Epoch 103, Loss: 345.431887113131\n",
      "Epoch 104, Loss: 338.2819389930138\n",
      "Epoch 105, Loss: 333.42221304086536\n",
      "Epoch 106, Loss: 328.8309854360727\n",
      "Epoch 107, Loss: 329.15978651780347\n",
      "Epoch 108, Loss: 328.54444415752704\n",
      "Epoch 109, Loss: 328.4052534836989\n",
      "Epoch 110, Loss: 324.7838885967548\n",
      "Epoch 111, Loss: 330.5574951171875\n",
      "Epoch 112, Loss: 322.4416938194862\n",
      "Epoch 113, Loss: 323.19835486778845\n",
      "Epoch 114, Loss: 335.4803138146034\n",
      "Epoch 115, Loss: 323.45589857835034\n",
      "Epoch 116, Loss: 331.00799853985126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:05:42,294] Trial 54 finished with value: 798.053297989421 and parameters: {'latent_dim_z1': 17, 'latent_dim_z2': 72, 'hidden_dim': 108, 'epochs': 124, 'causal_reg': 0.7821558643817815, 'learning_rate': 0.0006958281845495654}. Best is trial 25 with value: 394.9877769973252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117, Loss: 352.6482121394231\n",
      "Epoch 118, Loss: 327.5709486741286\n",
      "Epoch 119, Loss: 319.0286818284255\n",
      "Epoch 120, Loss: 319.11440218411957\n",
      "Epoch 121, Loss: 316.068602341872\n",
      "Epoch 122, Loss: 318.39524958683893\n",
      "Epoch 123, Loss: 316.3251554048978\n",
      "Epoch 124, Loss: 320.85851111778845\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:05:45,809] Trial 55 failed with parameters: {'latent_dim_z1': 26, 'latent_dim_z2': 54, 'hidden_dim': 83, 'epochs': 160, 'causal_reg': 0.6213355184833177, 'learning_rate': 0.07920431453079169} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:05:45,809] Trial 55 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:05:49,496] Trial 56 failed with parameters: {'latent_dim_z1': 31, 'latent_dim_z2': 53, 'hidden_dim': 140, 'epochs': 155, 'causal_reg': 0.6331348391902183, 'learning_rate': 0.09399525861398664} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:05:49,496] Trial 56 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:05:53,146] Trial 57 failed with parameters: {'latent_dim_z1': 29, 'latent_dim_z2': 52, 'hidden_dim': 136, 'epochs': 155, 'causal_reg': 0.621438134985047, 'learning_rate': 0.0951144836391906} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:05:53,146] Trial 57 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:05:56,801] Trial 58 failed with parameters: {'latent_dim_z1': 28, 'latent_dim_z2': 56, 'hidden_dim': 137, 'epochs': 155, 'causal_reg': 0.6120834887089365, 'learning_rate': 0.036100484175136595} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:05:56,802] Trial 58 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:06:00,383] Trial 59 failed with parameters: {'latent_dim_z1': 29, 'latent_dim_z2': 50, 'hidden_dim': 132, 'epochs': 157, 'causal_reg': 0.6241455562333872, 'learning_rate': 0.06550278482108816} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:06:00,383] Trial 59 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:06:03,964] Trial 60 failed with parameters: {'latent_dim_z1': 29, 'latent_dim_z2': 52, 'hidden_dim': 133, 'epochs': 157, 'causal_reg': 0.6268833698828917, 'learning_rate': 0.025035192924555732} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:06:03,964] Trial 60 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 1, Loss: 76687.00379356972\n",
      "Epoch 2, Loss: 10061.204026442309\n",
      "Epoch 3, Loss: 5451.001291128306\n",
      "Epoch 4, Loss: 3884.6263192983774\n",
      "Epoch 5, Loss: 3124.5780264047476\n",
      "Epoch 6, Loss: 2711.5060143103965\n",
      "Epoch 7, Loss: 2467.349853515625\n",
      "Epoch 8, Loss: 2316.4857318584736\n",
      "Epoch 9, Loss: 2212.4368685208833\n",
      "Epoch 10, Loss: 2135.3670161320615\n",
      "Epoch 11, Loss: 2074.144033578726\n",
      "Epoch 12, Loss: 2022.3151409442607\n",
      "Epoch 13, Loss: 1975.6478506234976\n",
      "Epoch 14, Loss: 1933.2806748610276\n",
      "Epoch 15, Loss: 1895.5391517052283\n",
      "Epoch 16, Loss: 1859.2797358586238\n",
      "Epoch 17, Loss: 1825.9946782038762\n",
      "Epoch 18, Loss: 1793.4002521221455\n",
      "Epoch 19, Loss: 1761.9676795372595\n",
      "Epoch 20, Loss: 1731.9964388333833\n",
      "Epoch 21, Loss: 1703.77927339994\n",
      "Epoch 22, Loss: 1676.7618361253005\n",
      "Epoch 23, Loss: 1650.1624427208533\n",
      "Epoch 24, Loss: 1623.1458974984976\n",
      "Epoch 25, Loss: 1598.3754624586838\n",
      "Epoch 26, Loss: 1574.0757141113281\n",
      "Epoch 27, Loss: 1549.589336688702\n",
      "Epoch 28, Loss: 1527.2042658879207\n",
      "Epoch 29, Loss: 1505.5845055213342\n",
      "Epoch 30, Loss: 1485.1669123722957\n",
      "Epoch 31, Loss: 1463.7639887883113\n",
      "Epoch 32, Loss: 1442.5992689866287\n",
      "Epoch 33, Loss: 1422.2589557354268\n",
      "Epoch 34, Loss: 1402.8426185021033\n",
      "Epoch 35, Loss: 1383.6601304274338\n",
      "Epoch 36, Loss: 1365.6219271146333\n",
      "Epoch 37, Loss: 1347.430436354417\n",
      "Epoch 38, Loss: 1329.0897662823018\n",
      "Epoch 39, Loss: 1312.0821509728064\n",
      "Epoch 40, Loss: 1294.6926856407752\n",
      "Epoch 41, Loss: 1278.2412860576924\n",
      "Epoch 42, Loss: 1261.5908062274639\n",
      "Epoch 43, Loss: 1245.8353060208833\n",
      "Epoch 44, Loss: 1229.3675818810095\n",
      "Epoch 45, Loss: 1214.5264916053186\n",
      "Epoch 46, Loss: 1198.9832904522236\n",
      "Epoch 47, Loss: 1183.69626558744\n",
      "Epoch 48, Loss: 1169.2828380878154\n",
      "Epoch 49, Loss: 1153.5923368013823\n",
      "Epoch 50, Loss: 1139.357182429387\n",
      "Epoch 51, Loss: 1126.4378709059495\n",
      "Epoch 52, Loss: 1111.4136305588943\n",
      "Epoch 53, Loss: 1098.6786381648137\n",
      "Epoch 54, Loss: 1085.6944556603064\n",
      "Epoch 55, Loss: 1072.0354755108174\n",
      "Epoch 56, Loss: 1059.910653921274\n",
      "Epoch 57, Loss: 1046.951920729417\n",
      "Epoch 58, Loss: 1032.8474707970252\n",
      "Epoch 59, Loss: 1020.0276559682993\n",
      "Epoch 60, Loss: 1007.7562502347506\n",
      "Epoch 61, Loss: 996.3051006610577\n",
      "Epoch 62, Loss: 981.8873983529897\n",
      "Epoch 63, Loss: 970.7146676870493\n",
      "Epoch 64, Loss: 959.8519474909856\n",
      "Epoch 65, Loss: 948.5695483867938\n",
      "Epoch 66, Loss: 934.5785686786359\n",
      "Epoch 67, Loss: 924.190898014949\n",
      "Epoch 68, Loss: 912.9567331167368\n",
      "Epoch 69, Loss: 903.060303908128\n",
      "Epoch 70, Loss: 890.8653294489934\n",
      "Epoch 71, Loss: 881.0464019775391\n",
      "Epoch 72, Loss: 868.7244767409104\n",
      "Epoch 73, Loss: 861.3619431715745\n",
      "Epoch 74, Loss: 849.9214101938101\n",
      "Epoch 75, Loss: 839.9034130389874\n",
      "Epoch 76, Loss: 831.2381216195913\n",
      "Epoch 77, Loss: 819.9613647460938\n",
      "Epoch 78, Loss: 814.0018627460187\n",
      "Epoch 79, Loss: 802.7115208552434\n",
      "Epoch 80, Loss: 793.1411754901593\n",
      "Epoch 81, Loss: 787.361814058744\n",
      "Epoch 82, Loss: 775.6382458026593\n",
      "Epoch 83, Loss: 769.5709428053635\n",
      "Epoch 84, Loss: 761.369381244366\n",
      "Epoch 85, Loss: 753.5631796029897\n",
      "Epoch 86, Loss: 746.9792046180138\n",
      "Epoch 87, Loss: 737.5547802264874\n",
      "Epoch 88, Loss: 730.110859797551\n",
      "Epoch 89, Loss: 721.1814340444712\n",
      "Epoch 90, Loss: 719.484623835637\n",
      "Epoch 91, Loss: 711.9686830960787\n",
      "Epoch 92, Loss: 707.2243969257062\n",
      "Epoch 93, Loss: 700.4357147216797\n",
      "Epoch 94, Loss: 695.4413557786208\n",
      "Epoch 95, Loss: 689.0611701378456\n",
      "Epoch 96, Loss: 683.3988107534556\n",
      "Epoch 97, Loss: 674.1348243126503\n",
      "Epoch 98, Loss: 668.4378098707932\n",
      "Epoch 99, Loss: 664.5693124624399\n",
      "Epoch 100, Loss: 657.3948939396785\n",
      "Epoch 101, Loss: 651.4783360407903\n",
      "Epoch 102, Loss: 646.2913759671725\n",
      "Epoch 103, Loss: 646.0988370455228\n",
      "Epoch 104, Loss: 635.9634528526893\n",
      "Epoch 105, Loss: 635.9821131779597\n",
      "Epoch 106, Loss: 629.6119114802434\n",
      "Epoch 107, Loss: 628.8485236534706\n",
      "Epoch 108, Loss: 619.2846491887019\n",
      "Epoch 109, Loss: 619.9825146014874\n",
      "Epoch 110, Loss: 617.3558725210337\n",
      "Epoch 111, Loss: 606.121317936824\n",
      "Epoch 112, Loss: 602.1643688495343\n",
      "Epoch 113, Loss: 595.2565248929537\n",
      "Epoch 114, Loss: 596.9540064885066\n",
      "Epoch 115, Loss: 594.241942185622\n",
      "Epoch 116, Loss: 582.9900935246394\n",
      "Epoch 117, Loss: 581.0005422738882\n",
      "Epoch 118, Loss: 581.1637514554537\n",
      "Epoch 119, Loss: 573.1116297795222\n",
      "Epoch 120, Loss: 567.2791208120493\n",
      "Epoch 121, Loss: 565.4570676363431\n",
      "Epoch 122, Loss: 572.4970480111929\n",
      "Epoch 123, Loss: 561.4401608980619\n",
      "Epoch 124, Loss: 561.9324728158804\n",
      "Epoch 125, Loss: 552.8080796461838\n",
      "Epoch 126, Loss: 550.3769108698918\n",
      "Epoch 127, Loss: 547.1208167442909\n",
      "Epoch 128, Loss: 541.1184516319862\n",
      "Epoch 129, Loss: 539.0719850980319\n",
      "Epoch 130, Loss: 536.3865591195913\n",
      "Epoch 131, Loss: 534.4430565467247\n",
      "Epoch 132, Loss: 531.5296513484075\n",
      "Epoch 133, Loss: 530.1290752704327\n",
      "Epoch 134, Loss: 528.7178532527043\n",
      "Epoch 135, Loss: 524.3833078237681\n",
      "Epoch 136, Loss: 519.9002486008865\n",
      "Epoch 137, Loss: 514.6205315223107\n",
      "Epoch 138, Loss: 512.4869736891526\n",
      "Epoch 139, Loss: 510.04581275353064\n",
      "Epoch 140, Loss: 507.20494783841644\n",
      "Epoch 141, Loss: 503.37951308030347\n",
      "Epoch 142, Loss: 504.4094766470102\n",
      "Epoch 143, Loss: 500.57154494065503\n",
      "Epoch 144, Loss: 497.0245924729567\n",
      "Epoch 145, Loss: 493.76553931603064\n",
      "Epoch 146, Loss: 494.7889075646034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:06:07,354] Trial 61 finished with value: 1631.0300022053862 and parameters: {'latent_dim_z1': 29, 'latent_dim_z2': 53, 'hidden_dim': 126, 'epochs': 148, 'causal_reg': 0.6080584999266357, 'learning_rate': 0.00017852191813020598}. Best is trial 25 with value: 394.9877769973252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147, Loss: 493.2055922288161\n",
      "Epoch 148, Loss: 493.330079298753\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:06:11,098] Trial 62 failed with parameters: {'latent_dim_z1': 19, 'latent_dim_z2': 68, 'hidden_dim': 74, 'epochs': 162, 'causal_reg': 0.4810335333925314, 'learning_rate': 0.0767368738918697} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:06:11,098] Trial 62 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:06:14,536] Trial 63 failed with parameters: {'latent_dim_z1': 19, 'latent_dim_z2': 70, 'hidden_dim': 16, 'epochs': 165, 'causal_reg': 0.5049456016462245, 'learning_rate': 0.04985601693302994} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:06:14,537] Trial 63 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165, Loss: nan\n",
      "Epoch 1, Loss: 99967.61831430289\n",
      "Epoch 2, Loss: 32101.79285606971\n",
      "Epoch 3, Loss: 22632.544677734375\n",
      "Epoch 4, Loss: 17913.681603064902\n",
      "Epoch 5, Loss: 14802.864426832934\n",
      "Epoch 6, Loss: 12803.13534780649\n",
      "Epoch 7, Loss: 11267.370981069711\n",
      "Epoch 8, Loss: 10027.256732647236\n",
      "Epoch 9, Loss: 9006.452110877404\n",
      "Epoch 10, Loss: 8143.096970778245\n",
      "Epoch 11, Loss: 7409.192861703726\n",
      "Epoch 12, Loss: 6777.564321664663\n",
      "Epoch 13, Loss: 6230.063936673678\n",
      "Epoch 14, Loss: 5751.8853102463945\n",
      "Epoch 15, Loss: 5331.416752741887\n",
      "Epoch 16, Loss: 4960.5010657677285\n",
      "Epoch 17, Loss: 4632.118619478666\n",
      "Epoch 18, Loss: 4339.6788987379805\n",
      "Epoch 19, Loss: 4078.7401169996997\n",
      "Epoch 20, Loss: 3844.9803232046274\n",
      "Epoch 21, Loss: 3634.4216590294473\n",
      "Epoch 22, Loss: 3443.5726881760816\n",
      "Epoch 23, Loss: 3270.9139122596152\n",
      "Epoch 24, Loss: 3114.369408240685\n",
      "Epoch 25, Loss: 2970.337139423077\n",
      "Epoch 26, Loss: 2838.7196514423076\n",
      "Epoch 27, Loss: 2718.320744441106\n",
      "Epoch 28, Loss: 2607.144033578726\n",
      "Epoch 29, Loss: 2505.4684025691104\n",
      "Epoch 30, Loss: 2411.1133986253003\n",
      "Epoch 31, Loss: 2323.7446969839243\n",
      "Epoch 32, Loss: 2242.597592867338\n",
      "Epoch 33, Loss: 2167.5582909217246\n",
      "Epoch 34, Loss: 2097.453549898588\n",
      "Epoch 35, Loss: 2032.167238675631\n",
      "Epoch 36, Loss: 1971.498028094952\n",
      "Epoch 37, Loss: 1914.7713177020732\n",
      "Epoch 38, Loss: 1860.9498220590444\n",
      "Epoch 39, Loss: 1810.8189274714543\n",
      "Epoch 40, Loss: 1763.375284048227\n",
      "Epoch 41, Loss: 1719.558837890625\n",
      "Epoch 42, Loss: 1677.7965393066406\n",
      "Epoch 43, Loss: 1638.5853083683894\n",
      "Epoch 44, Loss: 1601.574498103215\n",
      "Epoch 45, Loss: 1566.809074988732\n",
      "Epoch 46, Loss: 1533.436483529898\n",
      "Epoch 47, Loss: 1502.548093355619\n",
      "Epoch 48, Loss: 1472.9921123798076\n",
      "Epoch 49, Loss: 1445.4756493201623\n",
      "Epoch 50, Loss: 1419.15819138747\n",
      "Epoch 51, Loss: 1394.3911907489482\n",
      "Epoch 52, Loss: 1370.8721923828125\n",
      "Epoch 53, Loss: 1348.526839036208\n",
      "Epoch 54, Loss: 1327.33106642503\n",
      "Epoch 55, Loss: 1307.1831524188701\n",
      "Epoch 56, Loss: 1288.1593604454627\n",
      "Epoch 57, Loss: 1269.7965486966646\n",
      "Epoch 58, Loss: 1252.2106158916768\n",
      "Epoch 59, Loss: 1235.748774601863\n",
      "Epoch 60, Loss: 1219.5233553372896\n",
      "Epoch 61, Loss: 1204.2053433931792\n",
      "Epoch 62, Loss: 1189.767845740685\n",
      "Epoch 63, Loss: 1175.509249173678\n",
      "Epoch 64, Loss: 1162.1317091721755\n",
      "Epoch 65, Loss: 1148.7917785644531\n",
      "Epoch 66, Loss: 1136.3014033390925\n",
      "Epoch 67, Loss: 1123.8904888446514\n",
      "Epoch 68, Loss: 1112.6446016751802\n",
      "Epoch 69, Loss: 1101.164567213792\n",
      "Epoch 70, Loss: 1089.8866800161509\n",
      "Epoch 71, Loss: 1079.3071030836838\n",
      "Epoch 72, Loss: 1068.7050206111028\n",
      "Epoch 73, Loss: 1058.3770282451924\n",
      "Epoch 74, Loss: 1048.3085338885967\n",
      "Epoch 75, Loss: 1039.010500394381\n",
      "Epoch 76, Loss: 1029.4373568021333\n",
      "Epoch 77, Loss: 1020.668689434345\n",
      "Epoch 78, Loss: 1011.801513671875\n",
      "Epoch 79, Loss: 1003.7832829402043\n",
      "Epoch 80, Loss: 995.2442450890175\n",
      "Epoch 81, Loss: 986.9358320969802\n",
      "Epoch 82, Loss: 979.07057424692\n",
      "Epoch 83, Loss: 971.2354149451622\n",
      "Epoch 84, Loss: 963.4395235501803\n",
      "Epoch 85, Loss: 955.8784966102013\n",
      "Epoch 86, Loss: 948.8096489539513\n",
      "Epoch 87, Loss: 941.766841008113\n",
      "Epoch 88, Loss: 934.7357623760516\n",
      "Epoch 89, Loss: 927.5743607741135\n",
      "Epoch 90, Loss: 920.8419025127704\n",
      "Epoch 91, Loss: 913.9861426720253\n",
      "Epoch 92, Loss: 907.1133622389573\n",
      "Epoch 93, Loss: 900.8705127422626\n",
      "Epoch 94, Loss: 894.2554180438702\n",
      "Epoch 95, Loss: 887.8973564734825\n",
      "Epoch 96, Loss: 881.9015150803787\n",
      "Epoch 97, Loss: 875.6696296105018\n",
      "Epoch 98, Loss: 869.6537651648888\n",
      "Epoch 99, Loss: 863.7918982872596\n",
      "Epoch 100, Loss: 857.8991335355319\n",
      "Epoch 101, Loss: 852.0563377967247\n",
      "Epoch 102, Loss: 846.0205981914813\n",
      "Epoch 103, Loss: 840.3820507342999\n",
      "Epoch 104, Loss: 834.5604588435247\n",
      "Epoch 105, Loss: 829.0983722393329\n",
      "Epoch 106, Loss: 823.6662879356971\n",
      "Epoch 107, Loss: 818.3064927321213\n",
      "Epoch 108, Loss: 813.015644953801\n",
      "Epoch 109, Loss: 807.7791806734525\n",
      "Epoch 110, Loss: 802.5560067983774\n",
      "Epoch 111, Loss: 797.5475909893329\n",
      "Epoch 112, Loss: 792.7899592472957\n",
      "Epoch 113, Loss: 787.8177783672626\n",
      "Epoch 114, Loss: 783.0568436842698\n",
      "Epoch 115, Loss: 777.8951592078575\n",
      "Epoch 116, Loss: 773.3990994966947\n",
      "Epoch 117, Loss: 768.4678438626803\n",
      "Epoch 118, Loss: 763.8061793400691\n",
      "Epoch 119, Loss: 759.3815178504357\n",
      "Epoch 120, Loss: 754.9047464224009\n",
      "Epoch 121, Loss: 750.5019789475662\n",
      "Epoch 122, Loss: 745.864481999324\n",
      "Epoch 123, Loss: 741.59545311561\n",
      "Epoch 124, Loss: 737.3633845402644\n",
      "Epoch 125, Loss: 733.0267380934495\n",
      "Epoch 126, Loss: 728.5446507380559\n",
      "Epoch 127, Loss: 724.8802490234375\n",
      "Epoch 128, Loss: 720.6497427133413\n",
      "Epoch 129, Loss: 716.6171370286208\n",
      "Epoch 130, Loss: 712.7396991436298\n",
      "Epoch 131, Loss: 708.7141218919021\n",
      "Epoch 132, Loss: 705.1641787015475\n",
      "Epoch 133, Loss: 701.3216681847206\n",
      "Epoch 134, Loss: 697.8604654165415\n",
      "Epoch 135, Loss: 693.9599280724159\n",
      "Epoch 136, Loss: 690.7367248535156\n",
      "Epoch 137, Loss: 686.676743727464\n",
      "Epoch 138, Loss: 683.3543865497296\n",
      "Epoch 139, Loss: 679.7762944148137\n",
      "Epoch 140, Loss: 676.2262596717247\n",
      "Epoch 141, Loss: 672.8312213604266\n",
      "Epoch 142, Loss: 669.3695373535156\n",
      "Epoch 143, Loss: 666.0839374248798\n",
      "Epoch 144, Loss: 662.9458571213943\n",
      "Epoch 145, Loss: 659.4875323955829\n",
      "Epoch 146, Loss: 656.2771489070012\n",
      "Epoch 147, Loss: 653.2223545954778\n",
      "Epoch 148, Loss: 650.1494070199819\n",
      "Epoch 149, Loss: 647.1165630634015\n",
      "Epoch 150, Loss: 643.9478841928335\n",
      "Epoch 151, Loss: 641.4656970684345\n",
      "Epoch 152, Loss: 638.4446598933293\n",
      "Epoch 153, Loss: 635.2736217792218\n",
      "Epoch 154, Loss: 632.4441234882062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:06:18,230] Trial 64 finished with value: 2863.2197827837826 and parameters: {'latent_dim_z1': 18, 'latent_dim_z2': 68, 'hidden_dim': 76, 'epochs': 158, 'causal_reg': 0.4469500051885637, 'learning_rate': 1.5334329237259362e-05}. Best is trial 25 with value: 394.9877769973252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155, Loss: 629.7024031419021\n",
      "Epoch 156, Loss: 627.1334005502554\n",
      "Epoch 157, Loss: 624.2271810678335\n",
      "Epoch 158, Loss: 621.3067814753606\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:06:20,874] Trial 65 failed with parameters: {'latent_dim_z1': 47, 'latent_dim_z2': 75, 'hidden_dim': 15, 'epochs': 118, 'causal_reg': 0.6511263794839803, 'learning_rate': 0.025630190532782305} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:06:20,875] Trial 65 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:06:23,612] Trial 66 failed with parameters: {'latent_dim_z1': 48, 'latent_dim_z2': 75, 'hidden_dim': 23, 'epochs': 122, 'causal_reg': 0.6500640163708192, 'learning_rate': 0.0662764662790206} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:06:23,612] Trial 66 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:06:26,382] Trial 67 failed with parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 75, 'hidden_dim': 133, 'epochs': 119, 'causal_reg': 0.643421236739622, 'learning_rate': 0.08559530151942385} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:06:26,383] Trial 67 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 1, Loss: 71435.33122370794\n",
      "Epoch 2, Loss: 4734.1037034254805\n",
      "Epoch 3, Loss: 2986.4915161132812\n",
      "Epoch 4, Loss: 2486.6510784442607\n",
      "Epoch 5, Loss: 2218.9522798978364\n",
      "Epoch 6, Loss: 2034.064201941857\n",
      "Epoch 7, Loss: 1892.439697265625\n",
      "Epoch 8, Loss: 1762.7859027569111\n",
      "Epoch 9, Loss: 1638.549309363732\n",
      "Epoch 10, Loss: 1518.814213679387\n",
      "Epoch 11, Loss: 1427.0926490196814\n",
      "Epoch 12, Loss: 1344.2420912522537\n",
      "Epoch 13, Loss: 1283.0175311748799\n",
      "Epoch 14, Loss: 1240.1839998685396\n",
      "Epoch 15, Loss: 1215.7103717510518\n",
      "Epoch 16, Loss: 1199.6752788837139\n",
      "Epoch 17, Loss: 1186.3831176757812\n",
      "Epoch 18, Loss: 1153.1609731820913\n",
      "Epoch 19, Loss: 1129.895235501803\n",
      "Epoch 20, Loss: 1108.13278902494\n",
      "Epoch 21, Loss: 1084.3441044734075\n",
      "Epoch 22, Loss: 1068.6488001896785\n",
      "Epoch 23, Loss: 1053.5824326735276\n",
      "Epoch 24, Loss: 1037.650350717398\n",
      "Epoch 25, Loss: 1034.0969907320464\n",
      "Epoch 26, Loss: 1005.2401275634766\n",
      "Epoch 27, Loss: 990.5831521841196\n",
      "Epoch 28, Loss: 978.2930391751803\n",
      "Epoch 29, Loss: 968.2741922231821\n",
      "Epoch 30, Loss: 955.0002101017878\n",
      "Epoch 31, Loss: 916.6858602670522\n",
      "Epoch 32, Loss: 902.2924417349009\n",
      "Epoch 33, Loss: 904.7411276010366\n",
      "Epoch 34, Loss: 875.2821209247296\n",
      "Epoch 35, Loss: 861.7479412372296\n",
      "Epoch 36, Loss: 845.8533642108624\n",
      "Epoch 37, Loss: 829.6664827786959\n",
      "Epoch 38, Loss: 816.0890502929688\n",
      "Epoch 39, Loss: 791.8559347299429\n",
      "Epoch 40, Loss: 778.1179692195012\n",
      "Epoch 41, Loss: 790.2895167424128\n",
      "Epoch 42, Loss: 752.3517397367037\n",
      "Epoch 43, Loss: 760.5568530742938\n",
      "Epoch 44, Loss: 744.9093275803787\n",
      "Epoch 45, Loss: 742.7452345628005\n",
      "Epoch 46, Loss: 710.2499319223257\n",
      "Epoch 47, Loss: 700.7893301156851\n",
      "Epoch 48, Loss: 690.6607043926532\n",
      "Epoch 49, Loss: 671.6890904353215\n",
      "Epoch 50, Loss: 672.5988417405349\n",
      "Epoch 51, Loss: 665.5294142503005\n",
      "Epoch 52, Loss: 677.54466012808\n",
      "Epoch 53, Loss: 654.1081730769231\n",
      "Epoch 54, Loss: 656.7857231727013\n",
      "Epoch 55, Loss: 644.5099369929387\n",
      "Epoch 56, Loss: 630.3712709867037\n",
      "Epoch 57, Loss: 630.3749870887169\n",
      "Epoch 58, Loss: 602.6224588247446\n",
      "Epoch 59, Loss: 591.3812420184796\n",
      "Epoch 60, Loss: 602.175795335036\n",
      "Epoch 61, Loss: 627.742683997521\n",
      "Epoch 62, Loss: 606.650147658128\n",
      "Epoch 63, Loss: 596.6259894737831\n",
      "Epoch 64, Loss: 564.3009397066556\n",
      "Epoch 65, Loss: 608.2695242074819\n",
      "Epoch 66, Loss: 551.1493412898137\n",
      "Epoch 67, Loss: 551.5237062894381\n",
      "Epoch 68, Loss: 548.9293529803937\n",
      "Epoch 69, Loss: 546.2017587515024\n",
      "Epoch 70, Loss: 527.4276721660907\n",
      "Epoch 71, Loss: 592.6222346379207\n",
      "Epoch 72, Loss: 573.8406548133263\n",
      "Epoch 73, Loss: 606.1335237943209\n",
      "Epoch 74, Loss: 634.5113208477313\n",
      "Epoch 75, Loss: 683.2286470853365\n",
      "Epoch 76, Loss: 573.446523813101\n",
      "Epoch 77, Loss: 521.8144836425781\n",
      "Epoch 78, Loss: 512.7042858417218\n",
      "Epoch 79, Loss: 532.1733527550331\n",
      "Epoch 80, Loss: 511.72129587026745\n",
      "Epoch 81, Loss: 501.14379061185394\n",
      "Epoch 82, Loss: 511.5072995699369\n",
      "Epoch 83, Loss: 529.8923022930438\n",
      "Epoch 84, Loss: 500.5784583458534\n",
      "Epoch 85, Loss: 502.33499028132513\n",
      "Epoch 86, Loss: 542.3558126596304\n",
      "Epoch 87, Loss: 469.11095839280347\n",
      "Epoch 88, Loss: 477.11732482910156\n",
      "Epoch 89, Loss: 475.04107900766223\n",
      "Epoch 90, Loss: 471.94423029972955\n",
      "Epoch 91, Loss: 465.8440469595102\n",
      "Epoch 92, Loss: 467.92257925180286\n",
      "Epoch 93, Loss: 494.5576477050781\n",
      "Epoch 94, Loss: 533.3951286902794\n",
      "Epoch 95, Loss: 531.7291435828575\n",
      "Epoch 96, Loss: 454.70042536808893\n",
      "Epoch 97, Loss: 437.77459129920373\n",
      "Epoch 98, Loss: 443.66623159555286\n",
      "Epoch 99, Loss: 470.4283259465144\n",
      "Epoch 100, Loss: 449.6517721322867\n",
      "Epoch 101, Loss: 472.177740243765\n",
      "Epoch 102, Loss: 447.5346996600811\n",
      "Epoch 103, Loss: 459.9239501953125\n",
      "Epoch 104, Loss: 461.3808863713191\n",
      "Epoch 105, Loss: 487.803955078125\n",
      "Epoch 106, Loss: 449.7255295973558\n",
      "Epoch 107, Loss: 420.7409937931941\n",
      "Epoch 108, Loss: 435.6838578444261\n",
      "Epoch 109, Loss: 428.6830526498648\n",
      "Epoch 110, Loss: 422.4262038010817\n",
      "Epoch 111, Loss: 413.59983825683594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:06:29,044] Trial 68 finished with value: 1410.5881205214362 and parameters: {'latent_dim_z1': 46, 'latent_dim_z2': 80, 'hidden_dim': 11, 'epochs': 118, 'causal_reg': 0.6566485652577374, 'learning_rate': 0.0012990379422080267}. Best is trial 25 with value: 394.9877769973252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112, Loss: 401.907963679387\n",
      "Epoch 113, Loss: 412.86931786170373\n",
      "Epoch 114, Loss: 414.1163717416617\n",
      "Epoch 115, Loss: 541.197279710036\n",
      "Epoch 116, Loss: 469.401861337515\n",
      "Epoch 117, Loss: 412.5397456242488\n",
      "Epoch 118, Loss: 410.00430649977466\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:06:33,583] Trial 69 failed with parameters: {'latent_dim_z1': 35, 'latent_dim_z2': 77, 'hidden_dim': 177, 'epochs': 179, 'causal_reg': 0.8348541909207284, 'learning_rate': 0.0339510513354699} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:06:33,583] Trial 69 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:06:38,253] Trial 70 failed with parameters: {'latent_dim_z1': 33, 'latent_dim_z2': 77, 'hidden_dim': 181, 'epochs': 181, 'causal_reg': 0.816922218590952, 'learning_rate': 0.03944588573498707} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:06:38,254] Trial 70 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:06:42,938] Trial 71 failed with parameters: {'latent_dim_z1': 31, 'latent_dim_z2': 75, 'hidden_dim': 172, 'epochs': 187, 'causal_reg': 0.7933425466944368, 'learning_rate': 0.04241456687744819} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:06:42,938] Trial 71 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:06:47,702] Trial 72 failed with parameters: {'latent_dim_z1': 36, 'latent_dim_z2': 75, 'hidden_dim': 181, 'epochs': 182, 'causal_reg': 0.8357570172171735, 'learning_rate': 0.04808535857549386} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:06:47,702] Trial 72 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:06:52,402] Trial 73 failed with parameters: {'latent_dim_z1': 32, 'latent_dim_z2': 75, 'hidden_dim': 181, 'epochs': 180, 'causal_reg': 0.8060481789620323, 'learning_rate': 0.0768592564324107} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:06:52,403] Trial 73 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 1, Loss: 50650.19497445913\n",
      "Epoch 2, Loss: 5062.9360116811895\n",
      "Epoch 3, Loss: 2430.5930692232573\n",
      "Epoch 4, Loss: 1863.0018662672776\n",
      "Epoch 5, Loss: 1637.802013690655\n",
      "Epoch 6, Loss: 1512.0406423715444\n",
      "Epoch 7, Loss: 1420.7533428485576\n",
      "Epoch 8, Loss: 1337.7766911433293\n",
      "Epoch 9, Loss: 1269.7531198354868\n",
      "Epoch 10, Loss: 1207.4934457632212\n",
      "Epoch 11, Loss: 1153.3193864088792\n",
      "Epoch 12, Loss: 1106.972430889423\n",
      "Epoch 13, Loss: 1067.9543245755708\n",
      "Epoch 14, Loss: 1033.6723010723408\n",
      "Epoch 15, Loss: 1001.1055203951322\n",
      "Epoch 16, Loss: 971.7418752817007\n",
      "Epoch 17, Loss: 948.7574861966647\n",
      "Epoch 18, Loss: 923.8470752422626\n",
      "Epoch 19, Loss: 902.1487544133113\n",
      "Epoch 20, Loss: 886.7684220534104\n",
      "Epoch 21, Loss: 865.9316148024338\n",
      "Epoch 22, Loss: 850.4831296480619\n",
      "Epoch 23, Loss: 838.9869337815505\n",
      "Epoch 24, Loss: 826.0487048809345\n",
      "Epoch 25, Loss: 807.8629502516526\n",
      "Epoch 26, Loss: 791.0352712777944\n",
      "Epoch 27, Loss: 780.4191389817458\n",
      "Epoch 28, Loss: 766.7855846698468\n",
      "Epoch 29, Loss: 761.6575892521785\n",
      "Epoch 30, Loss: 741.990705049955\n",
      "Epoch 31, Loss: 723.7676332913912\n",
      "Epoch 32, Loss: 713.353994516226\n",
      "Epoch 33, Loss: 704.8094400259165\n",
      "Epoch 34, Loss: 694.847171490009\n",
      "Epoch 35, Loss: 684.1713972825271\n",
      "Epoch 36, Loss: 672.345937875601\n",
      "Epoch 37, Loss: 657.4959282508263\n",
      "Epoch 38, Loss: 655.8882716252253\n",
      "Epoch 39, Loss: 636.6158388577975\n",
      "Epoch 40, Loss: 633.0666257418119\n",
      "Epoch 41, Loss: 626.2236691988431\n",
      "Epoch 42, Loss: 614.5859527587891\n",
      "Epoch 43, Loss: 623.4620091364934\n",
      "Epoch 44, Loss: 611.5219350961538\n",
      "Epoch 45, Loss: 607.3706829364484\n",
      "Epoch 46, Loss: 597.0298004150391\n",
      "Epoch 47, Loss: 602.9243844839243\n",
      "Epoch 48, Loss: 587.03052344689\n",
      "Epoch 49, Loss: 582.9623460036057\n",
      "Epoch 50, Loss: 573.4710423396184\n",
      "Epoch 51, Loss: 562.0643733097957\n",
      "Epoch 52, Loss: 570.34422654372\n",
      "Epoch 53, Loss: 549.3526141826923\n",
      "Epoch 54, Loss: 548.9652616060697\n",
      "Epoch 55, Loss: 543.7446934626653\n",
      "Epoch 56, Loss: 536.9754439133865\n",
      "Epoch 57, Loss: 532.843019925631\n",
      "Epoch 58, Loss: 530.5154430682843\n",
      "Epoch 59, Loss: 530.4296382023738\n",
      "Epoch 60, Loss: 525.5496497521034\n",
      "Epoch 61, Loss: 519.8114119309646\n",
      "Epoch 62, Loss: 543.4792140080378\n",
      "Epoch 63, Loss: 528.4261861947866\n",
      "Epoch 64, Loss: 506.4730177659255\n",
      "Epoch 65, Loss: 505.8590064415565\n",
      "Epoch 66, Loss: 506.1117847149189\n",
      "Epoch 67, Loss: 494.0380612886869\n",
      "Epoch 68, Loss: 509.92540682279144\n",
      "Epoch 69, Loss: 500.69400728665863\n",
      "Epoch 70, Loss: 489.6149456317608\n",
      "Epoch 71, Loss: 482.5143913855919\n",
      "Epoch 72, Loss: 492.6060321514423\n",
      "Epoch 73, Loss: 491.77110760028546\n",
      "Epoch 74, Loss: 498.1157930814303\n",
      "Epoch 75, Loss: 483.11905611478363\n",
      "Epoch 76, Loss: 470.4154369647686\n",
      "Epoch 77, Loss: 475.2895812988281\n",
      "Epoch 78, Loss: 472.270503117488\n",
      "Epoch 79, Loss: 478.1364217904898\n",
      "Epoch 80, Loss: 467.33435880220856\n",
      "Epoch 81, Loss: 466.1511359581581\n",
      "Epoch 82, Loss: 476.0472729022686\n",
      "Epoch 83, Loss: 458.9503561166617\n",
      "Epoch 84, Loss: 458.9815145639273\n",
      "Epoch 85, Loss: 450.3874077430138\n",
      "Epoch 86, Loss: 450.1812215951773\n",
      "Epoch 87, Loss: 451.93750821627106\n",
      "Epoch 88, Loss: 445.8656792273888\n",
      "Epoch 89, Loss: 444.8594571627103\n",
      "Epoch 90, Loss: 439.610593355619\n",
      "Epoch 91, Loss: 441.6436450664814\n",
      "Epoch 92, Loss: 442.6193542480469\n",
      "Epoch 93, Loss: 435.13058589054987\n",
      "Epoch 94, Loss: 446.80178011380707\n",
      "Epoch 95, Loss: 432.6326657808744\n",
      "Epoch 96, Loss: 429.44871638371393\n",
      "Epoch 97, Loss: 436.2211338923528\n",
      "Epoch 98, Loss: 426.19096022385816\n",
      "Epoch 99, Loss: 428.5329061654898\n",
      "Epoch 100, Loss: 421.5157482440655\n",
      "Epoch 101, Loss: 418.862302339994\n",
      "Epoch 102, Loss: 418.85220219538763\n",
      "Epoch 103, Loss: 418.7656003511869\n",
      "Epoch 104, Loss: 414.797370323768\n",
      "Epoch 105, Loss: 422.43369821401745\n",
      "Epoch 106, Loss: 419.13235356257513\n",
      "Epoch 107, Loss: 410.91412353515625\n",
      "Epoch 108, Loss: 412.64410283015326\n",
      "Epoch 109, Loss: 418.21705862192005\n",
      "Epoch 110, Loss: 415.1075826791617\n",
      "Epoch 111, Loss: 407.3367485633263\n",
      "Epoch 112, Loss: 415.2042881892278\n",
      "Epoch 113, Loss: 402.9836989182692\n",
      "Epoch 114, Loss: 404.93197866586536\n",
      "Epoch 115, Loss: 398.33541400615985\n",
      "Epoch 116, Loss: 398.5801473764273\n",
      "Epoch 117, Loss: 400.10885854867786\n",
      "Epoch 118, Loss: 407.43505859375\n",
      "Epoch 119, Loss: 406.48360501802887\n",
      "Epoch 120, Loss: 392.90955998347357\n",
      "Epoch 121, Loss: 415.4524747408353\n",
      "Epoch 122, Loss: 422.1981729360727\n",
      "Epoch 123, Loss: 397.7014735295222\n",
      "Epoch 124, Loss: 396.3446068396935\n",
      "Epoch 125, Loss: 391.96151850773737\n",
      "Epoch 126, Loss: 396.3723719670222\n",
      "Epoch 127, Loss: 391.1047163743239\n",
      "Epoch 128, Loss: 390.0732879638672\n",
      "Epoch 129, Loss: 392.5124699519231\n",
      "Epoch 130, Loss: 391.53565392127405\n",
      "Epoch 131, Loss: 382.7458179180439\n",
      "Epoch 132, Loss: 384.25237332857574\n",
      "Epoch 133, Loss: 383.4250699556791\n",
      "Epoch 134, Loss: 378.19308119553784\n",
      "Epoch 135, Loss: 378.0003086970403\n",
      "Epoch 136, Loss: 377.5003274770883\n",
      "Epoch 137, Loss: 386.62222642164966\n",
      "Epoch 138, Loss: 390.5378617506761\n",
      "Epoch 139, Loss: 397.8792008620042\n",
      "Epoch 140, Loss: 384.0869398850661\n",
      "Epoch 141, Loss: 377.6884249173678\n",
      "Epoch 142, Loss: 373.3269512469952\n",
      "Epoch 143, Loss: 370.47244732196515\n",
      "Epoch 144, Loss: 373.9532717191256\n",
      "Epoch 145, Loss: 368.22227478027344\n",
      "Epoch 146, Loss: 377.3208324725811\n",
      "Epoch 147, Loss: 374.39524841308594\n",
      "Epoch 148, Loss: 368.32494060809796\n",
      "Epoch 149, Loss: 367.0373300405649\n",
      "Epoch 150, Loss: 364.7903477595403\n",
      "Epoch 151, Loss: 361.977543757512\n",
      "Epoch 152, Loss: 366.49232600285455\n",
      "Epoch 153, Loss: 364.51054030198316\n",
      "Epoch 154, Loss: 371.0041973407452\n",
      "Epoch 155, Loss: 369.8251166710487\n",
      "Epoch 156, Loss: 363.10137352576623\n",
      "Epoch 157, Loss: 372.2277327317458\n",
      "Epoch 158, Loss: 357.5476567195012\n",
      "Epoch 159, Loss: 377.49515826885516\n",
      "Epoch 160, Loss: 365.1730640117939\n",
      "Epoch 161, Loss: 368.76060837965747\n",
      "Epoch 162, Loss: 363.1220433161809\n",
      "Epoch 163, Loss: 367.5806051400992\n",
      "Epoch 164, Loss: 366.16918593186597\n",
      "Epoch 165, Loss: 355.33104412372296\n",
      "Epoch 166, Loss: 354.73724599984973\n",
      "Epoch 167, Loss: 358.7979043813852\n",
      "Epoch 168, Loss: 351.3906038724459\n",
      "Epoch 169, Loss: 354.9941711425781\n",
      "Epoch 170, Loss: 351.39055105356067\n",
      "Epoch 171, Loss: 351.41918358436\n",
      "Epoch 172, Loss: 355.9933248666617\n",
      "Epoch 173, Loss: 357.99021207369293\n",
      "Epoch 174, Loss: 354.7688422569862\n",
      "Epoch 175, Loss: 351.52536128117487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:06:57,318] Trial 74 finished with value: 855.4617144331971 and parameters: {'latent_dim_z1': 30, 'latent_dim_z2': 75, 'hidden_dim': 183, 'epochs': 180, 'causal_reg': 0.8367797690868298, 'learning_rate': 0.00040437408925856786}. Best is trial 25 with value: 394.9877769973252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176, Loss: 349.5474618765024\n",
      "Epoch 177, Loss: 358.3776550292969\n",
      "Epoch 178, Loss: 352.1509258563702\n",
      "Epoch 179, Loss: 344.50870220477765\n",
      "Epoch 180, Loss: 343.8517878605769\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:07:02,003] Trial 75 failed with parameters: {'latent_dim_z1': 39, 'latent_dim_z2': 76, 'hidden_dim': 138, 'epochs': 183, 'causal_reg': 0.8281527385161818, 'learning_rate': 0.06363602404014541} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:07:02,004] Trial 75 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:07:06,664] Trial 76 failed with parameters: {'latent_dim_z1': 38, 'latent_dim_z2': 76, 'hidden_dim': 138, 'epochs': 179, 'causal_reg': 0.8068369408915179, 'learning_rate': 0.029570513385269323} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:07:06,665] Trial 76 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 1, Loss: 15050.38108943059\n",
      "Epoch 2, Loss: 3404.4725060096152\n",
      "Epoch 3, Loss: 2173.1016399676982\n",
      "Epoch 4, Loss: 1833.198014479417\n",
      "Epoch 5, Loss: 1629.408933199369\n",
      "Epoch 6, Loss: 1468.3196434607873\n",
      "Epoch 7, Loss: 1322.9255887545073\n",
      "Epoch 8, Loss: 1200.244387113131\n",
      "Epoch 9, Loss: 1100.284660926232\n",
      "Epoch 10, Loss: 1024.6029616135818\n",
      "Epoch 11, Loss: 959.5297346848708\n",
      "Epoch 12, Loss: 893.4649857741135\n",
      "Epoch 13, Loss: 843.7674654447115\n",
      "Epoch 14, Loss: 792.3288703331581\n",
      "Epoch 15, Loss: 758.6352386474609\n",
      "Epoch 16, Loss: 731.3734095646785\n",
      "Epoch 17, Loss: 703.2864908071665\n",
      "Epoch 18, Loss: 689.9034482515775\n",
      "Epoch 19, Loss: 675.7045241135818\n",
      "Epoch 20, Loss: 663.3776468130259\n",
      "Epoch 21, Loss: 655.1881009615385\n",
      "Epoch 22, Loss: 639.0289870042068\n",
      "Epoch 23, Loss: 630.3502713716947\n",
      "Epoch 24, Loss: 622.0033428485577\n",
      "Epoch 25, Loss: 613.6045860877404\n",
      "Epoch 26, Loss: 607.2165339543269\n",
      "Epoch 27, Loss: 604.6414478008563\n",
      "Epoch 28, Loss: 591.9950749323918\n",
      "Epoch 29, Loss: 596.1356083796575\n",
      "Epoch 30, Loss: 581.0538975642278\n",
      "Epoch 31, Loss: 576.2895402174729\n",
      "Epoch 32, Loss: 577.6732835036057\n",
      "Epoch 33, Loss: 568.4003589336688\n",
      "Epoch 34, Loss: 572.850339449369\n",
      "Epoch 35, Loss: 568.087646484375\n",
      "Epoch 36, Loss: 575.1991776686448\n",
      "Epoch 37, Loss: 551.8406149057241\n",
      "Epoch 38, Loss: 559.7004523644081\n",
      "Epoch 39, Loss: 554.1991248497596\n",
      "Epoch 40, Loss: 541.0185816838191\n",
      "Epoch 41, Loss: 546.6188753568209\n",
      "Epoch 42, Loss: 533.2443777231069\n",
      "Epoch 43, Loss: 553.1971846360427\n",
      "Epoch 44, Loss: 555.2108576847957\n",
      "Epoch 45, Loss: 523.781017596905\n",
      "Epoch 46, Loss: 527.6038724459135\n",
      "Epoch 47, Loss: 522.8802842360276\n",
      "Epoch 48, Loss: 514.9646700345553\n",
      "Epoch 49, Loss: 519.4824559138372\n",
      "Epoch 50, Loss: 516.3031780536359\n",
      "Epoch 51, Loss: 519.5219515286959\n",
      "Epoch 52, Loss: 509.17598783052887\n",
      "Epoch 53, Loss: 510.03258925217847\n",
      "Epoch 54, Loss: 524.7285027137169\n",
      "Epoch 55, Loss: 502.96671705979566\n",
      "Epoch 56, Loss: 504.86805842472955\n",
      "Epoch 57, Loss: 508.34859642615686\n",
      "Epoch 58, Loss: 493.14960421048676\n",
      "Epoch 59, Loss: 496.1821500338041\n",
      "Epoch 60, Loss: 511.5441072904147\n",
      "Epoch 61, Loss: 489.5473820612981\n",
      "Epoch 62, Loss: 486.8712627704327\n",
      "Epoch 63, Loss: 486.58821575458234\n",
      "Epoch 64, Loss: 481.2342658409706\n",
      "Epoch 65, Loss: 494.88152371920074\n",
      "Epoch 66, Loss: 479.42635052020734\n",
      "Epoch 67, Loss: 484.7327869121845\n",
      "Epoch 68, Loss: 482.80035987267127\n",
      "Epoch 69, Loss: 478.2648432804988\n",
      "Epoch 70, Loss: 471.43997779259314\n",
      "Epoch 71, Loss: 481.393309373122\n",
      "Epoch 72, Loss: 471.05716294508716\n",
      "Epoch 73, Loss: 484.05367337740387\n",
      "Epoch 74, Loss: 469.90032372107873\n",
      "Epoch 75, Loss: 470.320069533128\n",
      "Epoch 76, Loss: 470.75439100999097\n",
      "Epoch 77, Loss: 463.93273573655347\n",
      "Epoch 78, Loss: 482.22684772198016\n",
      "Epoch 79, Loss: 480.78672790527344\n",
      "Epoch 80, Loss: 467.40396822415863\n",
      "Epoch 81, Loss: 464.1042867807242\n",
      "Epoch 82, Loss: 459.13636075533356\n",
      "Epoch 83, Loss: 450.1121321458083\n",
      "Epoch 84, Loss: 453.837155855619\n",
      "Epoch 85, Loss: 451.04354506272534\n",
      "Epoch 86, Loss: 455.3751009427584\n",
      "Epoch 87, Loss: 456.492430466872\n",
      "Epoch 88, Loss: 450.5975916935847\n",
      "Epoch 89, Loss: 447.32963796762317\n",
      "Epoch 90, Loss: 454.53362332857574\n",
      "Epoch 91, Loss: 447.48667907714844\n",
      "Epoch 92, Loss: 443.74566415640027\n",
      "Epoch 93, Loss: 437.45630821814905\n",
      "Epoch 94, Loss: 440.42615685096155\n",
      "Epoch 95, Loss: 452.31322185809796\n",
      "Epoch 96, Loss: 459.413328904372\n",
      "Epoch 97, Loss: 456.58146197979266\n",
      "Epoch 98, Loss: 446.1962503286508\n",
      "Epoch 99, Loss: 436.21651634803186\n",
      "Epoch 100, Loss: 436.8691641000601\n",
      "Epoch 101, Loss: 436.6960695706881\n",
      "Epoch 102, Loss: 435.9196272629958\n",
      "Epoch 103, Loss: 434.88735844538763\n",
      "Epoch 104, Loss: 432.99483548677887\n",
      "Epoch 105, Loss: 429.34923964280347\n",
      "Epoch 106, Loss: 427.17687636155347\n",
      "Epoch 107, Loss: 422.59448829064\n",
      "Epoch 108, Loss: 434.0939483642578\n",
      "Epoch 109, Loss: 435.23502643291766\n",
      "Epoch 110, Loss: 439.5256582406851\n",
      "Epoch 111, Loss: 434.5691680908203\n",
      "Epoch 112, Loss: 425.90900949331433\n",
      "Epoch 113, Loss: 420.65626525878906\n",
      "Epoch 114, Loss: 424.96164057804987\n",
      "Epoch 115, Loss: 418.35041574331433\n",
      "Epoch 116, Loss: 419.25089674729566\n",
      "Epoch 117, Loss: 416.9520463209886\n",
      "Epoch 118, Loss: 429.80934377817005\n",
      "Epoch 119, Loss: 425.3880615234375\n",
      "Epoch 120, Loss: 414.67628009502704\n",
      "Epoch 121, Loss: 416.1087317833534\n",
      "Epoch 122, Loss: 423.3004866379958\n",
      "Epoch 123, Loss: 408.0898132324219\n",
      "Epoch 124, Loss: 416.33108990009015\n",
      "Epoch 125, Loss: 409.0564727783203\n",
      "Epoch 126, Loss: 421.1092928372897\n",
      "Epoch 127, Loss: 416.67013784555286\n",
      "Epoch 128, Loss: 413.6998678354117\n",
      "Epoch 129, Loss: 416.38363295335034\n",
      "Epoch 130, Loss: 410.52786959134613\n",
      "Epoch 131, Loss: 415.474366408128\n",
      "Epoch 132, Loss: 411.50604834923377\n",
      "Epoch 133, Loss: 406.66585129957934\n",
      "Epoch 134, Loss: 408.42228229229266\n",
      "Epoch 135, Loss: 407.8086442213792\n",
      "Epoch 136, Loss: 404.9911099947416\n",
      "Epoch 137, Loss: 404.8397451547476\n",
      "Epoch 138, Loss: 426.96800114558295\n",
      "Epoch 139, Loss: 396.07652400090143\n",
      "Epoch 140, Loss: 422.2259721022386\n",
      "Epoch 141, Loss: 417.4989483173077\n",
      "Epoch 142, Loss: 416.3560579740084\n",
      "Epoch 143, Loss: 414.0041938194862\n",
      "Epoch 144, Loss: 400.3961627666767\n",
      "Epoch 145, Loss: 390.342528123122\n",
      "Epoch 146, Loss: 396.42822265625\n",
      "Epoch 147, Loss: 398.08082697941705\n",
      "Epoch 148, Loss: 388.477783203125\n",
      "Epoch 149, Loss: 395.192866398738\n",
      "Epoch 150, Loss: 392.92052048903247\n",
      "Epoch 151, Loss: 387.5921889085036\n",
      "Epoch 152, Loss: 387.29543832632214\n",
      "Epoch 153, Loss: 387.8120328463041\n",
      "Epoch 154, Loss: 394.6651400052584\n",
      "Epoch 155, Loss: 392.55951045109674\n",
      "Epoch 156, Loss: 387.6518155611478\n",
      "Epoch 157, Loss: 384.18769601675183\n",
      "Epoch 158, Loss: 387.6851677527794\n",
      "Epoch 159, Loss: 383.1066248967097\n",
      "Epoch 160, Loss: 383.27852689302887\n",
      "Epoch 161, Loss: 385.3979257436899\n",
      "Epoch 162, Loss: 379.4177809495192\n",
      "Epoch 163, Loss: 388.8382122333233\n",
      "Epoch 164, Loss: 380.1378561166617\n",
      "Epoch 165, Loss: 376.0768338716947\n",
      "Epoch 166, Loss: 382.5164290208083\n",
      "Epoch 167, Loss: 376.8339655949519\n",
      "Epoch 168, Loss: 378.7767075758714\n",
      "Epoch 169, Loss: 390.2552255483774\n",
      "Epoch 170, Loss: 381.9690457857572\n",
      "Epoch 171, Loss: 379.2807804987981\n",
      "Epoch 172, Loss: 371.9026348407452\n",
      "Epoch 173, Loss: 370.4494182880108\n",
      "Epoch 174, Loss: 378.37394949106067\n",
      "Epoch 175, Loss: 377.27274733323316\n",
      "Epoch 176, Loss: 381.2472933255709\n",
      "Epoch 177, Loss: 378.4258798452524\n",
      "Epoch 178, Loss: 385.27171090932995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:07:11,282] Trial 77 finished with value: 613.7222195300432 and parameters: {'latent_dim_z1': 39, 'latent_dim_z2': 75, 'hidden_dim': 140, 'epochs': 182, 'causal_reg': 0.8099548964991125, 'learning_rate': 0.0001798747086619019}. Best is trial 25 with value: 394.9877769973252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179, Loss: 376.59584280160755\n",
      "Epoch 180, Loss: 367.7830822284405\n",
      "Epoch 181, Loss: 366.01591726449817\n",
      "Epoch 182, Loss: 368.34422302246094\n",
      "Epoch 1, Loss: 12901.25708242563\n",
      "Epoch 2, Loss: 1209.23044527494\n",
      "Epoch 3, Loss: 1159.6569378192607\n",
      "Epoch 4, Loss: 1145.278052696815\n",
      "Epoch 5, Loss: 1130.5072796161357\n",
      "Epoch 6, Loss: 1115.859867976262\n",
      "Epoch 7, Loss: 1099.6358501727764\n",
      "Epoch 8, Loss: 1078.3928715632512\n",
      "Epoch 9, Loss: 1045.258772629958\n",
      "Epoch 10, Loss: 998.49268751878\n",
      "Epoch 11, Loss: 918.3445364145132\n",
      "Epoch 12, Loss: 824.1676471416766\n",
      "Epoch 13, Loss: 732.6063232421875\n",
      "Epoch 14, Loss: 662.7615321232722\n",
      "Epoch 15, Loss: 603.4821882981521\n",
      "Epoch 16, Loss: 586.7762087308444\n",
      "Epoch 17, Loss: 532.6792907714844\n",
      "Epoch 18, Loss: 510.31175231933594\n",
      "Epoch 19, Loss: 492.6893251859225\n",
      "Epoch 20, Loss: 475.1847452016977\n",
      "Epoch 21, Loss: 485.1497274545523\n",
      "Epoch 22, Loss: 476.2875847449669\n",
      "Epoch 23, Loss: 454.2111616868239\n",
      "Epoch 24, Loss: 452.66221618652344\n",
      "Epoch 25, Loss: 441.93126032902643\n",
      "Epoch 26, Loss: 437.77022963303784\n",
      "Epoch 27, Loss: 429.12804236778845\n",
      "Epoch 28, Loss: 432.8167161207933\n",
      "Epoch 29, Loss: 427.52798814039966\n",
      "Epoch 30, Loss: 432.3104189359225\n",
      "Epoch 31, Loss: 413.84378403883716\n",
      "Epoch 32, Loss: 418.6644334059495\n",
      "Epoch 33, Loss: 419.72963069035455\n",
      "Epoch 34, Loss: 406.390626173753\n",
      "Epoch 35, Loss: 406.4510040283203\n",
      "Epoch 36, Loss: 406.43644949106067\n",
      "Epoch 37, Loss: 418.75126882699817\n",
      "Epoch 38, Loss: 408.55287287785455\n",
      "Epoch 39, Loss: 417.7092073880709\n",
      "Epoch 40, Loss: 394.8399963378906\n",
      "Epoch 41, Loss: 408.08545743502106\n",
      "Epoch 42, Loss: 402.76966505784253\n",
      "Epoch 43, Loss: 388.6078643798828\n",
      "Epoch 44, Loss: 386.1740969144381\n",
      "Epoch 45, Loss: 388.62473942683295\n",
      "Epoch 46, Loss: 384.7263617882362\n",
      "Epoch 47, Loss: 385.0146014873798\n",
      "Epoch 48, Loss: 391.69379248985877\n",
      "Epoch 49, Loss: 388.138184767503\n",
      "Epoch 50, Loss: 376.88699692946216\n",
      "Epoch 51, Loss: 379.59890629695013\n",
      "Epoch 52, Loss: 383.1430276724008\n",
      "Epoch 53, Loss: 377.31534282977765\n",
      "Epoch 54, Loss: 371.9547353891226\n",
      "Epoch 55, Loss: 372.4640162541316\n",
      "Epoch 56, Loss: 373.21288123497595\n",
      "Epoch 57, Loss: 374.9801224928636\n",
      "Epoch 58, Loss: 373.3815559974083\n",
      "Epoch 59, Loss: 373.4329810509315\n",
      "Epoch 60, Loss: 382.30803152231067\n",
      "Epoch 61, Loss: 364.9894268329327\n",
      "Epoch 62, Loss: 402.5152071439303\n",
      "Epoch 63, Loss: 376.7968303973858\n",
      "Epoch 64, Loss: 358.2450150709886\n",
      "Epoch 65, Loss: 363.39281874436597\n",
      "Epoch 66, Loss: 355.8284407395583\n",
      "Epoch 67, Loss: 360.3228736290565\n",
      "Epoch 68, Loss: 358.3476092998798\n",
      "Epoch 69, Loss: 358.2129927415114\n",
      "Epoch 70, Loss: 354.09779240534857\n",
      "Epoch 71, Loss: 353.3628821739784\n",
      "Epoch 72, Loss: 369.0986157930814\n",
      "Epoch 73, Loss: 351.95264610877405\n",
      "Epoch 74, Loss: 350.49992018479566\n",
      "Epoch 75, Loss: 350.85821415827826\n",
      "Epoch 76, Loss: 349.80976046048676\n",
      "Epoch 77, Loss: 349.4686713585487\n",
      "Epoch 78, Loss: 367.2950181227464\n",
      "Epoch 79, Loss: 366.00894047663763\n",
      "Epoch 80, Loss: 349.43242117074817\n",
      "Epoch 81, Loss: 342.443359375\n",
      "Epoch 82, Loss: 352.5502683199369\n",
      "Epoch 83, Loss: 359.3709153395433\n",
      "Epoch 84, Loss: 342.7880295973558\n",
      "Epoch 85, Loss: 340.5849844125601\n",
      "Epoch 86, Loss: 338.8258749154898\n",
      "Epoch 87, Loss: 337.44456951434796\n",
      "Epoch 88, Loss: 338.49939727783203\n",
      "Epoch 89, Loss: 337.57442180926984\n",
      "Epoch 90, Loss: 337.7384866567758\n",
      "Epoch 91, Loss: 359.5999521108774\n",
      "Epoch 92, Loss: 352.8116901104267\n",
      "Epoch 93, Loss: 335.71797825739935\n",
      "Epoch 94, Loss: 335.45280691293567\n",
      "Epoch 95, Loss: 332.62282503568207\n",
      "Epoch 96, Loss: 337.60835735614484\n",
      "Epoch 97, Loss: 330.4620173527644\n",
      "Epoch 98, Loss: 329.92027458777795\n",
      "Epoch 99, Loss: 332.5650892991286\n",
      "Epoch 100, Loss: 331.2804084190956\n",
      "Epoch 101, Loss: 348.91956740159253\n",
      "Epoch 102, Loss: 358.4082489013672\n",
      "Epoch 103, Loss: 330.87904592660755\n",
      "Epoch 104, Loss: 333.6635437011719\n",
      "Epoch 105, Loss: 325.3504439133864\n",
      "Epoch 106, Loss: 334.5037384033203\n",
      "Epoch 107, Loss: 362.7950369027945\n",
      "Epoch 108, Loss: 331.52845059908356\n",
      "Epoch 109, Loss: 331.8426642784706\n",
      "Epoch 110, Loss: 322.0140650822566\n",
      "Epoch 111, Loss: 327.3844064565805\n",
      "Epoch 112, Loss: 339.56153752253607\n",
      "Epoch 113, Loss: 329.25029813326324\n",
      "Epoch 114, Loss: 331.4832775409405\n",
      "Epoch 115, Loss: 320.60974473219653\n",
      "Epoch 116, Loss: 344.0119875394381\n",
      "Epoch 117, Loss: 325.5372067964994\n",
      "Epoch 118, Loss: 319.01451580341046\n",
      "Epoch 119, Loss: 318.83348670372595\n",
      "Epoch 120, Loss: 330.03162442720856\n",
      "Epoch 121, Loss: 323.67497488168567\n",
      "Epoch 122, Loss: 316.495856651893\n",
      "Epoch 123, Loss: 315.4568763146034\n",
      "Epoch 124, Loss: 320.36325894869293\n",
      "Epoch 125, Loss: 342.6096578744742\n",
      "Epoch 126, Loss: 321.1319591815655\n",
      "Epoch 127, Loss: 321.02974407489484\n",
      "Epoch 128, Loss: 311.51323876014123\n",
      "Epoch 129, Loss: 318.75787470890924\n",
      "Epoch 130, Loss: 311.4015626173753\n",
      "Epoch 131, Loss: 318.19769991361176\n",
      "Epoch 132, Loss: 325.2022892878606\n",
      "Epoch 133, Loss: 317.75478891225964\n",
      "Epoch 134, Loss: 317.0641585129958\n",
      "Epoch 135, Loss: 318.45392432579627\n",
      "Epoch 136, Loss: 327.5535689133864\n",
      "Epoch 137, Loss: 344.3004385141226\n",
      "Epoch 138, Loss: 311.3468850942758\n",
      "Epoch 139, Loss: 325.6164093017578\n",
      "Epoch 140, Loss: 307.338987497183\n",
      "Epoch 141, Loss: 311.8613580557016\n",
      "Epoch 142, Loss: 307.30871934157153\n",
      "Epoch 143, Loss: 309.04079378568207\n",
      "Epoch 144, Loss: 332.121205843412\n",
      "Epoch 145, Loss: 306.4904022216797\n",
      "Epoch 146, Loss: 310.0467587984525\n",
      "Epoch 147, Loss: 302.8153721736028\n",
      "Epoch 148, Loss: 303.6209041888897\n",
      "Epoch 149, Loss: 318.8264465332031\n",
      "Epoch 150, Loss: 313.86966411884015\n",
      "Epoch 151, Loss: 303.61998690091644\n",
      "Epoch 152, Loss: 301.78976146991437\n",
      "Epoch 153, Loss: 301.26036189152643\n",
      "Epoch 154, Loss: 309.48833524263824\n",
      "Epoch 155, Loss: 311.11002056415265\n",
      "Epoch 156, Loss: 301.6778722909781\n",
      "Epoch 157, Loss: 298.06886995755707\n",
      "Epoch 158, Loss: 302.195556640625\n",
      "Epoch 159, Loss: 301.00626197228064\n",
      "Epoch 160, Loss: 306.6357809213492\n",
      "Epoch 161, Loss: 310.20074345515326\n",
      "Epoch 162, Loss: 297.1142578125\n",
      "Epoch 163, Loss: 298.8494215745192\n",
      "Epoch 164, Loss: 310.2614980844351\n",
      "Epoch 165, Loss: 321.89586228590747\n",
      "Epoch 166, Loss: 304.099115224985\n",
      "Epoch 167, Loss: 298.7328579242413\n",
      "Epoch 168, Loss: 294.67091604379505\n",
      "Epoch 169, Loss: 297.698735163762\n",
      "Epoch 170, Loss: 298.76759221003607\n",
      "Epoch 171, Loss: 297.43770188551684\n",
      "Epoch 172, Loss: 294.6417735173152\n",
      "Epoch 173, Loss: 298.7533686711238\n",
      "Epoch 174, Loss: 289.96746591421277\n",
      "Epoch 175, Loss: 291.5177436241737\n",
      "Epoch 176, Loss: 303.1991201547476\n",
      "Epoch 177, Loss: 301.05781613863434\n",
      "Epoch 178, Loss: 295.21845773550183\n",
      "Epoch 179, Loss: 289.79542365440955\n",
      "Epoch 180, Loss: 288.8773480928861\n",
      "Epoch 181, Loss: 289.9346923828125\n",
      "Epoch 182, Loss: 292.0110109769381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:07:15,496] Trial 78 finished with value: 465.70360503642854 and parameters: {'latent_dim_z1': 23, 'latent_dim_z2': 64, 'hidden_dim': 86, 'epochs': 185, 'causal_reg': 0.8838628376231994, 'learning_rate': 0.000576754070891603}. Best is trial 25 with value: 394.9877769973252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183, Loss: 295.70072408822864\n",
      "Epoch 184, Loss: 287.85635962853064\n",
      "Epoch 185, Loss: 290.51626645601715\n",
      "Epoch 1, Loss: 11748909894376.678\n",
      "Epoch 2, Loss: 30996.623591496395\n",
      "Epoch 3, Loss: 10801.505145733174\n",
      "Epoch 4, Loss: 6741.441077599158\n",
      "Epoch 5, Loss: 5378.298954890324\n",
      "Epoch 6, Loss: 4758.441410945012\n",
      "Epoch 7, Loss: 4649.666630671574\n",
      "Epoch 8, Loss: 4537.335946890024\n",
      "Epoch 9, Loss: 4404.870797964243\n",
      "Epoch 10, Loss: 4325.1030038686895\n",
      "Epoch 11, Loss: 4512.018127441406\n",
      "Epoch 12, Loss: 4801.2712965745195\n",
      "Epoch 13, Loss: 4448.0805898813105\n",
      "Epoch 14, Loss: 4644.2820387620195\n",
      "Epoch 15, Loss: 4733.849862905649\n",
      "Epoch 16, Loss: 3843.3632296048677\n",
      "Epoch 17, Loss: 4491.698002741887\n",
      "Epoch 18, Loss: 5020.5213951697715\n",
      "Epoch 19, Loss: 4406.851459209735\n",
      "Epoch 20, Loss: 3632.625248835637\n",
      "Epoch 21, Loss: 3291.85602745643\n",
      "Epoch 22, Loss: 3196.0572415865386\n",
      "Epoch 23, Loss: 3915.7458073542666\n",
      "Epoch 24, Loss: 3582.678466796875\n",
      "Epoch 25, Loss: 4314.71337890625\n",
      "Epoch 26, Loss: 3778.5133291391226\n",
      "Epoch 27, Loss: 3101.672640286959\n",
      "Epoch 28, Loss: 3149.325420673077\n",
      "Epoch 29, Loss: 3424.959716796875\n",
      "Epoch 30, Loss: 3436.462665264423\n",
      "Epoch 31, Loss: 3202.492013784555\n",
      "Epoch 32, Loss: 3993.094529371995\n",
      "Epoch 33, Loss: 4223.306438739483\n",
      "Epoch 34, Loss: 3484.1463669996997\n",
      "Epoch 35, Loss: 4688.213388296274\n",
      "Epoch 36, Loss: 4389.958425668569\n",
      "Epoch 37, Loss: 3757.984121469351\n",
      "Epoch 38, Loss: 3173.7376568134014\n",
      "Epoch 39, Loss: 2851.3783827561597\n",
      "Epoch 40, Loss: 4283.053875262921\n",
      "Epoch 41, Loss: 4646.548335148738\n",
      "Epoch 42, Loss: 4251.718045748197\n",
      "Epoch 43, Loss: 3767.4117619441104\n",
      "Epoch 44, Loss: 3716.8440974308896\n",
      "Epoch 45, Loss: 3701.428987943209\n",
      "Epoch 46, Loss: 2336.553480881911\n",
      "Epoch 47, Loss: 2203.8348506047178\n",
      "Epoch 48, Loss: 2689.3902634840747\n",
      "Epoch 49, Loss: 5025.939584585337\n",
      "Epoch 50, Loss: 6413.143141526442\n",
      "Epoch 51, Loss: 7551.040555513822\n",
      "Epoch 52, Loss: 7531.987783578726\n",
      "Epoch 53, Loss: 7789.3085374098555\n",
      "Epoch 54, Loss: 6563.93940617488\n",
      "Epoch 55, Loss: 6340.275522085337\n",
      "Epoch 56, Loss: 5533.141855093149\n",
      "Epoch 57, Loss: 4787.413616473858\n",
      "Epoch 58, Loss: 4227.881474421574\n",
      "Epoch 59, Loss: 4375.316439115084\n",
      "Epoch 60, Loss: 5105.899263822115\n",
      "Epoch 61, Loss: 3982.3784836989184\n",
      "Epoch 62, Loss: 3602.7091674804688\n",
      "Epoch 63, Loss: 3758.8986065204326\n",
      "Epoch 64, Loss: 4140.24697641226\n",
      "Epoch 65, Loss: 4303.084651066707\n",
      "Epoch 66, Loss: 4776.343632624699\n",
      "Epoch 67, Loss: 3210.612295297476\n",
      "Epoch 68, Loss: 2695.1021024263823\n",
      "Epoch 69, Loss: 2984.6273193359375\n",
      "Epoch 70, Loss: 3539.735382080078\n",
      "Epoch 71, Loss: 3730.1533954326924\n",
      "Epoch 72, Loss: 5014.902493990385\n",
      "Epoch 73, Loss: 5053.908405010517\n",
      "Epoch 74, Loss: 6433.618389423077\n",
      "Epoch 75, Loss: 5969.667837289663\n",
      "Epoch 76, Loss: 5091.4911545973555\n",
      "Epoch 77, Loss: 7313.518723707933\n",
      "Epoch 78, Loss: 9752.292949969951\n",
      "Epoch 79, Loss: 10219.815251277043\n",
      "Epoch 80, Loss: 9740.218149038461\n",
      "Epoch 81, Loss: 9173.982384314904\n",
      "Epoch 82, Loss: 8789.336923452523\n",
      "Epoch 83, Loss: 8737.136606069711\n",
      "Epoch 84, Loss: 8613.887582632211\n",
      "Epoch 85, Loss: 8610.454608623799\n",
      "Epoch 86, Loss: 8614.246243990385\n",
      "Epoch 87, Loss: 8392.718674879809\n",
      "Epoch 88, Loss: 8377.387084960938\n",
      "Epoch 89, Loss: 8421.61016376202\n",
      "Epoch 90, Loss: 8484.644578200121\n",
      "Epoch 91, Loss: 9733.026489257812\n",
      "Epoch 92, Loss: 8587.903254582332\n",
      "Epoch 93, Loss: 8234.089374248799\n",
      "Epoch 94, Loss: 7877.221529447115\n",
      "Epoch 95, Loss: 8016.755990835337\n",
      "Epoch 96, Loss: 8745.399216871996\n",
      "Epoch 97, Loss: 9103.536564753605\n",
      "Epoch 98, Loss: 9325.688100961539\n",
      "Epoch 99, Loss: 8180.91659780649\n",
      "Epoch 100, Loss: 8422.679217998799\n",
      "Epoch 101, Loss: 11491.882568359375\n",
      "Epoch 102, Loss: 13302.996150090145\n",
      "Epoch 103, Loss: 12548.523005558895\n",
      "Epoch 104, Loss: 11700.340341421273\n",
      "Epoch 105, Loss: 10573.77706204928\n",
      "Epoch 106, Loss: 9579.981755183293\n",
      "Epoch 107, Loss: 9112.529681865986\n",
      "Epoch 108, Loss: 8792.413236177885\n",
      "Epoch 109, Loss: 8478.412550706129\n",
      "Epoch 110, Loss: 8044.959482046274\n",
      "Epoch 111, Loss: 8053.693697415865\n",
      "Epoch 112, Loss: 8233.58642578125\n",
      "Epoch 113, Loss: 8552.098013070914\n",
      "Epoch 114, Loss: 12362.744027944711\n",
      "Epoch 115, Loss: 11360.856689453125\n",
      "Epoch 116, Loss: 9253.720562274639\n",
      "Epoch 117, Loss: 7960.810791015625\n",
      "Epoch 118, Loss: 7266.244591346154\n",
      "Epoch 119, Loss: 6881.58598445012\n",
      "Epoch 120, Loss: 9511.620661808895\n",
      "Epoch 121, Loss: 12149.717548076924\n",
      "Epoch 122, Loss: 11832.88837139423\n",
      "Epoch 123, Loss: 11279.487051156852\n",
      "Epoch 124, Loss: 10472.519681490385\n",
      "Epoch 125, Loss: 9852.768817608174\n",
      "Epoch 126, Loss: 9422.998197115385\n",
      "Epoch 127, Loss: 10025.293785682092\n",
      "Epoch 128, Loss: 10651.485182542066\n",
      "Epoch 129, Loss: 9210.261737530049\n",
      "Epoch 130, Loss: 7291.932945838342\n",
      "Epoch 131, Loss: 6742.817608173077\n",
      "Epoch 132, Loss: 7663.983088566707\n",
      "Epoch 133, Loss: 7524.240271935096\n",
      "Epoch 134, Loss: 7735.204571063702\n",
      "Epoch 135, Loss: 5972.591965895433\n",
      "Epoch 136, Loss: 5849.4773512620195\n",
      "Epoch 137, Loss: 6171.93594125601\n",
      "Epoch 138, Loss: 6310.969519981971\n",
      "Epoch 139, Loss: 4162.228576660156\n",
      "Epoch 140, Loss: 3001.977238581731\n",
      "Epoch 141, Loss: 2816.521484375\n",
      "Epoch 142, Loss: 4381.529541015625\n",
      "Epoch 143, Loss: 3682.732928936298\n",
      "Epoch 144, Loss: 2740.610605093149\n",
      "Epoch 145, Loss: 1929.9480802095854\n",
      "Epoch 146, Loss: 1879.0384192833533\n",
      "Epoch 147, Loss: 1815.1856102576623\n",
      "Epoch 148, Loss: 1665.5849163348857\n",
      "Epoch 149, Loss: 1675.5678382286658\n",
      "Epoch 150, Loss: 1731.393834040715\n",
      "Epoch 151, Loss: 2094.6141733022837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:07:19,261] Trial 79 finished with value: 73892521475.10944 and parameters: {'latent_dim_z1': 25, 'latent_dim_z2': 64, 'hidden_dim': 97, 'epochs': 159, 'causal_reg': 0.9133188535247851, 'learning_rate': 0.004127402180210469}. Best is trial 25 with value: 394.9877769973252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152, Loss: 3196.7362436147837\n",
      "Epoch 153, Loss: 6577.418691781851\n",
      "Epoch 154, Loss: 9742.38057767428\n",
      "Epoch 155, Loss: 10000.016761192908\n",
      "Epoch 156, Loss: 10687.171508789062\n",
      "Epoch 157, Loss: 9653.677865835336\n",
      "Epoch 158, Loss: 8752.69073016827\n",
      "Epoch 159, Loss: 8073.193246694712\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:07:22,071] Trial 80 failed with parameters: {'latent_dim_z1': 15, 'latent_dim_z2': 53, 'hidden_dim': 83, 'epochs': 129, 'causal_reg': 0.46594279763210983, 'learning_rate': 0.02226518578015783} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:07:22,071] Trial 80 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:07:25,015] Trial 81 failed with parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 52, 'hidden_dim': 81, 'epochs': 130, 'causal_reg': 0.47404774229244623, 'learning_rate': 0.04653873847853571} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:07:25,016] Trial 81 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:07:27,874] Trial 82 failed with parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 52, 'hidden_dim': 126, 'epochs': 127, 'causal_reg': 0.9966582083788799, 'learning_rate': 0.08999204641227607} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:07:27,874] Trial 82 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:07:30,716] Trial 83 failed with parameters: {'latent_dim_z1': 11, 'latent_dim_z2': 55, 'hidden_dim': 84, 'epochs': 130, 'causal_reg': 0.7403379143771562, 'learning_rate': 0.027892143625410246} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:07:30,716] Trial 83 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:07:33,559] Trial 84 failed with parameters: {'latent_dim_z1': 15, 'latent_dim_z2': 55, 'hidden_dim': 85, 'epochs': 127, 'causal_reg': 0.9943062521796009, 'learning_rate': 0.02292838557652696} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:07:33,559] Trial 84 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:07:36,400] Trial 85 failed with parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 54, 'hidden_dim': 122, 'epochs': 128, 'causal_reg': 0.7322379714744531, 'learning_rate': 0.0958306175492756} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:07:36,400] Trial 85 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:07:39,163] Trial 86 failed with parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 53, 'hidden_dim': 88, 'epochs': 126, 'causal_reg': 0.7409475923826998, 'learning_rate': 0.059027711229028015} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:07:39,163] Trial 86 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:07:43,266] Trial 87 failed with parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 53, 'hidden_dim': 84, 'epochs': 190, 'causal_reg': 0.47823744603786084, 'learning_rate': 0.02220889495682137} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:07:43,266] Trial 87 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:07:46,210] Trial 88 failed with parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 52, 'hidden_dim': 86, 'epochs': 131, 'causal_reg': 0.7480968970101811, 'learning_rate': 0.07145171135990719} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:07:46,211] Trial 88 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 1, Loss: 77711.61653019831\n",
      "Epoch 2, Loss: 5384.416865422176\n",
      "Epoch 3, Loss: 2171.8326908991885\n",
      "Epoch 4, Loss: 1656.8418743426982\n",
      "Epoch 5, Loss: 1493.2133108285757\n",
      "Epoch 6, Loss: 1406.3860191932092\n",
      "Epoch 7, Loss: 1341.8510061410757\n",
      "Epoch 8, Loss: 1291.6156921386719\n",
      "Epoch 9, Loss: 1246.1929156963643\n",
      "Epoch 10, Loss: 1206.3575768103967\n",
      "Epoch 11, Loss: 1170.4368943434495\n",
      "Epoch 12, Loss: 1137.750713641827\n",
      "Epoch 13, Loss: 1105.0404381385217\n",
      "Epoch 14, Loss: 1073.707758976863\n",
      "Epoch 15, Loss: 1044.5529080904448\n",
      "Epoch 16, Loss: 1016.7388481727013\n",
      "Epoch 17, Loss: 987.2620098407452\n",
      "Epoch 18, Loss: 956.9973919208234\n",
      "Epoch 19, Loss: 926.9937626765325\n",
      "Epoch 20, Loss: 895.68723825308\n",
      "Epoch 21, Loss: 864.2581200232872\n",
      "Epoch 22, Loss: 838.4069988544171\n",
      "Epoch 23, Loss: 819.9976548414963\n",
      "Epoch 24, Loss: 809.4671677809495\n",
      "Epoch 25, Loss: 797.3403848501353\n",
      "Epoch 26, Loss: 785.6019850510818\n",
      "Epoch 27, Loss: 776.3571178729718\n",
      "Epoch 28, Loss: 766.2991485595703\n",
      "Epoch 29, Loss: 762.4006828894982\n",
      "Epoch 30, Loss: 750.4809276874249\n",
      "Epoch 31, Loss: 741.3958329420823\n",
      "Epoch 32, Loss: 733.2268970196063\n",
      "Epoch 33, Loss: 725.0773655818059\n",
      "Epoch 34, Loss: 716.8787125807542\n",
      "Epoch 35, Loss: 711.0771871713491\n",
      "Epoch 36, Loss: 702.60621877817\n",
      "Epoch 37, Loss: 698.6021423339844\n",
      "Epoch 38, Loss: 688.5596513014573\n",
      "Epoch 39, Loss: 682.9467069185697\n",
      "Epoch 40, Loss: 673.8681945800781\n",
      "Epoch 41, Loss: 667.5490182729868\n",
      "Epoch 42, Loss: 660.8172360933744\n",
      "Epoch 43, Loss: 654.380873460036\n",
      "Epoch 44, Loss: 647.965828528771\n",
      "Epoch 45, Loss: 644.0355142446665\n",
      "Epoch 46, Loss: 637.5058488112229\n",
      "Epoch 47, Loss: 631.0113208477313\n",
      "Epoch 48, Loss: 626.6185337946965\n",
      "Epoch 49, Loss: 621.4468031663162\n",
      "Epoch 50, Loss: 617.822041438176\n",
      "Epoch 51, Loss: 614.4988203782302\n",
      "Epoch 52, Loss: 606.0276993971604\n",
      "Epoch 53, Loss: 605.0946549635667\n",
      "Epoch 54, Loss: 597.7983034574069\n",
      "Epoch 55, Loss: 594.5421083890475\n",
      "Epoch 56, Loss: 590.1846759502704\n",
      "Epoch 57, Loss: 586.5158081054688\n",
      "Epoch 58, Loss: 585.1770747258113\n",
      "Epoch 59, Loss: 578.9637310321515\n",
      "Epoch 60, Loss: 574.7809929480919\n",
      "Epoch 61, Loss: 570.5653580885667\n",
      "Epoch 62, Loss: 565.4253927377554\n",
      "Epoch 63, Loss: 565.2425736647385\n",
      "Epoch 64, Loss: 558.4805180476262\n",
      "Epoch 65, Loss: 555.9987053504357\n",
      "Epoch 66, Loss: 551.5361222487229\n",
      "Epoch 67, Loss: 548.3307729867788\n",
      "Epoch 68, Loss: 546.3660372220553\n",
      "Epoch 69, Loss: 541.6072821983925\n",
      "Epoch 70, Loss: 538.4234654353215\n",
      "Epoch 71, Loss: 533.3218278151292\n",
      "Epoch 72, Loss: 530.6094677264874\n",
      "Epoch 73, Loss: 525.5027289757362\n",
      "Epoch 74, Loss: 527.0875948392428\n",
      "Epoch 75, Loss: 518.6425088735728\n",
      "Epoch 76, Loss: 515.6226067176232\n",
      "Epoch 77, Loss: 512.5471543532151\n",
      "Epoch 78, Loss: 508.38092275766223\n",
      "Epoch 79, Loss: 504.8484403170072\n",
      "Epoch 80, Loss: 501.92266963078424\n",
      "Epoch 81, Loss: 501.96866548978363\n",
      "Epoch 82, Loss: 494.58689762995795\n",
      "Epoch 83, Loss: 492.3035102257362\n",
      "Epoch 84, Loss: 490.88221858097955\n",
      "Epoch 85, Loss: 486.33405245267426\n",
      "Epoch 86, Loss: 481.2749293400691\n",
      "Epoch 87, Loss: 478.69317861703723\n",
      "Epoch 88, Loss: 476.0073441725511\n",
      "Epoch 89, Loss: 469.3286696213942\n",
      "Epoch 90, Loss: 466.09076514610877\n",
      "Epoch 91, Loss: 465.3257997952975\n",
      "Epoch 92, Loss: 461.5092515211839\n",
      "Epoch 93, Loss: 458.633058988131\n",
      "Epoch 94, Loss: 456.00223365196814\n",
      "Epoch 95, Loss: 455.814453125\n",
      "Epoch 96, Loss: 449.4217740572416\n",
      "Epoch 97, Loss: 447.08377544696515\n",
      "Epoch 98, Loss: 445.536125769982\n",
      "Epoch 99, Loss: 443.83255591759314\n",
      "Epoch 100, Loss: 441.65711622971753\n",
      "Epoch 101, Loss: 437.67136911245495\n",
      "Epoch 102, Loss: 434.77799166165863\n",
      "Epoch 103, Loss: 431.42564978966345\n",
      "Epoch 104, Loss: 431.5553412804237\n",
      "Epoch 105, Loss: 426.23463674692005\n",
      "Epoch 106, Loss: 424.89488689716046\n",
      "Epoch 107, Loss: 423.4726304274339\n",
      "Epoch 108, Loss: 419.17201350285455\n",
      "Epoch 109, Loss: 422.15968205378607\n",
      "Epoch 110, Loss: 417.0622758131761\n",
      "Epoch 111, Loss: 413.6679922250601\n",
      "Epoch 112, Loss: 415.66285236065204\n",
      "Epoch 113, Loss: 410.4419719989483\n",
      "Epoch 114, Loss: 411.46641892653247\n",
      "Epoch 115, Loss: 401.5126037597656\n",
      "Epoch 116, Loss: 403.7441723163311\n",
      "Epoch 117, Loss: 404.2851433387169\n",
      "Epoch 118, Loss: 405.7593947190505\n",
      "Epoch 119, Loss: 392.9397759070763\n",
      "Epoch 120, Loss: 395.1335918719952\n",
      "Epoch 121, Loss: 399.6195643498347\n",
      "Epoch 122, Loss: 393.8490565373347\n",
      "Epoch 123, Loss: 386.82552513709436\n",
      "Epoch 124, Loss: 382.68798006497894\n",
      "Epoch 125, Loss: 394.70504056490387\n",
      "Epoch 126, Loss: 387.96453270545373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:07:49,123] Trial 89 finished with value: 1261.4085412913041 and parameters: {'latent_dim_z1': 15, 'latent_dim_z2': 53, 'hidden_dim': 88, 'epochs': 131, 'causal_reg': 0.9998110290365817, 'learning_rate': 0.0006493669042606286}. Best is trial 25 with value: 394.9877769973252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127, Loss: 380.68208665114184\n",
      "Epoch 128, Loss: 376.1061542217548\n",
      "Epoch 129, Loss: 375.84810227614184\n",
      "Epoch 130, Loss: 372.85687138484076\n",
      "Epoch 131, Loss: 374.5066434420072\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:07:53,928] Trial 90 failed with parameters: {'latent_dim_z1': 21, 'latent_dim_z2': 59, 'hidden_dim': 117, 'epochs': 190, 'causal_reg': 0.7360195831347601, 'learning_rate': 0.09066214225923737} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:07:53,929] Trial 90 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 190, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:07:58,534] Trial 91 failed with parameters: {'latent_dim_z1': 21, 'latent_dim_z2': 60, 'hidden_dim': 123, 'epochs': 189, 'causal_reg': 0.7476963443206162, 'learning_rate': 0.06461195554785748} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:07:58,534] Trial 91 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 1, Loss: 4415.308758075421\n",
      "Epoch 2, Loss: 677.1473764272837\n",
      "Epoch 3, Loss: 667.3849123441256\n",
      "Epoch 4, Loss: 662.6380392221304\n",
      "Epoch 5, Loss: 657.6427917480469\n",
      "Epoch 6, Loss: 652.5180088923528\n",
      "Epoch 7, Loss: 646.6894061748798\n",
      "Epoch 8, Loss: 639.4862846961388\n",
      "Epoch 9, Loss: 631.3669691819412\n",
      "Epoch 10, Loss: 622.1841700627253\n",
      "Epoch 11, Loss: 612.3209686279297\n",
      "Epoch 12, Loss: 598.2861598088191\n",
      "Epoch 13, Loss: 585.4056854248047\n",
      "Epoch 14, Loss: 570.5571559025691\n",
      "Epoch 15, Loss: 555.1939028226412\n",
      "Epoch 16, Loss: 537.7806279109075\n",
      "Epoch 17, Loss: 519.1903933011569\n",
      "Epoch 18, Loss: 500.24402207594653\n",
      "Epoch 19, Loss: 482.8845719557542\n",
      "Epoch 20, Loss: 467.6210632324219\n",
      "Epoch 21, Loss: 452.01701002854566\n",
      "Epoch 22, Loss: 439.4550910362831\n",
      "Epoch 23, Loss: 427.2306882418119\n",
      "Epoch 24, Loss: 420.08736126239484\n",
      "Epoch 25, Loss: 411.24835205078125\n",
      "Epoch 26, Loss: 400.1994241567758\n",
      "Epoch 27, Loss: 395.07113177959735\n",
      "Epoch 28, Loss: 386.7025416447566\n",
      "Epoch 29, Loss: 383.84988285945013\n",
      "Epoch 30, Loss: 377.72422086275543\n",
      "Epoch 31, Loss: 373.20085378793567\n",
      "Epoch 32, Loss: 369.12078153170074\n",
      "Epoch 33, Loss: 365.6666975754958\n",
      "Epoch 34, Loss: 363.0413290170523\n",
      "Epoch 35, Loss: 360.4166201077975\n",
      "Epoch 36, Loss: 356.537354689378\n",
      "Epoch 37, Loss: 356.6937819260817\n",
      "Epoch 38, Loss: 352.927494929387\n",
      "Epoch 39, Loss: 349.4166071965144\n",
      "Epoch 40, Loss: 348.14049353966345\n",
      "Epoch 41, Loss: 345.0437469482422\n",
      "Epoch 42, Loss: 345.9283212515024\n",
      "Epoch 43, Loss: 344.3297072190505\n",
      "Epoch 44, Loss: 346.87120056152344\n",
      "Epoch 45, Loss: 339.36010155311\n",
      "Epoch 46, Loss: 339.62846022385816\n",
      "Epoch 47, Loss: 337.0194033109225\n",
      "Epoch 48, Loss: 334.68755516639123\n",
      "Epoch 49, Loss: 334.56300588754505\n",
      "Epoch 50, Loss: 335.1994358943059\n",
      "Epoch 51, Loss: 332.7799506554237\n",
      "Epoch 52, Loss: 331.1130729088417\n",
      "Epoch 53, Loss: 330.5848817091722\n",
      "Epoch 54, Loss: 327.78654714731067\n",
      "Epoch 55, Loss: 330.8300910362831\n",
      "Epoch 56, Loss: 328.45462505634015\n",
      "Epoch 57, Loss: 325.91990544245795\n",
      "Epoch 58, Loss: 326.8969538762019\n",
      "Epoch 59, Loss: 323.09508455716644\n",
      "Epoch 60, Loss: 325.6370098407452\n",
      "Epoch 61, Loss: 322.0350646972656\n",
      "Epoch 62, Loss: 322.1294133112981\n",
      "Epoch 63, Loss: 323.42837876539966\n",
      "Epoch 64, Loss: 318.9750213623047\n",
      "Epoch 65, Loss: 320.1697681133564\n",
      "Epoch 66, Loss: 320.50726318359375\n",
      "Epoch 67, Loss: 318.55103478064905\n",
      "Epoch 68, Loss: 318.7478496844952\n",
      "Epoch 69, Loss: 316.62261376014123\n",
      "Epoch 70, Loss: 319.2989114614633\n",
      "Epoch 71, Loss: 316.55151954064\n",
      "Epoch 72, Loss: 314.5801344651442\n",
      "Epoch 73, Loss: 313.61082458496094\n",
      "Epoch 74, Loss: 315.0790111835186\n",
      "Epoch 75, Loss: 312.5384451059195\n",
      "Epoch 76, Loss: 315.8559100811298\n",
      "Epoch 77, Loss: 315.94712125338043\n",
      "Epoch 78, Loss: 312.5430362407978\n",
      "Epoch 79, Loss: 311.75082514836237\n",
      "Epoch 80, Loss: 311.76553931603064\n",
      "Epoch 81, Loss: 312.79188068096454\n",
      "Epoch 82, Loss: 309.1154292179988\n",
      "Epoch 83, Loss: 309.29506213848407\n",
      "Epoch 84, Loss: 311.13575157752405\n",
      "Epoch 85, Loss: 310.6212205153245\n",
      "Epoch 86, Loss: 309.1808178241436\n",
      "Epoch 87, Loss: 307.4613365760216\n",
      "Epoch 88, Loss: 306.82349454439606\n",
      "Epoch 89, Loss: 307.9746422400841\n",
      "Epoch 90, Loss: 307.2563781738281\n",
      "Epoch 91, Loss: 307.37049983097955\n",
      "Epoch 92, Loss: 305.7418987567608\n",
      "Epoch 93, Loss: 304.9643249511719\n",
      "Epoch 94, Loss: 305.7376462496244\n",
      "Epoch 95, Loss: 305.618657038762\n",
      "Epoch 96, Loss: 304.30762833815356\n",
      "Epoch 97, Loss: 303.965817964994\n",
      "Epoch 98, Loss: 303.314809359037\n",
      "Epoch 99, Loss: 303.14959129920373\n",
      "Epoch 100, Loss: 303.59244537353516\n",
      "Epoch 101, Loss: 303.0549075786884\n",
      "Epoch 102, Loss: 302.3073231623723\n",
      "Epoch 103, Loss: 301.90987689678485\n",
      "Epoch 104, Loss: 302.09831472543567\n",
      "Epoch 105, Loss: 302.1982222336989\n",
      "Epoch 106, Loss: 301.0132340651292\n",
      "Epoch 107, Loss: 301.36814762995795\n",
      "Epoch 108, Loss: 300.42079397348255\n",
      "Epoch 109, Loss: 300.74234713040863\n",
      "Epoch 110, Loss: 299.1155589177058\n",
      "Epoch 111, Loss: 299.74617767333984\n",
      "Epoch 112, Loss: 301.08722041203424\n",
      "Epoch 113, Loss: 299.961183988131\n",
      "Epoch 114, Loss: 298.18226447472205\n",
      "Epoch 115, Loss: 298.415414076585\n",
      "Epoch 116, Loss: 298.330803504357\n",
      "Epoch 117, Loss: 298.05899341289813\n",
      "Epoch 118, Loss: 297.71668243408203\n",
      "Epoch 119, Loss: 297.0059415377103\n",
      "Epoch 120, Loss: 297.31293428861176\n",
      "Epoch 121, Loss: 297.0727756206806\n",
      "Epoch 122, Loss: 295.99806624192456\n",
      "Epoch 123, Loss: 296.2546116755559\n",
      "Epoch 124, Loss: 297.47468919020434\n",
      "Epoch 125, Loss: 298.807611318735\n",
      "Epoch 126, Loss: 297.8008458064153\n",
      "Epoch 127, Loss: 296.4469522329477\n",
      "Epoch 128, Loss: 296.987305861253\n",
      "Epoch 129, Loss: 294.20430462176984\n",
      "Epoch 130, Loss: 295.48352578970105\n",
      "Epoch 131, Loss: 294.1702681321364\n",
      "Epoch 132, Loss: 297.00378770094653\n",
      "Epoch 133, Loss: 293.4370645376352\n",
      "Epoch 134, Loss: 295.4394742525541\n",
      "Epoch 135, Loss: 293.70074638953577\n",
      "Epoch 136, Loss: 293.1711871807392\n",
      "Epoch 137, Loss: 293.2977993304913\n",
      "Epoch 138, Loss: 292.6681324885442\n",
      "Epoch 139, Loss: 292.7808779202975\n",
      "Epoch 140, Loss: 292.78346252441406\n",
      "Epoch 141, Loss: 292.1445183387169\n",
      "Epoch 142, Loss: 291.78839346078723\n",
      "Epoch 143, Loss: 293.8520237849309\n",
      "Epoch 144, Loss: 291.9359348003681\n",
      "Epoch 145, Loss: 291.1539541391226\n",
      "Epoch 146, Loss: 291.3959614680364\n",
      "Epoch 147, Loss: 290.3445774958684\n",
      "Epoch 148, Loss: 292.2775462223933\n",
      "Epoch 149, Loss: 292.6821553156926\n",
      "Epoch 150, Loss: 291.9459721491887\n",
      "Epoch 151, Loss: 290.2198920616737\n",
      "Epoch 152, Loss: 289.62788743239184\n",
      "Epoch 153, Loss: 290.04735389122595\n",
      "Epoch 154, Loss: 292.86018899770886\n",
      "Epoch 155, Loss: 289.9919685950646\n",
      "Epoch 156, Loss: 289.24896533672626\n",
      "Epoch 157, Loss: 288.50257697472205\n",
      "Epoch 158, Loss: 289.9133629432091\n",
      "Epoch 159, Loss: 292.44464698204627\n",
      "Epoch 160, Loss: 288.4943301861103\n",
      "Epoch 161, Loss: 289.8354451106145\n",
      "Epoch 162, Loss: 288.7015451284555\n",
      "Epoch 163, Loss: 289.04829171987683\n",
      "Epoch 164, Loss: 290.2560765193059\n",
      "Epoch 165, Loss: 287.5052478496845\n",
      "Epoch 166, Loss: 289.696776169997\n",
      "Epoch 167, Loss: 287.59190955528845\n",
      "Epoch 168, Loss: 287.0465334378756\n",
      "Epoch 169, Loss: 287.8165752704327\n",
      "Epoch 170, Loss: 291.6508918175331\n",
      "Epoch 171, Loss: 288.50356820913464\n",
      "Epoch 172, Loss: 285.6708655724159\n",
      "Epoch 173, Loss: 285.6739243727464\n",
      "Epoch 174, Loss: 287.20967982365534\n",
      "Epoch 175, Loss: 286.2899657029372\n",
      "Epoch 176, Loss: 285.86920283390924\n",
      "Epoch 177, Loss: 285.12310204139123\n",
      "Epoch 178, Loss: 285.52777803861176\n",
      "Epoch 179, Loss: 286.25042607234076\n",
      "Epoch 180, Loss: 285.23104975773737\n",
      "Epoch 181, Loss: 285.8745868389423\n",
      "Epoch 182, Loss: 284.6767677894005\n",
      "Epoch 183, Loss: 285.5321661142203\n",
      "Epoch 184, Loss: 285.05577087402344\n",
      "Epoch 185, Loss: 284.1119513878456\n",
      "Epoch 186, Loss: 284.80305950458234\n",
      "Epoch 187, Loss: 291.46911092904895\n",
      "Epoch 188, Loss: 287.63333599384015\n",
      "Epoch 189, Loss: 286.2250647911659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:08:02,917] Trial 92 finished with value: 362.6048607026998 and parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 59, 'hidden_dim': 125, 'epochs': 190, 'causal_reg': 0.7380689811369623, 'learning_rate': 0.00014949572012646852}. Best is trial 92 with value: 362.6048607026998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190, Loss: 286.7057577279898\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:08:07,516] Trial 93 failed with parameters: {'latent_dim_z1': 10, 'latent_dim_z2': 59, 'hidden_dim': 125, 'epochs': 193, 'causal_reg': 0.4778510416181917, 'learning_rate': 0.06170658730188355} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:08:07,517] Trial 93 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 190, Loss: nan\n",
      "Epoch 191, Loss: nan\n",
      "Epoch 192, Loss: nan\n",
      "Epoch 193, Loss: nan\n",
      "Epoch 1, Loss: 66882.85588191106\n",
      "Epoch 2, Loss: 17161.091045673078\n",
      "Epoch 3, Loss: 12064.983905498799\n",
      "Epoch 4, Loss: 9597.879422701322\n",
      "Epoch 5, Loss: 7951.414719801683\n",
      "Epoch 6, Loss: 6741.689622145433\n",
      "Epoch 7, Loss: 5819.945810171274\n",
      "Epoch 8, Loss: 5102.968886155349\n",
      "Epoch 9, Loss: 4534.744168795072\n",
      "Epoch 10, Loss: 4081.3038377028247\n",
      "Epoch 11, Loss: 3714.2789776141826\n",
      "Epoch 12, Loss: 3413.0003474308896\n",
      "Epoch 13, Loss: 3164.415269118089\n",
      "Epoch 14, Loss: 2956.829796424279\n",
      "Epoch 15, Loss: 2782.4671583909253\n",
      "Epoch 16, Loss: 2634.4832857572114\n",
      "Epoch 17, Loss: 2508.4454064002402\n",
      "Epoch 18, Loss: 2399.492901141827\n",
      "Epoch 19, Loss: 2305.332707331731\n",
      "Epoch 20, Loss: 2223.0812307504507\n",
      "Epoch 21, Loss: 2151.188723050631\n",
      "Epoch 22, Loss: 2087.2862736628604\n",
      "Epoch 23, Loss: 2030.8345289963943\n",
      "Epoch 24, Loss: 1980.5065190241887\n",
      "Epoch 25, Loss: 1934.7643057016226\n",
      "Epoch 26, Loss: 1894.0098313551682\n",
      "Epoch 27, Loss: 1856.5344801682693\n",
      "Epoch 28, Loss: 1823.0796344463643\n",
      "Epoch 29, Loss: 1792.1643277681792\n",
      "Epoch 30, Loss: 1762.8780470628005\n",
      "Epoch 31, Loss: 1736.0814655010518\n",
      "Epoch 32, Loss: 1711.8078143780049\n",
      "Epoch 33, Loss: 1688.1515855055588\n",
      "Epoch 34, Loss: 1666.8523653470552\n",
      "Epoch 35, Loss: 1645.776113656851\n",
      "Epoch 36, Loss: 1626.78616802509\n",
      "Epoch 37, Loss: 1607.7968491774338\n",
      "Epoch 38, Loss: 1590.0308251014123\n",
      "Epoch 39, Loss: 1572.7567068246694\n",
      "Epoch 40, Loss: 1555.508580134465\n",
      "Epoch 41, Loss: 1539.7127145620493\n",
      "Epoch 42, Loss: 1524.0204256497896\n",
      "Epoch 43, Loss: 1507.741950401893\n",
      "Epoch 44, Loss: 1492.8519334059495\n",
      "Epoch 45, Loss: 1478.0541663536658\n",
      "Epoch 46, Loss: 1463.0923837515024\n",
      "Epoch 47, Loss: 1449.0274493877705\n",
      "Epoch 48, Loss: 1435.6529752291167\n",
      "Epoch 49, Loss: 1422.8702979454627\n",
      "Epoch 50, Loss: 1409.867438683143\n",
      "Epoch 51, Loss: 1397.1152954101562\n",
      "Epoch 52, Loss: 1384.2919029822717\n",
      "Epoch 53, Loss: 1372.2325369027944\n",
      "Epoch 54, Loss: 1360.4909104567307\n",
      "Epoch 55, Loss: 1348.9238750751201\n",
      "Epoch 56, Loss: 1337.7373680701623\n",
      "Epoch 57, Loss: 1326.8374962439905\n",
      "Epoch 58, Loss: 1315.6680274376502\n",
      "Epoch 59, Loss: 1304.6998549241287\n",
      "Epoch 60, Loss: 1294.621347280649\n",
      "Epoch 61, Loss: 1284.3399611253005\n",
      "Epoch 62, Loss: 1274.166017972506\n",
      "Epoch 63, Loss: 1264.4068486140325\n",
      "Epoch 64, Loss: 1254.125255878155\n",
      "Epoch 65, Loss: 1244.9104309082031\n",
      "Epoch 66, Loss: 1235.277623103215\n",
      "Epoch 67, Loss: 1225.3497056227463\n",
      "Epoch 68, Loss: 1216.38475388747\n",
      "Epoch 69, Loss: 1207.1415640024038\n",
      "Epoch 70, Loss: 1198.2437814565806\n",
      "Epoch 71, Loss: 1189.2871563251201\n",
      "Epoch 72, Loss: 1180.7674701397236\n",
      "Epoch 73, Loss: 1172.3131432166467\n",
      "Epoch 74, Loss: 1163.2433236929087\n",
      "Epoch 75, Loss: 1154.9034517728364\n",
      "Epoch 76, Loss: 1146.3075326772837\n",
      "Epoch 77, Loss: 1138.4504089355469\n",
      "Epoch 78, Loss: 1130.4590594951924\n",
      "Epoch 79, Loss: 1122.4129826472356\n",
      "Epoch 80, Loss: 1115.217005803035\n",
      "Epoch 81, Loss: 1107.184302696815\n",
      "Epoch 82, Loss: 1099.4867131159856\n",
      "Epoch 83, Loss: 1091.9870382455679\n",
      "Epoch 84, Loss: 1085.0218294583833\n",
      "Epoch 85, Loss: 1077.0787611741287\n",
      "Epoch 86, Loss: 1070.3270075871394\n",
      "Epoch 87, Loss: 1063.3820307804988\n",
      "Epoch 88, Loss: 1056.1784221942607\n",
      "Epoch 89, Loss: 1050.0279810978816\n",
      "Epoch 90, Loss: 1043.1590623121995\n",
      "Epoch 91, Loss: 1036.2503720797026\n",
      "Epoch 92, Loss: 1030.0586641751802\n",
      "Epoch 93, Loss: 1023.6070016714243\n",
      "Epoch 94, Loss: 1016.997067964994\n",
      "Epoch 95, Loss: 1011.0875889704778\n",
      "Epoch 96, Loss: 1004.8147794283353\n",
      "Epoch 97, Loss: 998.7102121206431\n",
      "Epoch 98, Loss: 992.9139099121094\n",
      "Epoch 99, Loss: 986.476814856896\n",
      "Epoch 100, Loss: 980.7796513484075\n",
      "Epoch 101, Loss: 974.6283674973708\n",
      "Epoch 102, Loss: 968.586905846229\n",
      "Epoch 103, Loss: 963.6859717735878\n",
      "Epoch 104, Loss: 956.9838738074669\n",
      "Epoch 105, Loss: 951.4473759577825\n",
      "Epoch 106, Loss: 945.5067068246694\n",
      "Epoch 107, Loss: 940.0354226919321\n",
      "Epoch 108, Loss: 934.160891019381\n",
      "Epoch 109, Loss: 928.333736713116\n",
      "Epoch 110, Loss: 923.2947505070613\n",
      "Epoch 111, Loss: 916.9550945575421\n",
      "Epoch 112, Loss: 911.2420360858624\n",
      "Epoch 113, Loss: 905.1332327035757\n",
      "Epoch 114, Loss: 899.1863555908203\n",
      "Epoch 115, Loss: 893.0211674616887\n",
      "Epoch 116, Loss: 886.476814856896\n",
      "Epoch 117, Loss: 880.4025597205529\n",
      "Epoch 118, Loss: 873.8048882117638\n",
      "Epoch 119, Loss: 867.4362194354718\n",
      "Epoch 120, Loss: 860.9161564753606\n",
      "Epoch 121, Loss: 854.1303041898287\n",
      "Epoch 122, Loss: 847.5526158259465\n",
      "Epoch 123, Loss: 840.9497692401593\n",
      "Epoch 124, Loss: 834.5097034160907\n",
      "Epoch 125, Loss: 827.4189770038312\n",
      "Epoch 126, Loss: 821.0933814415565\n",
      "Epoch 127, Loss: 814.0697960486779\n",
      "Epoch 128, Loss: 807.3807643010066\n",
      "Epoch 129, Loss: 801.3672168438251\n",
      "Epoch 130, Loss: 795.3565075214093\n",
      "Epoch 131, Loss: 789.4243011474609\n",
      "Epoch 132, Loss: 783.7576786921575\n",
      "Epoch 133, Loss: 777.906245304988\n",
      "Epoch 134, Loss: 772.181882418119\n",
      "Epoch 135, Loss: 766.5392596905048\n",
      "Epoch 136, Loss: 761.7519859900841\n",
      "Epoch 137, Loss: 756.82470703125\n",
      "Epoch 138, Loss: 752.219734778771\n",
      "Epoch 139, Loss: 747.6053548959585\n",
      "Epoch 140, Loss: 743.6195784348708\n",
      "Epoch 141, Loss: 739.5478891225962\n",
      "Epoch 142, Loss: 735.9892143836388\n",
      "Epoch 143, Loss: 731.8715198223407\n",
      "Epoch 144, Loss: 727.9888845590444\n",
      "Epoch 145, Loss: 724.7738001896785\n",
      "Epoch 146, Loss: 720.8458979679988\n",
      "Epoch 147, Loss: 717.0055107703575\n",
      "Epoch 148, Loss: 713.7398470365084\n",
      "Epoch 149, Loss: 710.4076784574069\n",
      "Epoch 150, Loss: 706.9884150578425\n",
      "Epoch 151, Loss: 703.9081221360427\n",
      "Epoch 152, Loss: 700.6119478665865\n",
      "Epoch 153, Loss: 697.6404219407302\n",
      "Epoch 154, Loss: 694.3014561579778\n",
      "Epoch 155, Loss: 691.1301938570463\n",
      "Epoch 156, Loss: 687.8919560359075\n",
      "Epoch 157, Loss: 685.3488229604868\n",
      "Epoch 158, Loss: 682.2385664719802\n",
      "Epoch 159, Loss: 679.482421875\n",
      "Epoch 160, Loss: 676.6190396822416\n",
      "Epoch 161, Loss: 673.7121746356671\n",
      "Epoch 162, Loss: 670.8950453538162\n",
      "Epoch 163, Loss: 667.9387371356671\n",
      "Epoch 164, Loss: 665.4717595027043\n",
      "Epoch 165, Loss: 662.7921518179087\n",
      "Epoch 166, Loss: 659.8384622427134\n",
      "Epoch 167, Loss: 657.1752108060397\n",
      "Epoch 168, Loss: 654.7212782639724\n",
      "Epoch 169, Loss: 651.8115774301382\n",
      "Epoch 170, Loss: 648.9457503098708\n",
      "Epoch 171, Loss: 646.2465245173528\n",
      "Epoch 172, Loss: 643.7974372276893\n",
      "Epoch 173, Loss: 641.3746091402494\n",
      "Epoch 174, Loss: 638.5905327430138\n",
      "Epoch 175, Loss: 635.5668041522687\n",
      "Epoch 176, Loss: 632.6224599984976\n",
      "Epoch 177, Loss: 629.9991502028245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-09 16:08:11,573] Trial 94 finished with value: 1864.0603816273451 and parameters: {'latent_dim_z1': 11, 'latent_dim_z2': 59, 'hidden_dim': 121, 'epochs': 182, 'causal_reg': 0.45453608768239634, 'learning_rate': 2.9482882447451452e-05}. Best is trial 92 with value: 362.6048607026998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178, Loss: 627.3134976900541\n",
      "Epoch 179, Loss: 624.320048405574\n",
      "Epoch 180, Loss: 621.7925872802734\n",
      "Epoch 181, Loss: 618.8839029165415\n",
      "Epoch 182, Loss: 615.7032916729266\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:08:16,055] Trial 95 failed with parameters: {'latent_dim_z1': 20, 'latent_dim_z2': 52, 'hidden_dim': 153, 'epochs': 194, 'causal_reg': 0.6860652889174356, 'learning_rate': 0.025949897212593317} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:08:16,055] Trial 95 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 190, Loss: nan\n",
      "Epoch 191, Loss: nan\n",
      "Epoch 192, Loss: nan\n",
      "Epoch 193, Loss: nan\n",
      "Epoch 194, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 190, Loss: nan\n",
      "Epoch 191, Loss: nan\n",
      "Epoch 192, Loss: nan\n",
      "Epoch 193, Loss: nan\n",
      "Epoch 194, Loss: nan\n",
      "Epoch 195, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:08:20,646] Trial 96 failed with parameters: {'latent_dim_z1': 20, 'latent_dim_z2': 52, 'hidden_dim': 158, 'epochs': 198, 'causal_reg': 0.7237678558817836, 'learning_rate': 0.055395788742110985} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:08:20,646] Trial 96 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196, Loss: nan\n",
      "Epoch 197, Loss: nan\n",
      "Epoch 198, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 190, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:08:25,159] Trial 97 failed with parameters: {'latent_dim_z1': 21, 'latent_dim_z2': 52, 'hidden_dim': 146, 'epochs': 197, 'causal_reg': 0.7135198213551776, 'learning_rate': 0.06861661183710298} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:08:25,160] Trial 97 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191, Loss: nan\n",
      "Epoch 192, Loss: nan\n",
      "Epoch 193, Loss: nan\n",
      "Epoch 194, Loss: nan\n",
      "Epoch 195, Loss: nan\n",
      "Epoch 196, Loss: nan\n",
      "Epoch 197, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n",
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Epoch 190, Loss: nan\n",
      "Epoch 191, Loss: nan\n",
      "Epoch 192, Loss: nan\n",
      "Epoch 193, Loss: nan\n",
      "Epoch 194, Loss: nan\n",
      "Epoch 195, Loss: nan\n",
      "Epoch 196, Loss: nan\n",
      "Epoch 197, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:08:29,742] Trial 98 failed with parameters: {'latent_dim_z1': 22, 'latent_dim_z2': 52, 'hidden_dim': 153, 'epochs': 200, 'causal_reg': 0.7257152234502848, 'learning_rate': 0.07053262497829192} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:08:29,742] Trial 98 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198, Loss: nan\n",
      "Epoch 199, Loss: nan\n",
      "Epoch 200, Loss: nan\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Epoch 51, Loss: nan\n",
      "Epoch 52, Loss: nan\n",
      "Epoch 53, Loss: nan\n",
      "Epoch 54, Loss: nan\n",
      "Epoch 55, Loss: nan\n",
      "Epoch 56, Loss: nan\n",
      "Epoch 57, Loss: nan\n",
      "Epoch 58, Loss: nan\n",
      "Epoch 59, Loss: nan\n",
      "Epoch 60, Loss: nan\n",
      "Epoch 61, Loss: nan\n",
      "Epoch 62, Loss: nan\n",
      "Epoch 63, Loss: nan\n",
      "Epoch 64, Loss: nan\n",
      "Epoch 65, Loss: nan\n",
      "Epoch 66, Loss: nan\n",
      "Epoch 67, Loss: nan\n",
      "Epoch 68, Loss: nan\n",
      "Epoch 69, Loss: nan\n",
      "Epoch 70, Loss: nan\n",
      "Epoch 71, Loss: nan\n",
      "Epoch 72, Loss: nan\n",
      "Epoch 73, Loss: nan\n",
      "Epoch 74, Loss: nan\n",
      "Epoch 75, Loss: nan\n",
      "Epoch 76, Loss: nan\n",
      "Epoch 77, Loss: nan\n",
      "Epoch 78, Loss: nan\n",
      "Epoch 79, Loss: nan\n",
      "Epoch 80, Loss: nan\n",
      "Epoch 81, Loss: nan\n",
      "Epoch 82, Loss: nan\n",
      "Epoch 83, Loss: nan\n",
      "Epoch 84, Loss: nan\n",
      "Epoch 85, Loss: nan\n",
      "Epoch 86, Loss: nan\n",
      "Epoch 87, Loss: nan\n",
      "Epoch 88, Loss: nan\n",
      "Epoch 89, Loss: nan\n",
      "Epoch 90, Loss: nan\n",
      "Epoch 91, Loss: nan\n",
      "Epoch 92, Loss: nan\n",
      "Epoch 93, Loss: nan\n",
      "Epoch 94, Loss: nan\n",
      "Epoch 95, Loss: nan\n",
      "Epoch 96, Loss: nan\n",
      "Epoch 97, Loss: nan\n",
      "Epoch 98, Loss: nan\n",
      "Epoch 99, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 101, Loss: nan\n",
      "Epoch 102, Loss: nan\n",
      "Epoch 103, Loss: nan\n",
      "Epoch 104, Loss: nan\n",
      "Epoch 105, Loss: nan\n",
      "Epoch 106, Loss: nan\n",
      "Epoch 107, Loss: nan\n",
      "Epoch 108, Loss: nan\n",
      "Epoch 109, Loss: nan\n",
      "Epoch 110, Loss: nan\n",
      "Epoch 111, Loss: nan\n",
      "Epoch 112, Loss: nan\n",
      "Epoch 113, Loss: nan\n",
      "Epoch 114, Loss: nan\n",
      "Epoch 115, Loss: nan\n",
      "Epoch 116, Loss: nan\n",
      "Epoch 117, Loss: nan\n",
      "Epoch 118, Loss: nan\n",
      "Epoch 119, Loss: nan\n",
      "Epoch 120, Loss: nan\n",
      "Epoch 121, Loss: nan\n",
      "Epoch 122, Loss: nan\n",
      "Epoch 123, Loss: nan\n",
      "Epoch 124, Loss: nan\n",
      "Epoch 125, Loss: nan\n",
      "Epoch 126, Loss: nan\n",
      "Epoch 127, Loss: nan\n",
      "Epoch 128, Loss: nan\n",
      "Epoch 129, Loss: nan\n",
      "Epoch 130, Loss: nan\n",
      "Epoch 131, Loss: nan\n",
      "Epoch 132, Loss: nan\n",
      "Epoch 133, Loss: nan\n",
      "Epoch 134, Loss: nan\n",
      "Epoch 135, Loss: nan\n",
      "Epoch 136, Loss: nan\n",
      "Epoch 137, Loss: nan\n",
      "Epoch 138, Loss: nan\n",
      "Epoch 139, Loss: nan\n",
      "Epoch 140, Loss: nan\n",
      "Epoch 141, Loss: nan\n",
      "Epoch 142, Loss: nan\n",
      "Epoch 143, Loss: nan\n",
      "Epoch 144, Loss: nan\n",
      "Epoch 145, Loss: nan\n",
      "Epoch 146, Loss: nan\n",
      "Epoch 147, Loss: nan\n",
      "Epoch 148, Loss: nan\n",
      "Epoch 149, Loss: nan\n",
      "Epoch 150, Loss: nan\n",
      "Epoch 151, Loss: nan\n",
      "Epoch 152, Loss: nan\n",
      "Epoch 153, Loss: nan\n",
      "Epoch 154, Loss: nan\n",
      "Epoch 155, Loss: nan\n",
      "Epoch 156, Loss: nan\n",
      "Epoch 157, Loss: nan\n",
      "Epoch 158, Loss: nan\n",
      "Epoch 159, Loss: nan\n",
      "Epoch 160, Loss: nan\n",
      "Epoch 161, Loss: nan\n",
      "Epoch 162, Loss: nan\n",
      "Epoch 163, Loss: nan\n",
      "Epoch 164, Loss: nan\n",
      "Epoch 165, Loss: nan\n",
      "Epoch 166, Loss: nan\n",
      "Epoch 167, Loss: nan\n",
      "Epoch 168, Loss: nan\n",
      "Epoch 169, Loss: nan\n",
      "Epoch 170, Loss: nan\n",
      "Epoch 171, Loss: nan\n",
      "Epoch 172, Loss: nan\n",
      "Epoch 173, Loss: nan\n",
      "Epoch 174, Loss: nan\n",
      "Epoch 175, Loss: nan\n",
      "Epoch 176, Loss: nan\n",
      "Epoch 177, Loss: nan\n",
      "Epoch 178, Loss: nan\n",
      "Epoch 179, Loss: nan\n",
      "Epoch 180, Loss: nan\n",
      "Epoch 181, Loss: nan\n",
      "Epoch 182, Loss: nan\n",
      "Epoch 183, Loss: nan\n",
      "Epoch 184, Loss: nan\n",
      "Epoch 185, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-09 16:08:34,032] Trial 99 failed with parameters: {'latent_dim_z1': 21, 'latent_dim_z2': 52, 'hidden_dim': 146, 'epochs': 189, 'causal_reg': 0.7391718430918132, 'learning_rate': 0.07845032000076091} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-03-09 16:08:34,032] Trial 99 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186, Loss: nan\n",
      "Epoch 187, Loss: nan\n",
      "Epoch 188, Loss: nan\n",
      "Epoch 189, Loss: nan\n",
      "Best hyperparameters:  {'latent_dim_z1': 10, 'latent_dim_z2': 59, 'hidden_dim': 125, 'epochs': 190, 'causal_reg': 0.7380689811369623, 'learning_rate': 0.00014949572012646852}\n",
      "Best validation loss:  362.6048607026998\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")  # Minimize the validation loss\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "00ee0fa4-5b46-486e-acb5-66ff722a7eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'latent_dim_z1': 10, 'latent_dim_z2': 59, 'hidden_dim': 125, 'epochs': 190, 'causal_reg': 0.7380689811369623, 'learning_rate': 0.00014949572012646852}\n",
      "Best validation loss:  362.6048607026998\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Best validation loss: \", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "396d4e19-79dc-4abe-b062-bf1e969c8e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7277.553905780499\n",
      "Epoch 2, Loss: 962.6521712083083\n",
      "Epoch 3, Loss: 708.9931910588191\n",
      "Epoch 4, Loss: 680.9330808199369\n",
      "Epoch 5, Loss: 669.0604294996995\n",
      "Epoch 6, Loss: 661.0105860783503\n",
      "Epoch 7, Loss: 655.287107027494\n",
      "Epoch 8, Loss: 650.4564878023588\n",
      "Epoch 9, Loss: 646.2296330378606\n",
      "Epoch 10, Loss: 642.2817112849309\n",
      "Epoch 11, Loss: 638.9939704308143\n",
      "Epoch 12, Loss: 635.9552905742938\n",
      "Epoch 13, Loss: 632.710452740009\n",
      "Epoch 14, Loss: 629.9885594294622\n",
      "Epoch 15, Loss: 627.4507188063401\n",
      "Epoch 16, Loss: 623.4884713979868\n",
      "Epoch 17, Loss: 619.7812981238732\n",
      "Epoch 18, Loss: 614.8439483642578\n",
      "Epoch 19, Loss: 607.0066422682542\n",
      "Epoch 20, Loss: 594.9738699106069\n",
      "Epoch 21, Loss: 576.6617631178635\n",
      "Epoch 22, Loss: 552.0675858717698\n",
      "Epoch 23, Loss: 526.1634404109075\n",
      "Epoch 24, Loss: 500.874514066256\n",
      "Epoch 25, Loss: 476.44667287973255\n",
      "Epoch 26, Loss: 456.90846487192005\n",
      "Epoch 27, Loss: 440.9939751258263\n",
      "Epoch 28, Loss: 427.8338177020733\n",
      "Epoch 29, Loss: 418.0857708270733\n",
      "Epoch 30, Loss: 407.48635042630707\n",
      "Epoch 31, Loss: 400.29664024939905\n",
      "Epoch 32, Loss: 393.2763284536508\n",
      "Epoch 33, Loss: 388.2552560659555\n",
      "Epoch 34, Loss: 382.91191335824817\n",
      "Epoch 35, Loss: 383.0323955829327\n",
      "Epoch 36, Loss: 378.5381387563852\n",
      "Epoch 37, Loss: 371.4051525409405\n",
      "Epoch 38, Loss: 368.44689002403845\n",
      "Epoch 39, Loss: 365.0138326791617\n",
      "Epoch 40, Loss: 362.11209810697113\n",
      "Epoch 41, Loss: 358.5856487567608\n",
      "Epoch 42, Loss: 357.6291010929988\n",
      "Epoch 43, Loss: 355.78095069298377\n",
      "Epoch 44, Loss: 354.22953326885516\n",
      "Epoch 45, Loss: 350.3900574904222\n",
      "Epoch 46, Loss: 348.8031252347506\n",
      "Epoch 47, Loss: 348.57310485839844\n",
      "Epoch 48, Loss: 348.291264460637\n",
      "Epoch 49, Loss: 344.4904597355769\n",
      "Epoch 50, Loss: 343.1579190767728\n",
      "Epoch 51, Loss: 341.4284198467548\n",
      "Epoch 52, Loss: 340.84942744328424\n",
      "Epoch 53, Loss: 339.6058924748347\n",
      "Epoch 54, Loss: 338.41968125563403\n",
      "Epoch 55, Loss: 338.52832852877106\n",
      "Epoch 56, Loss: 336.45110966609076\n",
      "Epoch 57, Loss: 335.43473346416766\n",
      "Epoch 58, Loss: 334.4540927593525\n",
      "Epoch 59, Loss: 333.39422372671277\n",
      "Epoch 60, Loss: 333.27344923753003\n",
      "Epoch 61, Loss: 335.0372443565956\n",
      "Epoch 62, Loss: 331.53511516864484\n",
      "Epoch 63, Loss: 330.55579669658954\n",
      "Epoch 64, Loss: 329.7677271916316\n",
      "Epoch 65, Loss: 328.680173433744\n",
      "Epoch 66, Loss: 329.4087383563702\n",
      "Epoch 67, Loss: 329.1535139817458\n",
      "Epoch 68, Loss: 326.3568314772386\n",
      "Epoch 69, Loss: 326.37532395582934\n",
      "Epoch 70, Loss: 326.34757173978363\n",
      "Epoch 71, Loss: 324.79072922926684\n",
      "Epoch 72, Loss: 325.38908620981067\n",
      "Epoch 73, Loss: 324.08466280423676\n",
      "Epoch 74, Loss: 324.80204420823316\n",
      "Epoch 75, Loss: 323.69131821852466\n",
      "Epoch 76, Loss: 322.9150860126202\n",
      "Epoch 77, Loss: 322.5868577223558\n",
      "Epoch 78, Loss: 320.6212909405048\n",
      "Epoch 79, Loss: 322.84102337176984\n",
      "Epoch 80, Loss: 322.4937814565805\n",
      "Epoch 81, Loss: 320.1600905198317\n",
      "Epoch 82, Loss: 320.1403016310472\n",
      "Epoch 83, Loss: 320.63492290790265\n",
      "Epoch 84, Loss: 318.97059279221753\n",
      "Epoch 85, Loss: 317.4122971754808\n",
      "Epoch 86, Loss: 319.86896338829627\n",
      "Epoch 87, Loss: 318.2401111309345\n",
      "Epoch 88, Loss: 317.0676598182091\n",
      "Epoch 89, Loss: 316.60482318584735\n",
      "Epoch 90, Loss: 316.17566856971155\n",
      "Epoch 91, Loss: 315.95071998009314\n",
      "Epoch 92, Loss: 315.46440946138824\n",
      "Epoch 93, Loss: 315.21171804574817\n",
      "Epoch 94, Loss: 313.75352595402643\n",
      "Epoch 95, Loss: 315.4745518611028\n",
      "Epoch 96, Loss: 314.6342010498047\n",
      "Epoch 97, Loss: 314.4775173480694\n",
      "Epoch 98, Loss: 313.058839064378\n",
      "Epoch 99, Loss: 312.53153639573316\n",
      "Epoch 100, Loss: 311.8690684391902\n",
      "Epoch 101, Loss: 310.7709397536058\n",
      "Epoch 102, Loss: 312.86678725022534\n",
      "Epoch 103, Loss: 311.98041006234973\n",
      "Epoch 104, Loss: 309.85845243013824\n",
      "Epoch 105, Loss: 309.28737112192005\n",
      "Epoch 106, Loss: 309.4703539334811\n",
      "Epoch 107, Loss: 310.24115224984973\n",
      "Epoch 108, Loss: 310.42374126727765\n",
      "Epoch 109, Loss: 308.279541015625\n",
      "Epoch 110, Loss: 308.9675310575045\n",
      "Epoch 111, Loss: 308.2116511418269\n",
      "Epoch 112, Loss: 307.17991520808295\n",
      "Epoch 113, Loss: 307.3197091909555\n",
      "Epoch 114, Loss: 307.8907423753005\n",
      "Epoch 115, Loss: 307.36022597092847\n",
      "Epoch 116, Loss: 311.6259554349459\n",
      "Epoch 117, Loss: 306.0038833618164\n",
      "Epoch 118, Loss: 305.3945782001202\n",
      "Epoch 119, Loss: 305.1304438664363\n",
      "Epoch 120, Loss: 305.388185941256\n",
      "Epoch 121, Loss: 304.17312504695013\n",
      "Epoch 122, Loss: 304.96192345252405\n",
      "Epoch 123, Loss: 303.2101082435021\n",
      "Epoch 124, Loss: 304.6774327204778\n",
      "Epoch 125, Loss: 304.9667147122897\n",
      "Epoch 126, Loss: 304.5301008958083\n",
      "Epoch 127, Loss: 304.07286658653845\n",
      "Epoch 128, Loss: 308.16509892390326\n",
      "Epoch 129, Loss: 303.01731168306793\n",
      "Epoch 130, Loss: 301.7957974947416\n",
      "Epoch 131, Loss: 302.22903325007513\n",
      "Epoch 132, Loss: 301.2854391244742\n",
      "Epoch 133, Loss: 300.39962885929987\n",
      "Epoch 134, Loss: 301.6185948298528\n",
      "Epoch 135, Loss: 300.81736637995795\n",
      "Epoch 136, Loss: 300.52281188964844\n",
      "Epoch 137, Loss: 300.0260138878456\n",
      "Epoch 138, Loss: 300.0098606989934\n",
      "Epoch 139, Loss: 300.0519503079928\n",
      "Epoch 140, Loss: 299.1258744459886\n",
      "Epoch 141, Loss: 299.3239499605619\n",
      "Epoch 142, Loss: 299.89085388183594\n",
      "Epoch 143, Loss: 303.04320702186\n",
      "Epoch 144, Loss: 300.86652902456433\n",
      "Epoch 145, Loss: 299.2434610220102\n",
      "Epoch 146, Loss: 297.56722670335034\n",
      "Epoch 147, Loss: 298.4799487774189\n",
      "Epoch 148, Loss: 298.82086885892426\n",
      "Epoch 149, Loss: 297.60611548790564\n",
      "Epoch 150, Loss: 296.04883340688855\n",
      "Epoch 151, Loss: 296.8387709397536\n",
      "Epoch 152, Loss: 296.7216562124399\n",
      "Epoch 153, Loss: 297.7884451059195\n",
      "Epoch 154, Loss: 296.3674058180589\n",
      "Epoch 155, Loss: 296.21246455265924\n",
      "Epoch 156, Loss: 295.69492692213794\n",
      "Epoch 157, Loss: 295.20056973970856\n",
      "Epoch 158, Loss: 298.0192683293269\n",
      "Epoch 159, Loss: 295.0254997840294\n",
      "Epoch 160, Loss: 295.19650503305286\n",
      "Epoch 161, Loss: 297.4450472318209\n",
      "Epoch 162, Loss: 294.41801687387317\n",
      "Epoch 163, Loss: 294.26380920410156\n",
      "Epoch 164, Loss: 293.87703411395734\n",
      "Epoch 165, Loss: 293.7130889892578\n",
      "Epoch 166, Loss: 294.4500474196214\n",
      "Epoch 167, Loss: 296.9983696570763\n",
      "Epoch 168, Loss: 293.45183621920074\n",
      "Epoch 169, Loss: 294.1515420766977\n",
      "Epoch 170, Loss: 293.5670870267428\n",
      "Epoch 171, Loss: 292.65748126690204\n",
      "Epoch 172, Loss: 292.8170911348783\n",
      "Epoch 173, Loss: 292.8413185706505\n",
      "Epoch 174, Loss: 293.62200634296124\n",
      "Epoch 175, Loss: 295.33109694260816\n",
      "Epoch 176, Loss: 292.46886150653546\n",
      "Epoch 177, Loss: 291.6880141038161\n",
      "Epoch 178, Loss: 291.2341062105619\n",
      "Epoch 179, Loss: 290.75943462665265\n",
      "Epoch 180, Loss: 291.1354323167067\n",
      "Epoch 181, Loss: 293.03785940317005\n",
      "Epoch 182, Loss: 290.1808307354267\n",
      "Epoch 183, Loss: 292.63170271653394\n",
      "Epoch 184, Loss: 289.53025935246393\n",
      "Epoch 185, Loss: 290.76453751784106\n",
      "Epoch 186, Loss: 290.76369006817157\n",
      "Epoch 187, Loss: 289.6230568519005\n",
      "Epoch 188, Loss: 289.5112603994516\n",
      "Epoch 189, Loss: 289.4365046574519\n",
      "Epoch 190, Loss: 290.03895216721753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "394.9844367980957"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_latent_dim_z1 = 10\n",
    "best_latent_dim_z2 = 59\n",
    "best_hidden_dim = 125\n",
    "best_epochs = 190\n",
    "best_learning_rate = 0.00014949572012646852\n",
    "best_causal_reg = 0.7380689811369623\n",
    "\n",
    "model_ilr = ModifiedVAE(614, best_latent_dim_z1,best_latent_dim_z2,best_hidden_dim)\n",
    "optimizer = torch.optim.Adam(model_ilr.parameters(), lr=best_learning_rate)\n",
    "\n",
    "train(model_ilr, data_loader, optimizer, best_epochs, best_causal_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9acdd574-489a-4296-bca8-307eb2bfebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ilr.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed_data = []\n",
    "    latent_z1 = []\n",
    "    latent_z2 = []\n",
    "    for x, _ in data_loader:\n",
    "        recon_x, mu_z1, logvar_z1, mu_z2, logvar_z2 = model_ilr(x, _)\n",
    "        latent_z1.append(mu_z1)\n",
    "        latent_z2.append(mu_z2)\n",
    "        reconstructed_data.append(recon_x)\n",
    "        \n",
    "    reconstructed_data = torch.cat(reconstructed_data).numpy()\n",
    "    latent_z1 = torch.cat(latent_z1).numpy()\n",
    "    latent_z2 = torch.cat(latent_z2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05752c45-e59a-4867-9123-bba6c9fb8df9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 Classifier Results:\n",
      "[[  0  87]\n",
      " [  0 157]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.64      1.00      0.78       157\n",
      "\n",
      "    accuracy                           0.64       244\n",
      "   macro avg       0.32      0.50      0.39       244\n",
      "weighted avg       0.41      0.64      0.50       244\n",
      "\n",
      "Z2 Classifier Results:\n",
      "[[  0  87]\n",
      " [  0 157]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.64      1.00      0.78       157\n",
      "\n",
      "    accuracy                           0.64       244\n",
      "   macro avg       0.32      0.50      0.39       244\n",
      "weighted avg       0.41      0.64      0.50       244\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Test with Z1\n",
    "X_train_z1, X_test_z1, y_train_z1, y_test_z1 = train_test_split(latent_z1, Y_labels, test_size=0.3, random_state=42)\n",
    "classifier_z1 = LogisticRegression(max_iter=1000)\n",
    "classifier_z1.fit(X_train_z1, y_train_z1)\n",
    "\n",
    "y_pred_z1 = classifier_z1.predict(X_test_z1)\n",
    "\n",
    "print(\"Z1 Classifier Results:\")\n",
    "print(confusion_matrix(y_test_z1, y_pred_z1))\n",
    "print(classification_report(y_test_z1, y_pred_z1))\n",
    "\n",
    "# Test with Z2\n",
    "X_train_z2, X_test_z2, y_train_z2, y_test_z2 = train_test_split(latent_z2, Y_labels, test_size=0.3, random_state=42)\n",
    "classifier_z2 = LogisticRegression(max_iter=1000)\n",
    "classifier_z2.fit(X_train_z2, y_train_z2)\n",
    "\n",
    "y_pred_z2 = classifier_z2.predict(X_test_z2)\n",
    "\n",
    "print(\"Z2 Classifier Results:\")\n",
    "print(confusion_matrix(y_test_z2, y_pred_z2))\n",
    "print(classification_report(y_test_z2, y_pred_z2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
